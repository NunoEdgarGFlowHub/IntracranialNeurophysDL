{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "# Common imports\n",
    "from pathlib import Path\n",
    "import os\n",
    "import numpy as np\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Standard block to equalize local and Colab.\n",
    "try:\n",
    "    # See if we are running on google.colab\n",
    "    import google.colab\n",
    "    from google.colab import files\n",
    "    # Configure kaggle\n",
    "    files.upload()  # Find the kaggle.json file in your ~/.kaggle directory.\n",
    "    !pip install -q kaggle\n",
    "    !mkdir -p ~/.kaggle\n",
    "    !mv kaggle.json ~/.kaggle/\n",
    "    !chmod 600 ~/.kaggle/kaggle.json\n",
    "    # Download the workshop repo and change to its directory\n",
    "    # For now edit the username/password. This requirement will be removed when the repo is made public.\n",
    "    !git clone https://<username>:<password>@github.com/SachsLab/IntracranialNeurophysDL.git ../repo\n",
    "    os.chdir('../repo')\n",
    "    !pip install tensorboardcolab\n",
    "    from tensorboardcolab import TensorBoardColab, TensorBoardColabCallback\n",
    "    tbc=TensorBoardColab(startup_waiting_time=30)\n",
    "    tensorboard_callback = TensorBoardColabCallback(tbc)\n",
    "    IN_COLAB = True\n",
    "except ModuleNotFoundError:\n",
    "    import sys\n",
    "    if Path.cwd().stem == 'notebooks':\n",
    "        os.chdir(Path.cwd().parent)\n",
    "    # Add Python env Scripts to path for kaggle use.\n",
    "    os.environ['PATH'] = os.environ['PATH'] + ';' + str(Path(sys.executable).parent / 'Scripts')\n",
    "    # Clear any logs from previous runs\n",
    "    if (Path.cwd() / 'logs').is_dir():\n",
    "        !rmdir /S /Q logs\n",
    "    import datetime\n",
    "    %load_ext tensorboard.notebook\n",
    "    log_dir = Path.cwd() / \"logs\" / datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "    tensorboard_callback = tf.keras.callbacks.TensorBoard(log_dir=log_dir, histogram_freq=1)\n",
    "    IN_COLAB = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Download faces_basic if we don't already have it.\n",
    "# If you get 401 - Unauthorized, then Chad needs to add your kaggle account,\n",
    "# or you need to create a new API token from kaggle\n",
    "\n",
    "datadir = Path.cwd() / 'data' / 'kjm_ecog'\n",
    "if not (datadir / 'converted').is_dir():\n",
    "    !kaggle datasets download -d cboulay/kjm-ecog-faces-basic\n",
    "    print(\"Finished downloading. Now extracting contents...\")\n",
    "    data_path = Path('kjm-ecog-faces-basic.zip')\n",
    "    import zipfile\n",
    "    with zipfile.ZipFile(data_path, 'r') as zip_ref:\n",
    "        zip_ref.extractall(datadir / 'converted' / 'faces_basic')\n",
    "    data_path.unlink()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(603, 527) (603, 1)\n"
     ]
    }
   ],
   "source": [
    "# Load data from one participant.\n",
    "SUB_ID = 'de'\n",
    "from data.utils.fileio import from_neuropype_h5\n",
    "test_file = datadir / 'converted' / 'faces_basic' / (SUB_ID + '_bp.h5')\n",
    "chunks = from_neuropype_h5(test_file)\n",
    "chunk_names = [_[0] for _ in chunks]\n",
    "chunk = chunks[chunk_names.index('signals')][1]\n",
    "ax_types = [_['type'] for _ in chunk['axes']]\n",
    "instance_axis = chunk['axes'][ax_types.index('instance')]\n",
    "n_trials = len(instance_axis['data'])\n",
    "X = chunk['data'].reshape((n_trials, -1))  # 603 trials x 527 features\n",
    "Y = instance_axis['data']['Marker'].values.reshape(-1, 1)\n",
    "n_trials, n_features = X.shape\n",
    "print(X.shape, Y.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Creating an artificial neural net with Tensorflow and Keras\n",
    "\n",
    "We will next create a \"model\". The model maps the input data to some output.\n",
    "\n",
    "Each trial _idx_ in our input has our brain signal data X[idx] and a label Y[idx].\n",
    "\n",
    "Each data point in X[idx] is of shape (n_features,),\n",
    "where n_features is the product m_channels * n_samples.\n",
    "\n",
    "Each label (Y[idx]) is 1 of 3 values: \"ISI\", \"face\", or \"house\".\n",
    "\n",
    "So we want to find a model that maps from an n_features input to 3 outputs,\n",
    "where each of the outputs is a score associated with a class.\n",
    "We will use a simple linear fully-connected model with a single layer.\n",
    "I honestly couldn't think of a simpler neural network.\n",
    "\n",
    "![simple linear network](img/simple_linear_nn.png) \n",
    "created with [NN-SVG](http://alexlenail.me/NN-SVG/index.html)\n",
    "\n",
    "We use Tensorflow to create and train the model.\n",
    "The model specification is done using the [Keras API, as provided by tensorflow](https://www.tensorflow.org/guide/keras).\n",
    "\n",
    "Tensorflow has [\"eager execution\"](https://www.tensorflow.org/guide/eager)\n",
    "enabled so TensorFlow operations execute immediately and their results can be inspected.\n",
    "This is the default behaviour in upcoming TensorFlow 2.0 so we might as well get used to it.\n",
    "[Another tf.eager tutorial here.](https://adventuresinmachinelearning.com/tensorflow-eager-tutorial/)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Create the model. Use functional interface.\n",
    "inputs = tf.keras.layers.Input(shape=(X.shape[1],))\n",
    "dat_in_model = tf.keras.layers.Dense(3, activation='linear')(inputs)\n",
    "outputs = tf.keras.layers.Softmax()(dat_in_model)  # Convert to probability scores, summing to 1\n",
    "model = tf.keras.Model(inputs, outputs)\n",
    "\n",
    "# Note: The Dense layer can use an activation='softmax' and we could drop the Softmax layer."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Before the network is ready, it needs three more things:\n",
    "* A loss function. See options in tf.losses\n",
    "* An optimizer. See options in tf.keras.optimizers ('Adam', 'rmsprop', 'SGD')\n",
    "* Metrics to monitor during training\n",
    "\n",
    "[neat graph for optimizers](https://www.tensorflow.org/tutorials/eager/custom_training_walkthrough#create_an_optimizer)\n",
    "\n",
    "These are configured during compilation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_1 (InputLayer)         [(None, 527)]             0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 3)                 1584      \n",
      "_________________________________________________________________\n",
      "softmax (Softmax)            (None, 3)                 0         \n",
      "=================================================================\n",
      "Total params: 1,584\n",
      "Trainable params: 1,584\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.compile(loss='categorical_crossentropy', optimizer='rmsprop', metrics=['accuracy'])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Prepare data to feed into the model\n",
    "\n",
    "### Create the dataset\n",
    "Tensorflow has a data API called [tf.data](https://www.tensorflow.org/guide/datasets).\n",
    "We will use it to create a [`Dataset`](https://www.tensorflow.org/api_docs/python/tf/data/Dataset).\n",
    "\n",
    "Note that we use Eager so the Dataset usage is maybe different to what you will find in other Dataset\n",
    "tutorials. The differences are [explained here](https://www.tensorflow.org/tutorials/eager/eager_basics#datasets).\n",
    "\n",
    "Datasets provide several important features:\n",
    "* implicit conversion from other types to tensorflow Tensor types\n",
    "* easy transformations\n",
    "* automatic feeding of data into hungry models\n",
    "\n",
    "See a more complete example [here](https://gist.github.com/datlife/abfe263803691a8864b7a2d4f87c4ab8).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Convert Y from strings to integers.\n",
    "classes, y = np.unique(Y, return_inverse=True)\n",
    "n_classes = len(classes)\n",
    "n_trials = len(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Organize data for training and validation.\n",
    "Let's use 80% of the data for training and the remaining for validation.\n",
    "\n",
    "Note: It is good practice to separate data into 3 sets: training, validation, and test.\n",
    "Otherwise, the hyperparameter tuning to improve performance on the validation set will leak\n",
    "information about the validation set into the model, so the model training is no longer blind to the\n",
    "validation set and performance on the validation set is not indicative of real world performance.\n",
    "However, today we are not concerned with real-world performance, so we don't want to hinder ourselves\n",
    "by limiting the amount of training data we have.\n",
    "\n",
    "Before we start slicing up our data, it's useful to put the neural data (X) and labels (Y)\n",
    "into a single container. For that we use [tf.data.Datset.zip](https://www.tensorflow.org/api_docs/python/tf/data/Dataset#zip), which is implied by passing a tuple of data items to `from_tensor_slices`.\n",
    "\n",
    "Finally, we separate into training and validation sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "PTRAIN = 0.8\n",
    "X_train, X_valid, y_train, y_valid = train_test_split(X, y, train_size=PTRAIN)\n",
    "n_train = len(y_train)\n",
    "n_valid = len(y_valid)\n",
    "ds_train = tf.data.Dataset.from_tensor_slices((X_train, y_train))\n",
    "ds_valid = tf.data.Dataset.from_tensor_slices((X_valid, y_valid))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Apply transformations to the dataset\n",
    "\n",
    "We define a function to separately transform X and y.\n",
    "\n",
    "First we downcast X from float64 to float32.\n",
    "\n",
    "y is a set of integers from 0-2 representing the id of the class for each trial.\n",
    "There is no reasoning that the distance between 0 (ISI) and 1 (face) is the same\n",
    "as the distance between 1 (face) and 2 (house). We cannot train a model to think\n",
    "that house-related activity is twice as face-y as face-related activity.\n",
    "In other words, Y is not a continuous variable. It is a categorical variable.\n",
    "\n",
    "One way to represent a non-binary categorical variable is to use one-hot encoding:\n",
    "[0, 1, 2, 1] becomes [[1, 0, 0], [0, 1, 0], [0, 0, 1], [0, 1, 0]].\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def preprocess_fn(x_dat, y_dat):\n",
    "    x_dat = tf.cast(x_dat, tf.float32)\n",
    "    y_dat = tf.one_hot(tf.cast(y_dat, tf.uint8), n_classes)\n",
    "    return x_dat, y_dat\n",
    "ds_train = ds_train.map(preprocess_fn)\n",
    "ds_valid = ds_valid.map(preprocess_fn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Further transform the data to prepare for training\n",
    "When training a neural net, we can update the model using only a single trial at a time,\n",
    "or we can update the model using a batch of trials. Let's tell our training data that we\n",
    "want it to be fed into the model in batches, using [dataset.batch(BATCH_SIZE)](https://www.tensorflow.org/api_docs/python/tf/data/Dataset#batch).\n",
    "Having larger batches speeds up training but requires more memory. You can try increasing\n",
    "BATCH_SIZE (then try training) until you run out of GPU memory.\n",
    "The downside to having a larger BATCH_SIZE is that there is less granularity during training. \n",
    "\n",
    "We also [shuffle](https://www.tensorflow.org/api_docs/python/tf/data/Dataset#shuffle) the training trials."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "BATCH_SIZE = 5\n",
    "ds_train = ds_train.shuffle(int(n_trials * PTRAIN) + 1).batch(BATCH_SIZE, drop_remainder=True).repeat()  # , drop_remainder=True?\n",
    "ds_valid = ds_valid.batch(BATCH_SIZE).repeat()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train the model\n",
    "\n",
    "Typically we will go through the dataset more than once.\n",
    "Each trip through the entire training dataset is called an 'epoch'.\n",
    "How many epochs do we want? Typically you want as many epochs as you need to maximize performance\n",
    "on the validation set. Too many can lead to over-fitting on the training set.\n",
    "We should also be concerned about training time, energy usage, etc.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      " 1/96 [..............................] - ETA: 1:07 - loss: 1.2892 - accuracy: 0.4000"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Logging before flag parsing goes to stderr.\n",
      "W0522 15:53:20.285254 12132 callbacks.py:236] Method (on_train_batch_end) is slow compared to the batch update (0.154507). Check your callbacks.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 2/96 [..............................] - ETA: 40s - loss: 1.7517 - accuracy: 0.2000 "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0522 15:53:20.295257 12132 callbacks.py:236] Method (on_train_batch_end) is slow compared to the batch update (0.138016). Check your callbacks.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "96/96 [==============================] - ETA: 4s - loss: 1.4280 - accuracy: 0.312 - ETA: 2s - loss: 1.2352 - accuracy: 0.45 - ETA: 1s - loss: 1.0782 - accuracy: 0.52 - ETA: 0s - loss: 0.9257 - accuracy: 0.59 - ETA: 0s - loss: 0.8401 - accuracy: 0.63 - ETA: 0s - loss: 0.7722 - accuracy: 0.65 - 2s 18ms/step - loss: 0.7453 - accuracy: 0.6729 - val_loss: 0.3904 - val_accuracy: 0.8250\n",
      "Epoch 2/10\n",
      "96/96 [==============================] - ETA: 2s - loss: 0.2032 - accuracy: 1.00 - ETA: 0s - loss: 0.2387 - accuracy: 0.88 - ETA: 0s - loss: 0.2666 - accuracy: 0.86 - ETA: 0s - loss: 0.2512 - accuracy: 0.88 - ETA: 0s - loss: 0.2472 - accuracy: 0.89 - ETA: 0s - loss: 0.2328 - accuracy: 0.89 - ETA: 0s - loss: 0.2213 - accuracy: 0.90 - 1s 7ms/step - loss: 0.2163 - accuracy: 0.9062 - val_loss: 0.1997 - val_accuracy: 0.9250\n",
      "Epoch 3/10\n",
      "96/96 [==============================] - ETA: 2s - loss: 0.0967 - accuracy: 1.00 - ETA: 0s - loss: 0.0866 - accuracy: 0.97 - ETA: 0s - loss: 0.0860 - accuracy: 0.98 - ETA: 0s - loss: 0.0924 - accuracy: 0.97 - ETA: 0s - loss: 0.0978 - accuracy: 0.97 - ETA: 0s - loss: 0.0955 - accuracy: 0.97 - ETA: 0s - loss: 0.0940 - accuracy: 0.97 - 1s 7ms/step - loss: 0.0921 - accuracy: 0.9771 - val_loss: 0.1527 - val_accuracy: 0.9250\n",
      "Epoch 4/10\n",
      "96/96 [==============================] - ETA: 2s - loss: 0.0056 - accuracy: 1.00 - ETA: 0s - loss: 0.0539 - accuracy: 0.98 - ETA: 0s - loss: 0.0540 - accuracy: 0.98 - ETA: 0s - loss: 0.0461 - accuracy: 0.99 - ETA: 0s - loss: 0.0496 - accuracy: 0.99 - ETA: 0s - loss: 0.0469 - accuracy: 0.99 - ETA: 0s - loss: 0.0467 - accuracy: 0.99 - 1s 7ms/step - loss: 0.0463 - accuracy: 0.9958 - val_loss: 0.1096 - val_accuracy: 0.9750\n",
      "Epoch 5/10\n",
      "96/96 [==============================] - ETA: 2s - loss: 0.0671 - accuracy: 1.00 - ETA: 0s - loss: 0.0290 - accuracy: 1.00 - ETA: 0s - loss: 0.0270 - accuracy: 1.00 - ETA: 0s - loss: 0.0248 - accuracy: 1.00 - ETA: 0s - loss: 0.0255 - accuracy: 1.00 - ETA: 0s - loss: 0.0237 - accuracy: 1.00 - ETA: 0s - loss: 0.0245 - accuracy: 1.00 - 1s 6ms/step - loss: 0.0248 - accuracy: 1.0000 - val_loss: 0.0918 - val_accuracy: 0.9750\n",
      "Epoch 6/10\n",
      "96/96 [==============================] - ETA: 2s - loss: 0.0082 - accuracy: 1.00 - ETA: 0s - loss: 0.0142 - accuracy: 1.00 - ETA: 0s - loss: 0.0153 - accuracy: 1.00 - ETA: 0s - loss: 0.0139 - accuracy: 1.00 - ETA: 0s - loss: 0.0135 - accuracy: 1.00 - ETA: 0s - loss: 0.0134 - accuracy: 1.00 - ETA: 0s - loss: 0.0126 - accuracy: 1.00 - 1s 6ms/step - loss: 0.0132 - accuracy: 1.0000 - val_loss: 0.0765 - val_accuracy: 0.9917\n",
      "Epoch 7/10\n",
      "96/96 [==============================] - ETA: 2s - loss: 5.2090e-04 - accuracy: 1.00 - ETA: 0s - loss: 0.0065 - accuracy: 1.0000   - ETA: 0s - loss: 0.0060 - accuracy: 1.00 - ETA: 0s - loss: 0.0065 - accuracy: 1.00 - ETA: 0s - loss: 0.0071 - accuracy: 1.00 - ETA: 0s - loss: 0.0074 - accuracy: 1.00 - ETA: 0s - loss: 0.0073 - accuracy: 1.00 - 1s 6ms/step - loss: 0.0076 - accuracy: 1.0000 - val_loss: 0.0731 - val_accuracy: 0.9833\n",
      "Epoch 8/10\n",
      "96/96 [==============================] - ETA: 2s - loss: 0.0021 - accuracy: 1.00 - ETA: 0s - loss: 0.0050 - accuracy: 1.00 - ETA: 0s - loss: 0.0042 - accuracy: 1.00 - ETA: 0s - loss: 0.0041 - accuracy: 1.00 - ETA: 0s - loss: 0.0040 - accuracy: 1.00 - ETA: 0s - loss: 0.0041 - accuracy: 1.00 - ETA: 0s - loss: 0.0042 - accuracy: 1.00 - 1s 6ms/step - loss: 0.0042 - accuracy: 1.0000 - val_loss: 0.0697 - val_accuracy: 0.9917\n",
      "Epoch 9/10\n",
      "96/96 [==============================] - ETA: 2s - loss: 0.0064 - accuracy: 1.00 - ETA: 0s - loss: 0.0021 - accuracy: 1.00 - ETA: 0s - loss: 0.0019 - accuracy: 1.00 - ETA: 0s - loss: 0.0020 - accuracy: 1.00 - ETA: 0s - loss: 0.0024 - accuracy: 1.00 - ETA: 0s - loss: 0.0025 - accuracy: 1.00 - ETA: 0s - loss: 0.0024 - accuracy: 1.00 - 1s 6ms/step - loss: 0.0024 - accuracy: 1.0000 - val_loss: 0.0684 - val_accuracy: 0.9833\n",
      "Epoch 10/10\n",
      "96/96 [==============================] - ETA: 2s - loss: 0.0010 - accuracy: 1.00 - ETA: 0s - loss: 0.0013 - accuracy: 1.00 - ETA: 0s - loss: 0.0017 - accuracy: 1.00 - ETA: 0s - loss: 0.0015 - accuracy: 1.00 - ETA: 0s - loss: 0.0016 - accuracy: 1.00 - ETA: 0s - loss: 0.0015 - accuracy: 1.00 - ETA: 0s - loss: 0.0014 - accuracy: 1.00 - 1s 6ms/step - loss: 0.0014 - accuracy: 1.0000 - val_loss: 0.0680 - val_accuracy: 0.9833\n"
     ]
    }
   ],
   "source": [
    "N_EPOCHS = 10\n",
    "history = model.fit(x=ds_train,  \n",
    "                    epochs=N_EPOCHS, \n",
    "                    validation_data=ds_valid,\n",
    "                    steps_per_epoch=n_train // BATCH_SIZE,\n",
    "                    validation_steps=(len(Y)-n_train) // BATCH_SIZE,\n",
    "                    callbacks=[tensorboard_callback],\n",
    "                    verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ERROR: Timed out waiting for TensorBoard to start. It may still be running as pid 1824."
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%tensorboard --logdir {log_dir}"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  },
  "pycharm": {
   "stem_cell": {
    "cell_type": "raw",
    "metadata": {
     "collapsed": false
    },
    "source": []
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
