{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "Note: This is a copy of [the original notebook](https://github.com/google-research/computation-thru-dynamics/blob/master/notebooks/Integrator%20RNN%20Tutorial.ipynb)\n",
    "adapted to [run on Google Colab](https://colab.research.google.com/github/SachsLab/IntracranialNeurophysDL/blob/master/notebooks/05_00_RNN_white_noise_integrator.ipynb). \n",
    "\n",
    "# Integrator RNN tutorial\n",
    "\n",
    "In this notebook, we train a vanilla RNN to integrate white noise.  This example is useful on its own to understand how RNN training works, and how to use JAX.  In addition, it provides the input for the LFADS JAX tutorial. "
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Copyright 2019 Google LLC\n",
    "\n",
    "Licensed under the Apache License, Version 2.0 (the \"License\");\n",
    "you may not use this file except in compliance with the License.\n",
    "You may obtain a copy of the License at\n",
    "\n",
    "     https://www.apache.org/licenses/LICENSE-2.0\n",
    "\n",
    "Unless required by applicable law or agreed to in writing, software\n",
    "distributed under the License is distributed on an \"AS IS\" BASIS,\n",
    "WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
    "See the License for the specific language governing permissions and\n",
    "limitations under the License."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Imports"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Numpy, JAX, Matplotlib and h5py should all be correctly installed and on the python path.\n",
    "from __future__ import print_function, division, absolute_import\n",
    "import datetime\n",
    "import h5py\n",
    "import jax.numpy as np\n",
    "from jax import random\n",
    "from jax.experimental import optimizers\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as onp             # original CPU-backed NumPy\n",
    "import os\n",
    "import sys\n",
    "import time"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Import the tutorial code.\n",
    "\n",
    "# You must change this to the location of computation-thru-dynamics directory.\n",
    "HOME_DIR = '/home/sussillo/' \n",
    "\n",
    "sys.path.append(os.path.join(HOME_DIR,'computation-thru-dynamics'))\n",
    "import integrator_rnn_tutorial.integrator as integrator\n",
    "import integrator_rnn_tutorial.rnn as rnn\n",
    "import integrator_rnn_tutorial.utils as utils"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Hyperparameters"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Integration parameters\n",
    "T = 1.0          # Arbitrary amount time, roughly physiological.\n",
    "ntimesteps = 25  # Divide T into this many bins\n",
    "bval = 0.01      # bias value limit\n",
    "sval = 0.025     # standard deviation (before dividing by sqrt(dt))\n",
    "input_params = (bval, sval, T, ntimesteps)\n",
    "\n",
    "# Integrator RNN hyperparameters\n",
    "u = 1         # Number of inputs to the RNN\n",
    "n = 100       # Number of units in the RNN\n",
    "o = 1         # Number of outputs in the RNN\n",
    "\n",
    "# The scaling of the recurrent parameters in an RNN really matters. \n",
    "# The correct scaling is 1/sqrt(number of recurrent inputs), which \n",
    "# yields an order 1 signal output to a neuron if the input is order 1.\n",
    "# Given that VRNN uses a tanh nonlinearity, with min and max output \n",
    "# values of -1 and 1, this works out.  The scaling just below 1 \n",
    "# (0.95) is because we know we are making a line attractor so, we \n",
    "# might as well start it off basically right 1.0 is also basically \n",
    "# right, but perhaps will lead to crazier dynamics.\n",
    "param_scale = 0.85 # Scaling of the recurrent weight matrix\n",
    "\n",
    "# Optimization hyperparameters\n",
    "num_batchs = 10000         # Total number of batches to train on.\n",
    "batch_size = 128          # How many examples in each batch\n",
    "eval_batch_size = 1024    # How large a batch for evaluating the RNN\n",
    "step_size = 0.025          # initial learning rate\n",
    "decay_factor = 0.99975     # decay the learning rate this much\n",
    "# Gradient clipping is HUGELY important for training RNNs\n",
    "max_grad_norm = 10.0      # max gradient norm before clipping, clip to this value.\n",
    "l2reg = 0.0002           # amount of L2 regularization on the weights\n",
    "adam_b1 = 0.9             # Adam parameters\n",
    "adam_b2 = 0.999\n",
    "adam_eps = 1e-1\n",
    "print_every = 100          # Print training informatino every so often"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# JAX handles randomness differently than numpy or matlab. \n",
    "# one threads the randomness through to each function. \n",
    "#  It's a bit tedious, but very easy to understand and with\n",
    "# reliable effect.\n",
    "seed = onp.random.randint(0, 1000000) # get randomness from CPU level numpy\n",
    "print(\"Seed: %d\" % seed)\n",
    "key = random.PRNGKey(seed) # create a random key for jax for use on device.\n",
    "\n",
    "# Plot a few input/target examples to make sure things look sane.\n",
    "ntoplot = 10    # how many examples to plot\n",
    "# With this split command, we are always getting a new key from the old key,\n",
    "# and I use first key as as source of randomness for new keys.\n",
    "#     key, subkey = random.split(key, 2)\n",
    "#     ## do something random with subkey\n",
    "#     key, subkey = random.split(key, 2)\n",
    "#     ## do something random with subkey\n",
    "# In this way, the same top level randomness source stays random.\n",
    "\n",
    "# The number of examples to plot is given by the number of \n",
    "# random keys in this function.\n",
    "key, skey = random.split(key, 2)\n",
    "skeys = random.split(skey, ntoplot) # get ntoplot random keys\n",
    "inputs, targets = integrator.build_inputs_and_targets_jit(input_params, skeys)\n",
    "\n",
    "# Plot the input to the RNN and the target for the RNN.\n",
    "integrator.plot_batch(ntimesteps, inputs, targets)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Init some parameters for training.\n",
    "key, subkey = random.split(key, 2)\n",
    "init_params = rnn.random_vrnn_params(subkey, u, n, o, g=param_scale)\n",
    "rnn.plot_params(init_params)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Create a decay function for the learning rate\n",
    "decay_fun = optimizers.exponential_decay(step_size, decay_steps=1, \n",
    "                                         decay_rate=decay_factor)\n",
    "\n",
    "batch_idxs = onp.linspace(1, num_batchs)\n",
    "plt.plot(batch_idxs, [decay_fun(b) for b in batch_idxs])\n",
    "plt.axis('tight')\n",
    "plt.xlabel('Batch number')\n",
    "plt.ylabel('Learning rate');"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Train the VRNN"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "reload(rnn)\n",
    "# Initialize the optimizer.  Please see jax/experimental/optimizers.py\n",
    "opt_init, opt_update, get_params = optimizers.adam(decay_fun, adam_b1, adam_b2, adam_eps)\n",
    "opt_state = opt_init(init_params)\n",
    "\n",
    "# Run the optimization loop, first jit'd call will take a minute.\n",
    "start_time = time.time()\n",
    "all_train_losses = []\n",
    "for batch in range(num_batchs):\n",
    "    key, subkey = random.split(key, 2)\n",
    "    skeys = random.split(subkey, batch_size)\n",
    "    inputs, targets = integrator.build_inputs_and_targets_jit(input_params, skeys)\n",
    "    opt_state = rnn.update_w_gc_jit(batch, opt_state, opt_update, get_params, inputs,\n",
    "                                  targets, max_grad_norm, l2reg)\n",
    "    if batch % print_every == 0:\n",
    "        params = get_params(opt_state)\n",
    "        all_train_losses.append(rnn.loss_jit(params, inputs, targets, l2reg))\n",
    "        train_loss = all_train_losses[-1]['total']\n",
    "        batch_time = time.time() - start_time\n",
    "        step_size = decay_fun(batch)\n",
    "        s = \"Batch {} in {:0.2f} sec, step size: {:0.5f}, training loss {:0.4f}\"\n",
    "        print(s.format(batch, batch_time, step_size, train_loss))\n",
    "        start_time = time.time()\n",
    "        \n",
    "# List of dicts to dict of lists\n",
    "all_train_losses = {k: [dic[k] for dic in all_train_losses] for k in all_train_losses[0]}"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Show the loss through training.\n",
    "xlims = [2, 50]\n",
    "plt.figure(figsize=(16,4))\n",
    "plt.subplot(141)\n",
    "plt.plot(all_train_losses['total'][xlims[0]:xlims[1]], 'k')\n",
    "plt.title('Total')\n",
    "\n",
    "plt.subplot(142)\n",
    "plt.plot(all_train_losses['lms'][xlims[0]:xlims[1]], 'r')\n",
    "plt.title('Least mean square')\n",
    "\n",
    "plt.subplot(143)\n",
    "plt.plot(all_train_losses['l2'][xlims[0]:xlims[1]], 'g');\n",
    "plt.title('L2')\n",
    "\n",
    "plt.subplot(144)\n",
    "plt.plot(all_train_losses['total'][xlims[0]:xlims[1]], 'k')\n",
    "plt.plot(all_train_losses['lms'][xlims[0]:xlims[1]], 'r')\n",
    "plt.plot(all_train_losses['l2'][xlims[0]:xlims[1]], 'g')\n",
    "plt.title('All losses')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Testing"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Take a batch for an evalulation loss, notice the L2 penalty is 0\n",
    "# for the evaluation.\n",
    "params = get_params(opt_state)\n",
    "\n",
    "key, subkey = random.split(key, 2)\n",
    "skeys = random.split(subkey, batch_size)\n",
    "inputs, targets = integrator.build_inputs_and_targets_jit(input_params, skeys)\n",
    "eval_loss = rnn.loss_jit(params, inputs, targets, l2reg=0.0)['total']\n",
    "eval_loss_str = \"{:.5f}\".format(eval_loss)\n",
    "print(\"Loss on a new large batch: %s\" % (eval_loss_str))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Visualizations of trained system"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "reload(rnn)\n",
    "\n",
    "# Visualize how good this trained integrator is\n",
    "def inputs_targets_no_h0s(keys):\n",
    "    inputs_b, targets_b = \\\n",
    "        integrator.build_inputs_and_targets_jit(input_params, keys)\n",
    "    h0s_b = None # Use trained h0\n",
    "    return inputs_b, targets_b, h0s_b\n",
    "\n",
    "rnn_run = lambda inputs: rnn.batched_rnn_run(params, inputs)\n",
    "\n",
    "give_trained_h0 = lambda batch_size : np.array([params['h0']] * batch_size)\n",
    "\n",
    "rnn_internals = rnn.run_trials(rnn_run, inputs_targets_no_h0s, 1, 16)\n",
    "\n",
    "integrator.plot_batch(ntimesteps, rnn_internals['inputs'], \n",
    "                      rnn_internals['targets'], rnn_internals['outputs'], \n",
    "                      onp.abs(rnn_internals['targets'] - rnn_internals['outputs']))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Visualize the hidden state, as an example.\n",
    "rnn.plot_examples(ntimesteps, rnn_internals, nexamples=4)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Take a look at the trained parameters.\n",
    "rnn.plot_params(params)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Saving"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Define directories, etc.\n",
    "task_type = 'pure_int'\n",
    "rnn_type = 'vrnn'\n",
    "fname_uniquifier = datetime.datetime.now().strftime(\"%Y-%m-%d_%H:%M:%S\")\n",
    "data_dir = os.path.join(os.path.join('/tmp', rnn_type), task_type)\n",
    "\n",
    "print(data_dir)\n",
    "print(fname_uniquifier)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Save parameters\n",
    "\n",
    "params_fname = ('trained_params_' + rnn_type + '_' + task_type + '_' + \\\n",
    "                eval_loss_str + '_' + fname_uniquifier + '.h5')\n",
    "params_fname = os.path.join(data_dir, params_fname)\n",
    "\n",
    "print(\"Saving params in %s\" % (params_fname))\n",
    "utils.write_file(params_fname, params)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Create per-trial initial conditions along the line attractor.\n",
    "Let's create some per-trial initial conditions by running the integrator for a few time steps. This will make comparing the learned initial states in the LFADS tutorial easier to visualize."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "nsave_batches = 10 # Save about 10000 trials\n",
    "\n",
    "h0_ntimesteps = 7 # Only a few steps, we don't wanna get too far out of it's range.\n",
    "h0_input_params = (bval, sval,  \n",
    "                   T * float(h0_ntimesteps) / float(ntimesteps), \n",
    "                   h0_ntimesteps)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def get_h0s_inputs_targets_h0s(keys):\n",
    "    inputs_bxtxu, targets_bxtxm = \\\n",
    "        integrator.build_inputs_and_targets_jit(h0_input_params, keys)\n",
    "    B, T, U = inputs_bxtxu.shape\n",
    "    _, _, M = targets_bxtxm.shape\n",
    "    # We add a zero to the end of the input so that the network will \"snap back\"\n",
    "    # to the line attractor, which will make comparing the learned initial states \n",
    "    # in the LFADS example easier to visualize.\n",
    "    inputs_bxtp1xu = np.concatenate([inputs_bxtxu, np.zeros([B, 1, U])], axis=1)\n",
    "    targets_bxtp1xm = np.concatenate([targets_bxtxm, targets_bxtxm[:, -1:, :]], axis=1)\n",
    "                                    \n",
    "    h0s = give_trained_h0(len(keys))\n",
    "    return (inputs_bxtp1xu, targets_bxtp1xm, h0s)\n",
    "    \n",
    "rnn_run_w_h0 = lambda inputs, h0s: rnn.batched_rnn_run_w_h0(params, inputs, h0s)\n",
    "\n",
    "h0_data_dict = rnn.run_trials(rnn_run_w_h0, get_h0s_inputs_targets_h0s, \n",
    "                              nsave_batches, eval_batch_size)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "The saved trials will *begin* with initial conditions that are the *end* of these trials."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "rnn.plot_examples(h0_ntimesteps, h0_data_dict, nexamples=4)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "### Save some trials, this is for the LFADS tutorial, and no need \n",
    "### to save this otherwise.\n",
    "h0s_bxn = h0_data_dict['hiddens'][:, -1, :] # take last time step\n",
    "\n",
    "# This generator is a damned hack, but it's a throw-away method\n",
    "# for making h0s.\n",
    "h0gen = (h0 for h0 in h0s_bxn)\n",
    "def h0s_on_a_line_fun(batch_size):\n",
    "    \"\"\"Spit out batch_size h0s at a time as nd.array.\"\"\"\n",
    "    h0s = []\n",
    "    for b in range(batch_size):\n",
    "        h0s.append(next(h0gen))\n",
    "    return onp.array(h0s)\n",
    "\n",
    "\n",
    "def use_h0s_inputs_targets_h0s(keys):\n",
    "    \"\"\"Use the line attractro h0s for the saved data.\"\"\"\n",
    "    inputs_b, targets_b = \\\n",
    "        integrator.build_inputs_and_targets_jit(input_params, keys)\n",
    "    h0s_b = h0s_on_a_line_fun(len(keys))\n",
    "    \n",
    "    # Target has to updated based on differing initial condition.\n",
    "    offsets_b = rnn.batch_affine(params, h0s_b)\n",
    "    targets_b = targets_b + np.expand_dims(offsets_b, axis=2)\n",
    "    return inputs_b, targets_b, h0s_b\n",
    "\n",
    "# Save about 10,000 trials for playing around with the LFADS tutorial.\n",
    "nsave_batches = 10 # Save about 10000 trials\n",
    "data_dict = rnn.run_trials(rnn_run_w_h0, use_h0s_inputs_targets_h0s, nsave_batches,\n",
    "                           eval_batch_size)\n",
    "\n",
    "data_fname = ('trained_data_' + rnn_type + '_' + task_type + '_' + \\\n",
    "              eval_loss_str + '_' + fname_uniquifier + '.h5')\n",
    "data_fname = os.path.join(data_dir, data_fname)\n",
    "print(\"Saving data in %s\" %(data_fname))\n",
    "utils.write_file(data_fname, data_dict)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Finally, here are some example trials that are saved. Notice the hidden state starts now from different positions on the line and the integrator integrates for a larger number of time steps.  This will allow the LFADS tutorial to infer the intial state (along the line), the white noise input, and the underlying rates (which are 'spikified' in the LFADS example)."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "rnn.plot_examples(ntimesteps, data_dict, nexamples=4)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  },
  "kernelspec": {
   "name": "python3",
   "language": "python",
   "display_name": "Python 3"
  },
  "pycharm": {
   "stem_cell": {
    "cell_type": "raw",
    "source": [],
    "metadata": {
     "collapsed": false
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}