{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "06_00_LFADS_Tutorial.ipynb",
      "version": "0.3.2",
      "provenance": []
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 2
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython2",
      "version": "2.7.6"
    },
    "kernelspec": {
      "name": "python3",
      "language": "python",
      "display_name": "Python 3"
    },
    "accelerator": "GPU",
    "stem_cell": {
      "cell_type": "raw",
      "source": "",
      "metadata": {
        "pycharm": {
          "metadata": false
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": "# LFADS\nThis notebook is part of the [SachsLab Workshop for Intracranial Neurophysiology and Deep Learning](https://github.com/SachsLab/IntracranialNeurophysDL).\n\nFollow the link below to run in Google Colab or continue if running locally.\n\nRun the first few cells to normalize Local / Colab environments, then proceed below for the lesson.\n\n\u003ctable class\u003d\"tfo-notebook-buttons\" align\u003d\"left\"\u003e\n  \u003ctd\u003e\n    \u003ca target\u003d\"_blank\" href\u003d\"https://colab.research.google.com/github/SachsLab/IntracranialNeurophysDL/blob/master/notebooks/06_01_LFADS.ipynb\"\u003e\u003cimg src\u003d\"https://www.tensorflow.org/images/colab_logo_32px.png\" /\u003eRun in Google Colab\u003c/a\u003e\n  \u003c/td\u003e\n  \u003ctd\u003e\n    \u003ca target\u003d\"_blank\" href\u003d\"https://github.com/SachsLab/IntracranialNeurophysDL/blob/master/notebooks/06_01_LFADS.ipynb\"\u003e\u003cimg src\u003d\"https://www.tensorflow.org/images/GitHub-Mark-32px.png\" /\u003eView source on GitHub\u003c/a\u003e\n  \u003c/td\u003e\n\u003c/table\u003e",
      "metadata": {
        "pycharm": {
          "metadata": false
        }
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "outputs": [],
      "source": "import os\nimport sys\nfrom pathlib import Path\ntry:\n    # See if we are running on google.colab\n    import google.colab\n    from google.colab import files\n    if sys.version_info \u003e (3, 0):\n        from importlib import reload\n        !pip install --upgrade -q https://storage.googleapis.com/jax-wheels/cuda$(echo $CUDA_VERSION | sed -e \u0027s/\\.//\u0027 -e \u0027s/\\..*//\u0027)/jaxlib-latest-cp36-none-linux_x86_64.whl\n    else:\n        !pip install --upgrade -q https://storage.googleapis.com/jax-wheels/cuda$(echo $CUDA_VERSION | sed -e \u0027s/\\.//\u0027 -e \u0027s/\\..*//\u0027)/jaxlib-latest-cp27-none-linux_x86_64.whl\n    !pip install --upgrade -q git+https://github.com/google/jax.git\n\n    os.chdir(\u0027..\u0027)\n    if not (Path.home() / \u0027.kaggle\u0027).is_dir():\n        # Configure kaggle\n        files.upload()  # Find the kaggle.json file in your ~/.kaggle directory.\n        !pip install -q kaggle\n        !mkdir -p ~/.kaggle\n        !mv kaggle.json ~/.kaggle/\n        !chmod 600 ~/.kaggle/kaggle.json\n    if Path.cwd().stem !\u003d \u0027IntracranialNeurophysDL\u0027:\n        if not (Path.cwd() / \u0027IntracranialNeurophysDL\u0027).is_dir():\n            # Download the workshop repo and change to its directory\n            !git clone --recursive https://github.com/SachsLab/IntracranialNeurophysDL.git\n        os.chdir(\u0027IntracranialNeurophysDL\u0027)\n    IN_COLAB \u003d True\nexcept ModuleNotFoundError:\n    IN_COLAB \u003d False\n    if Path.cwd().stem \u003d\u003d \u0027notebooks\u0027:\n        os.chdir(Path.cwd().parent)\n    # Make sure the kaggle executable is on the PATH\n    os.environ[\u0027PATH\u0027] \u003d os.environ[\u0027PATH\u0027] + \u0027;\u0027 + str(Path(sys.executable).parent / \u0027Scripts\u0027)",
      "metadata": {
        "pycharm": {
          "metadata": false,
          "name": "#%%\n"
        }
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "outputs": [],
      "source": "# Download and unzip data (2.1 GB)\ndatadir \u003d Path.cwd() / \u0027data\u0027 / \u0027joeyo\u0027\nif not (datadir / \u0027converted\u0027).is_dir():\n    !kaggle datasets download --unzip --path {str(datadir / \u0027converted\u0027)} cboulay/joeyo_nhp_reach_mea\n    print(\"Finished downloading and extracting data.\")\nelse:\n    print(\"Data directory found. Skipping download.\")",
      "metadata": {
        "pycharm": {
          "metadata": false,
          "name": "#%%\n"
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": "## Imports",
      "metadata": {
        "pycharm": {
          "metadata": false,
          "name": "#%% md\n"
        }
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "outputs": [],
      "source": "import datetime\nimport h5py\nimport jax.numpy as np\nfrom jax import random\nfrom jax.experimental import optimizers\nfrom jax.config import config\n#config.update(\"jax_debug_nans\", True) # Useful for finding numerical errors\nimport matplotlib.pyplot as plt\nimport numpy as onp  # original CPU-backed NumPy\nimport scipy.signal\nimport scipy.stats\nimport time",
      "metadata": {
        "pycharm": {
          "metadata": false,
          "name": "#%%\n"
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": "## Configs, Constants, and Hyperparameters",
      "metadata": {
        "pycharm": {
          "metadata": false,
          "name": "#%% md\n"
        }
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "outputs": [],
      "source": "SESS_IDX \u003d 0          # Index of recording session we will use. 0:8\nBIN_DURATION \u003d 0.250  # Width of window used to bin spikes, in seconds\nN_TAPS \u003d 4            # Number of bins of history used in a sequence.\nP_TRAIN \u003d 0.8         # Proportion of data used for training.\nBATCH_SIZE \u003d 32       # Number of sequences in each training step.\nP_DROPOUT \u003d 0.05      # Proportion of units to set to 0 on each step.\nN_RNN_UNITS \u003d 60      # Size of RNN output (state)\nL2_REG \u003d 1.7e-5       # Parameter regularization strength.\nSTATEFUL \u003d False      # Whether or not to keep state between sequences (True is not tested)\nEPOCHS \u003d 10           # Number of loops through the entire data set.\n\n# LFADS Hyper parameters\n# data_dim \u003d train_data.shape[2]  # input to lfads should have dimensions:\n# ntimesteps \u003d train_data.shape[1] #   (batch_size x ntimesteps x data_dim)\n\n# LFADS architecture - The size of the numbers is rather arbitrary, \n# but relatively small because we know the integrator RNN isn\u0027t too high \n# dimensional in its activity.\nENC_DIM \u003d 128         # encoder dim\nCON_DIM \u003d 128         # contoller dim\nII_DIM \u003d 1            # inferred input dim, we know there is 1 dim in integrator RNN\nGEN_DIM \u003d 128         # generator dim, should be large enough to generate integrator RNN dynamics\nFACTORS_DIM \u003d 32      # factors dim, shoudl be large enough to capture most variance of dynamics\n\n# Numerical stability\nVAR_MIN \u003d 0.001       # Minimal variance any gaussian can become.\n\n# Initial state prior parameters\n# the mean is set to zero in the code\nic_prior_var \u003d 0.1 # this is $\\sigma^2_p$ of uninformative prior\n\n# Inferred input autoregressive prior parameters. I don\u0027t plan to do inputs.\n# Again, these hyper parameters are set \"in the ballpark\" but otherwise\n# pretty randomly.\nar_mean \u003d 0.0                 # process mean\nar_autocorrelation_tau \u003d 1.0  # seconds, how correlated each time point is, related to $\\phi$ above.\nar_noise_variance \u003d 0.1       # noise variance\n",
      "metadata": {
        "pycharm": {
          "metadata": false,
          "name": "#%%\n"
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": "## Load Spiking Data",
      "metadata": {
        "pycharm": {
          "metadata": false,
          "name": "#%% md\n"
        }
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "outputs": [],
      "source": "from data.utils.fileio import load_joeyo_reaching\n\ndef load_dat_with_vel_accel(datadir, sess_idx):\n    BEHAV_CHANS \u003d [\u0027CursorX\u0027, \u0027CursorY\u0027]\n    sess_names \u003d [\u0027indy_201\u0027 + _ for _ in [\u002760921_01\u0027, \u002760927_04\u0027, \u002760927_06\u0027, \u002760930_02\u0027, \u002760930_05\u0027, \u002761005_06\u0027,\n                                       \u002761006_02\u0027, \u002760124_01\u0027, \u002760127_03\u0027]]\n    X, Y, X_ax_info, Y_ax_info \u003d load_joeyo_reaching(datadir, sess_names[sess_idx], x_chunk\u003d\u0027mu_spiketimes\u0027)\n\n    # Slice Y to only keep required behaviour data (cursor position)\n    b_keep_y_chans \u003d np.in1d(Y_ax_info[\u0027channel_names\u0027] , BEHAV_CHANS)\n    Y \u003d Y[b_keep_y_chans, :]\n    Y_ax_info[\u0027channel_names\u0027] \u003d [_ for _ in Y_ax_info[\u0027channel_names\u0027] if _ in BEHAV_CHANS]\n\n    # Calculate discrete derivative and double-derivative to get velocity and acceleration.\n    vel \u003d np.diff(Y, axis\u003d1)\n    vel \u003d np.concatenate((vel[:, 0][:, None], vel), axis\u003d1)  # Assume velocity was constant across the first two samples.\n    accel \u003d np.concatenate(([[0], [0]], np.diff(vel, axis\u003d1)), axis\u003d1)  # Assume accel was 0 in the first sample.\n    Y \u003d np.concatenate((Y, vel, accel), axis\u003d0)\n    Y_ax_info[\u0027channel_names\u0027] +\u003d [\u0027VelX\u0027, \u0027VelY\u0027, \u0027AccX\u0027, \u0027AccY\u0027]\n    \n    return X, Y, X_ax_info, Y_ax_info\n\nX, Y, X_ax_info, Y_ax_info \u003d load_dat_with_vel_accel(datadir, SESS_IDX)\n",
      "metadata": {
        "pycharm": {
          "metadata": false,
          "name": "#%%\n"
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": "### Segment Into Trials \n",
      "metadata": {
        "pycharm": {
          "metadata": false
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": "### Visualize Data",
      "metadata": {
        "pycharm": {
          "metadata": false,
          "name": "#%% md\n"
        }
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "outputs": [],
      "source": "#TODO",
      "metadata": {
        "pycharm": {
          "metadata": false,
          "name": "#%%\n"
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": "## LFADS - Latent Factor Analysis via Dynamical Systems\nThis notebook is based on a notebook found in in the [google-research/computation-thru-dynamics github repo](https://github.com/google-research/computation-thru-dynamics).\n\nIn the below diagram, the \"Generator\" is the cortical neuronal population, which is assumed to be a nonlinear,\ndynamical system that is modeled with a RNN. For now we will ignore the bottom \"Controller\" and \"Inferred inputs\" parts.\n\n![](https://raw.githubusercontent.com/google-research/computation-thru-dynamics/master/images/lfads_architecture_w_inferred_inputs_3.png)\n\nThe data is put through nonlinear, recurrent **encoders**, and this produces an **initial state distribution**,\nwhich is a per-trial mean and variance to produce random vectors to encode that trial.\nThis is exactly the same as the \u0027bottleneck\u0027 or \u0027latent variables\u0027 we saw in the variational auto-encoder tutorial.\n\nThe initial state of the generator is a randomly drawn vector from this distribution.\nThe **generator** marches through time and at each time point produces \"factors\" and \"rates\".\nThe \"factors\" are the low-dimensional neural state. The \"rates\" are the projection of the neural state into neuronal activations.\nThe rates are then used to parameterize a Poisson process to generate spikes.\nThe loss function compares the generated spike trains to the original input spike trains.\n",
      "metadata": {
        "pycharm": {
          "metadata": false,
          "name": "#%% md\n"
        }
      }
    }
  ]
}