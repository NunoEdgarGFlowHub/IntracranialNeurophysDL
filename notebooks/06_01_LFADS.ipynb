{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "06_00_LFADS_Tutorial.ipynb",
      "version": "0.3.2",
      "provenance": []
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 2
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython2",
      "version": "2.7.6"
    },
    "kernelspec": {
      "name": "python3",
      "language": "python",
      "display_name": "Python 3"
    },
    "accelerator": "GPU",
    "stem_cell": {
      "cell_type": "raw",
      "source": "",
      "metadata": {
        "pycharm": {
          "metadata": false
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": "# LFADS\nThis notebook is part of the [SachsLab Workshop for Intracranial Neurophysiology and Deep Learning](https://github.com/SachsLab/IntracranialNeurophysDL).\n\nFollow the link below to run in Google Colab or continue if running locally.\n\nRun the first few cells to normalize Local / Colab environments, then proceed below for the lesson.\n\n\u003ctable class\u003d\"tfo-notebook-buttons\" align\u003d\"left\"\u003e\n  \u003ctd\u003e\n    \u003ca target\u003d\"_blank\" href\u003d\"https://colab.research.google.com/github/SachsLab/IntracranialNeurophysDL/blob/master/notebooks/06_01_LFADS.ipynb\"\u003e\u003cimg src\u003d\"https://www.tensorflow.org/images/colab_logo_32px.png\" /\u003eRun in Google Colab\u003c/a\u003e\n  \u003c/td\u003e\n  \u003ctd\u003e\n    \u003ca target\u003d\"_blank\" href\u003d\"https://github.com/SachsLab/IntracranialNeurophysDL/blob/master/notebooks/06_01_LFADS.ipynb\"\u003e\u003cimg src\u003d\"https://www.tensorflow.org/images/GitHub-Mark-32px.png\" /\u003eView source on GitHub\u003c/a\u003e\n  \u003c/td\u003e\n\u003c/table\u003e",
      "metadata": {
        "pycharm": {
          "metadata": false
        }
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "outputs": [],
      "source": "import os\nimport sys\nfrom pathlib import Path\ntry:\n    # See if we are running on google.colab\n    import google.colab\n    from google.colab import files\n    if sys.version_info \u003e (3, 0):\n        from importlib import reload\n        !pip install --upgrade -q https://storage.googleapis.com/jax-wheels/cuda$(echo $CUDA_VERSION | sed -e \u0027s/\\.//\u0027 -e \u0027s/\\..*//\u0027)/jaxlib-latest-cp36-none-linux_x86_64.whl\n    else:\n        !pip install --upgrade -q https://storage.googleapis.com/jax-wheels/cuda$(echo $CUDA_VERSION | sed -e \u0027s/\\.//\u0027 -e \u0027s/\\..*//\u0027)/jaxlib-latest-cp27-none-linux_x86_64.whl\n    !pip install --upgrade -q git+https://github.com/google/jax.git\n\n    os.chdir(\u0027..\u0027)\n    if not (Path.home() / \u0027.kaggle\u0027).is_dir():\n        # Configure kaggle\n        files.upload()  # Find the kaggle.json file in your ~/.kaggle directory.\n        !pip install -q kaggle\n        !mkdir -p ~/.kaggle\n        !mv kaggle.json ~/.kaggle/\n        !chmod 600 ~/.kaggle/kaggle.json\n    if Path.cwd().stem !\u003d \u0027IntracranialNeurophysDL\u0027:\n        if not (Path.cwd() / \u0027IntracranialNeurophysDL\u0027).is_dir():\n            # Download the workshop repo and change to its directory\n            !git clone --recursive https://github.com/SachsLab/IntracranialNeurophysDL.git\n        os.chdir(\u0027IntracranialNeurophysDL\u0027)\n    IN_COLAB \u003d True\nexcept ModuleNotFoundError:\n    IN_COLAB \u003d False\n    if Path.cwd().stem \u003d\u003d \u0027notebooks\u0027:\n        os.chdir(Path.cwd().parent)\n    # Make sure the kaggle executable is on the PATH\n    os.environ[\u0027PATH\u0027] \u003d os.environ[\u0027PATH\u0027] + \u0027;\u0027 + str(Path(sys.executable).parent / \u0027Scripts\u0027)",
      "metadata": {
        "pycharm": {
          "metadata": false,
          "name": "#%%\n",
          "is_executing": false
        }
      }
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "outputs": [
        {
          "name": "stdout",
          "text": [
            "Data directory found. Skipping download.\n"
          ],
          "output_type": "stream"
        }
      ],
      "source": "# Download and unzip data (2.1 GB)\ndatadir \u003d Path.cwd() / \u0027data\u0027 / \u0027joeyo\u0027\nif not (datadir / \u0027converted\u0027).is_dir():\n    !kaggle datasets download --unzip --path {str(datadir / \u0027converted\u0027)} cboulay/joeyo_nhp_reach_mea\n    print(\"Finished downloading and extracting data.\")\nelse:\n    print(\"Data directory found. Skipping download.\")",
      "metadata": {
        "pycharm": {
          "metadata": false,
          "name": "#%%\n",
          "is_executing": false
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": "## Imports",
      "metadata": {
        "pycharm": {
          "metadata": false,
          "name": "#%% md\n"
        }
      }
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "outputs": [],
      "source": "import jax.numpy as np\nfrom jax import jit, random, vmap, grad, lax\nfrom jax.experimental import optimizers\n#config.update(\"jax_debug_nans\", True) # Useful for finding numerical errors\nimport matplotlib.pyplot as plt\nimport numpy as onp  # original CPU-backed NumPy\nimport time",
      "metadata": {
        "pycharm": {
          "metadata": false,
          "name": "#%%\n",
          "is_executing": false
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": "## Get Data\nUnlike notebook 05_02, where we used a configurable but small number of wide bins per sequence,\nhere we use a large number of narrow bins per sequence, where a sequence corresponds to an entire trial\nfrom target onset to target hit (and a bit beyond).\n### Data Hyperparameters",
      "metadata": {
        "pycharm": {
          "metadata": false,
          "name": "#%% md\n"
        }
      }
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "outputs": [],
      "source": "SESS_IDX \u003d 7          # Index of recording session we will use. 0:8\nMAX_TRIAL_DUR \u003d 1.7   # This gets rid of about 7% of the slowest trials (long tail distribution)\nBIN_DURATION \u003d 0.020  # Width of window used to bin spikes, in seconds\nP_TRAIN \u003d 0.8         # Proportion of data used for training.",
      "metadata": {
        "pycharm": {
          "metadata": false,
          "name": "#%%\n",
          "is_executing": false
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": "### Load Spiking Data",
      "metadata": {
        "pycharm": {
          "metadata": false,
          "name": "#%% md\n"
        }
      }
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "outputs": [
        {
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m\u003cipython-input-10-7031d9310f8f\u003e\u001b[0m in \u001b[0;36m\u003cmodule\u003e\u001b[0;34m\u001b[0m\n\u001b[1;32m     24\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mY\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_ax_info\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mY_ax_info\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---\u003e 26\u001b[0;31m \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mY\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_ax_info\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mY_ax_info\u001b[0m \u001b[0;34m\u003d\u001b[0m \u001b[0mload_dat_with_vel_accel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdatadir\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mSESS_IDX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m\u003cipython-input-10-7031d9310f8f\u003e\u001b[0m in \u001b[0;36mload_dat_with_vel_accel\u001b[0;34m(datadir, sess_idx)\u001b[0m\n\u001b[1;32m      7\u001b[0m     \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mY\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_ax_info\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mY_ax_info\u001b[0m \u001b[0;34m\u003d\u001b[0m \u001b[0mload_joeyo_reaching\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdatadir\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msess_names\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0msess_idx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx_chunk\u001b[0m\u001b[0;34m\u003d\u001b[0m\u001b[0;34m\u0027mu_spiketimes\u0027\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----\u003e 9\u001b[0;31m     \u001b[0mtarg_ch_ix\u001b[0m \u001b[0;34m\u003d\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwhere\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mY_ax_info\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\u0027channel_names\u0027\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m\u003d\u003d\u001b[0m \u001b[0;34m\u0027TargetX\u0027\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m     \u001b[0mtarg_onset\u001b[0m \u001b[0;34m\u003d\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhstack\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdiff\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mY\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtarg_ch_ix\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m!\u003d\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m~/miniconda3/envs/indl/lib/python3.6/site-packages/jax/numpy/lax_numpy.py\u001b[0m in \u001b[0;36mwhere\u001b[0;34m(condition, x, y)\u001b[0m\n\u001b[1;32m    743\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mwhere\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcondition\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m\u003d\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m\u003d\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    744\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0my\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--\u003e 745\u001b[0;31m     \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Must use the three-argument form of where().\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    746\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0monp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0missubdtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_dtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcondition\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0monp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbool_\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    747\u001b[0m     \u001b[0mcondition\u001b[0m \u001b[0;34m\u003d\u001b[0m \u001b[0mlax\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mne\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcondition\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mzeros_like\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcondition\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: Must use the three-argument form of where()."
          ],
          "ename": "ValueError",
          "evalue": "Must use the three-argument form of where().",
          "output_type": "error"
        }
      ],
      "source": "from data.utils.fileio import load_joeyo_reaching\n\ndef load_dat_with_vel_accel(datadir, sess_idx, x_chunk\u003d\u0027mu_spiketimes\u0027, trial_dur\u003d1.7):\n    BEHAV_CHANS \u003d [\u0027CursorX\u0027, \u0027CursorY\u0027]\n    sess_names \u003d [\u0027indy_201\u0027 + _ for _ in [\u002760921_01\u0027, \u002760927_04\u0027, \u002760927_06\u0027, \u002760930_02\u0027, \u002760930_05\u0027, \u002761005_06\u0027,\n                                           \u002761006_02\u0027, \u002770124_01\u0027, \u002770127_03\u0027]]\n    X, Y, X_ax_info, Y_ax_info \u003d load_joeyo_reaching(datadir, sess_names[sess_idx], x_chunk\u003dx_chunk)\n\n    # Determine target onset times that we will keep.\n    targ_ch_ix \u003d onp.where(onp.in1d(Y_ax_info[\u0027channel_names\u0027], [\u0027TargetX\u0027, \u0027TargetY\u0027]))[0]\n    b_targ_onset \u003d onp.hstack((True, onp.any(onp.diff(Y[targ_ch_ix]) !\u003d 0, axis\u003d0)))\n    targ_onset_times \u003d Y_ax_info[\u0027timestamps\u0027][b_targ_onset]\n    targ_onset_times \u003d targ_onset_times[onp.hstack((onp.diff(targ_onset_times) \u003c\u003d trial_dur, False))]\n\n    # Slice Y to only keep required behaviour data (cursor position)\n    b_keep_y_chans \u003d onp.in1d(Y_ax_info[\u0027channel_names\u0027], BEHAV_CHANS)\n    Y \u003d Y[b_keep_y_chans, :]\n    Y_ax_info[\u0027channel_names\u0027] \u003d [_ for _ in Y_ax_info[\u0027channel_names\u0027] if _ in BEHAV_CHANS]\n\n    # Calculate discrete derivative and double-derivative to get velocity and acceleration.\n    vel \u003d onp.diff(Y, axis\u003d1)\n    vel \u003d onp.concatenate((vel[:, 0][:, None], vel),\n                         axis\u003d1)  # Assume velocity was constant across the first two samples.\n    accel \u003d onp.concatenate(([[0], [0]], np.diff(vel, axis\u003d1)), axis\u003d1)  # Assume accel was 0 in the first sample.\n    Y \u003d onp.concatenate((Y, vel, accel), axis\u003d0)\n    Y_ax_info[\u0027channel_names\u0027] +\u003d [\u0027VelX\u0027, \u0027VelY\u0027, \u0027AccX\u0027, \u0027AccY\u0027]\n\n    return X, Y, X_ax_info, Y_ax_info, targ_onset_times\n\nX, Y, X_ax_info, Y_ax_info, targ_times \u003d load_dat_with_vel_accel(datadir, SESS_IDX, trial_dur\u003dMAX_TRIAL_DUR)\n",
      "metadata": {
        "pycharm": {
          "metadata": false,
          "name": "#%%\n",
          "is_executing": false
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": "### Segment Into Trials ",
      "metadata": {
        "pycharm": {
          "metadata": false
        }
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "outputs": [],
      "source": "def bin_and_segment_spike_times(X, X_ax_info, targ_onset_times,\n                                nearest_bin_dur\u003d0.005, nearest_bin_step_dur\u003dNone, trial_dur\u003d1.7):\n\n    # We\u0027ll use an integer number of samples per bin: the next highest required to get the requested nearest_bin_dur\n    samps_per_bin \u003d int(onp.ceil(nearest_bin_dur * X_ax_info[\u0027fs\u0027]))\n    true_bin_dur \u003d samps_per_bin / X_ax_info[\u0027fs\u0027]\n    print(\"Actual bin duration: {}\".format(true_bin_dur))\n\n    # Similar for number of samples per bin-step.\n    if nearest_bin_step_dur is not None:\n        samps_per_step \u003d int(onp.ceil(nearest_bin_step_dur * X_ax_info[\u0027fs\u0027]))\n        print(\"Actual bin duration: {}\".format(samps_per_bin / X_ax_info[\u0027fs\u0027]))\n    else:\n        # No overlap\n        samps_per_step \u003d samps_per_bin\n\n    # Get the indices of each bin-start\n    bin_starts_idx \u003d onp.arange(0, X.shape[-1], samps_per_step)\n    b_full_bins \u003d bin_starts_idx \u003c\u003d (X.shape[-1] - samps_per_bin)\n    bin_starts_idx \u003d bin_starts_idx[b_full_bins]\n\n    # The next chunk of code counts the number of spikes in each bin.\n    # -Create array of indices to reslice the raster data\n    bin_ix \u003d onp.arange(samps_per_bin)[:, None] + bin_starts_idx[None, :]\n    # -Create buffer to hold the dense raster data\n    _temp \u003d onp.zeros(X[0].shape, dtype\u003dbool)\n    # -Preallocate _X to hold spike counts per bin\n    _X \u003d onp.zeros((len(bin_starts_idx), X.shape[0]), dtype\u003dnp.int32)\n    for chan_ix in range(X.shape[0]):\n        _X[:, chan_ix] \u003d np.sum(X[chan_ix].toarray(out\u003d_temp)[0][bin_ix], axis\u003d0)\n\n    # Now that our data are binned, let\u0027s slice it up into trials.\n    bins_per_trial \u003d int(trial_dur / true_bin_dur)\n    trial_X \u003d onp.zeros((len(targ_onset_times), bins_per_trial, _X.shape[-1]))\n\n    bin_stops_t \u003d X_ax_info[\u0027timestamps\u0027][bin_starts_idx + samps_per_bin - 1]\n    trial_starts_idx \u003d onp.searchsorted(bin_stops_t, targ_onset_times)\n    for trial_ix, t_start_idx in enumerate(trial_starts_idx):\n        trial_X[trial_ix] \u003d _X[t_start_idx:t_start_idx+bins_per_trial, :]\n\n    in_trial_tvec \u003d onp.arange(bins_per_trial) * true_bin_dur\n\n    return trial_X, in_trial_tvec, true_bin_dur\n\ntrial_X, trial_tvec, true_bin_dur \u003d bin_and_segment_spike_times(X, X_ax_info, targ_times,\n                                                                nearest_bin_dur\u003dBIN_DURATION, trial_dur\u003dMAX_TRIAL_DUR)",
      "metadata": {
        "pycharm": {
          "metadata": false,
          "name": "#%%\n"
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": "### Split Train and Validation",
      "metadata": {
        "pycharm": {
          "metadata": false
        }
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "outputs": [],
      "source": "from sklearn.model_selection import train_test_split\nX_train, X_valid, y_train, y_valid \u003d train_test_split(trial_X, y, train_size\u003dP_TRAIN)",
      "metadata": {
        "pycharm": {
          "metadata": false,
          "name": "#%%\n"
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": "### Visualize Data",
      "metadata": {
        "pycharm": {
          "metadata": false,
          "name": "#%% md\n"
        }
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "outputs": [],
      "source": "n_trials, n_timesteps, n_neurons \u003d trial_X.shape\nprint(\"trial_X has {} trials, {} bin-steps per trial, and {} neurons.\".format(n_trials, n_timesteps, n_neurons))\ntrial_idx \u003d onp.random.randint(0, n_trials-1, 1)[0]\nneuron_idx \u003d onp.random.randint(0, n_neurons-1, 4)\nplt.subplot(2, 1, 1)\nplt.plot(trial_tvec, trial_X[trial_idx, :, neuron_idx].T)\nplt.title(\"{} random neurons from trial {}\".format(len(neuron_idx), trial_idx))\nplt.xlabel(\u0027Time after target onset (s)\u0027)\nplt.ylabel(\u0027Spikes / Bin\u0027)\n\nfrom sklearn.decomposition import PCA\npca \u003d PCA(5)\npca.fit(onp.reshape(trial_X, (-1, n_neurons)))\nplt.subplot(2, 1, 2)\nplt.plot(100 - 100*pca.explained_variance_ratio_)\nplt.xlabel(\u0027Component ID\u0027)\nplt.ylabel(\u0027% VAF\u0027)\nplt.ylim([0, 100])\n\n# pca_filt_dat \u003d pca.inverse_transform(pca.transform(trial_X[trial_idx, :, :]))\n# plt.subplot(3, 1, 3)\n# plt.plot(trial_tvec, pca_filt_dat[:, neuron_idx], \u0027-\u0027)\nplt.show()",
      "metadata": {
        "pycharm": {
          "metadata": false,
          "name": "#%%\n"
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": "## LFADS - Latent Factor Analysis via Dynamical Systems\nThis notebook is based on a notebook found in in the [google-research/computation-thru-dynamics github repo](https://github.com/google-research/computation-thru-dynamics).\n\nIn the below diagram, the \"Generator\" is the cortical neuronal population, which is assumed to be a nonlinear,\ndynamical system that is modeled with a RNN. For now we will ignore the bottom \"Controller\" and \"Inferred inputs\" parts.\n\n![](https://raw.githubusercontent.com/google-research/computation-thru-dynamics/master/images/lfads_architecture_w_inferred_inputs_3.png)\n\nThe data is put through nonlinear, recurrent **encoders**, and this produces an **initial state distribution**,\nwhich is a per-trial mean and variance to produce random vectors to encode that trial.\nThis is exactly the same as the \u0027bottleneck\u0027 or \u0027latent variables\u0027 we saw in the variational auto-encoder tutorial.\n\nThe initial state of the generator is a randomly drawn vector from this distribution.\nThe **generator** marches through time and at each time point produces \"factors\" and \"rates\".\nThe \"factors\" are the low-dimensional neural state. The \"rates\" are the projection of the neural state into neuronal activations.\nThe rates are then used to parameterize a Poisson process to generate spikes.\nThe loss function compares the generated spike trains to the original input spike trains.\n",
      "metadata": {
        "pycharm": {
          "metadata": false,
          "name": "#%% md\n"
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": "## LFADS Hyperparameters",
      "metadata": {
        "pycharm": {
          "metadata": false
        }
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "outputs": [],
      "source": "BATCH_SIZE \u003d 4        # Number of trials in each training step during optimization\n\nN_RNN_UNITS \u003d 60      # Size of RNN output (state)\nEPOCHS \u003d 10           # Number of loops through the entire data set.\n\n# LFADS architecture - The size of the numbers is rather arbitrary, \n# but relatively small because we know the integrator RNN isn\u0027t too high \n# dimensional in its activity.\nENC_DIM \u003d 128         # encoder dim\nCON_DIM \u003d 128         # controller dim\nII_DIM \u003d 1            # inferred input dim, we know there is 1 dim in integrator RNN\nGEN_DIM \u003d 128         # generator dim, should be large enough to generate integrator RNN dynamics\nFACTORS_DIM \u003d 32      # factors dim, should be large enough to capture most variance of dynamics\n\n# Numerical stability\nVAR_MIN \u003d 0.001       # Minimal variance any gaussian can become.\n\n# Initial state prior parameters\n# the mean is set to zero in the code\nic_prior_var \u003d 0.1 # this is sigma^2 of uninformative prior\n\n# Optimization Hyperparameters\n# \nnum_batches \u003d int(n_trials * EPOCHS / BATCH_SIZE)  # how many batches do we train\nprint_every \u003d 20            # give information every so often\n\n# Learning rate HPs\nstep_size \u003d 0.05            # initial learning rate\ndecay_factor \u003d 0.99985      # learning rate decay param\ndecay_steps \u003d 1             # learning rate decay param\n\n# Regularization HPs\nkeep_rate \u003d 0.98            # dropout keep rate during training\nP_DROPOUT \u003d 0.05      # Proportion of units to set to 0 on each step.\nL2_REG \u003d 2.0e-5       # Parameter regularization strength.\n\n# Numerical stability HPs\nmax_grad_norm \u003d 10.0        # gradient clipping above this value\n\n# For randomization\nMAX_SEED_INT \u003d 10000000\n",
      "metadata": {
        "pycharm": {
          "metadata": false,
          "name": "#%%\n"
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": "### KL Warmup\nCopied straight from [LFADS tutorial optimize.py](https://github.com/google-research/computation-thru-dynamics/blob/master/lfads_tutorial/optimize.py).\n\n\u003e It turns out that the KL term can be a lot easier to optimize initially than learning how to\nreconstruct your data. This results in a pathological stoppage of training where the KL goes to\nnearly zero and training is broken there on out (as you cannot represent any a given trial from\nuninformative priors). One way out of this is to warmup the KL penality, starting it off with a\nweight term of 0 and then slowly building to 1, giving the reconstruction a chance to train a bit\nwithout the KL penalty messing things up.",
      "metadata": {
        "pycharm": {
          "metadata": false,
          "name": "#%% md\n"
        }
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "outputs": [],
      "source": "# The fact that the start and end values are required to be floats is something I need to fix.\nkl_warmup_start \u003d 500.0 # batch number to start kl warmup, explicitly float\nkl_warmup_end \u003d 1000.0  # batch number to be finished with kl warmup, explicitly float\nkl_min \u003d 0.01 # The minimum KL value, non-zero to make sure KL doesn\u0027t grow crazy before kicking in.\nkl_max \u003d 1.0\n\ndef get_kl_warmup_fun():\n    def kl_warmup(batch_idx):\n        progress_frac \u003d ((batch_idx - kl_warmup_start) /\n                         (kl_warmup_end - kl_warmup_start))\n        kl_warmup \u003d np.where(batch_idx \u003c kl_warmup_start, kl_min,\n                             (kl_max - kl_min) * progress_frac + kl_min)\n        return np.where(batch_idx \u003e kl_warmup_end, kl_max, kl_warmup)\n    return kl_warmup\n",
      "metadata": {
        "pycharm": {
          "metadata": false,
          "name": "#%%\n"
        }
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "outputs": [],
      "source": "# Plot the warmup function and the learning rate decay function.\nplt.figure(figsize\u003d(16,4))\nplt.subplot(121)\nx \u003d onp.arange(0, num_batches, print_every)\nkl_warmup_fun \u003d get_kl_warmup_fun()\nplt.plot(x, [kl_warmup_fun(i) for i in onp.arange(1, num_batches, print_every)]);\nplt.title(\u0027KL warmup function\u0027)\nplt.xlabel(\u0027Training batch\u0027)\n\nplt.subplot(122)\ndecay_fun \u003d optimizers.exponential_decay(step_size, decay_steps, decay_factor)                                                          \nplt.plot(x, [decay_fun(i) for i in range(1, num_batches, print_every)]);\nplt.title(\u0027learning rate function\u0027)\nplt.xlabel(\u0027Training batch\u0027) \n",
      "metadata": {
        "pycharm": {
          "metadata": false,
          "name": "#%%\n"
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": "## Randomization\n\u003e JAX uses its own setup to handle randomness and seeding the pseudo-random number generators.  You can read about it [here](https://github.com/google/jax/blob/master/README.md#random-numbers-are-different).",
      "metadata": {
        "pycharm": {
          "metadata": false,
          "name": "#%% md\n"
        }
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "outputs": [],
      "source": "key \u003d random.PRNGKey(onp.random.randint(0, MAX_SEED_INT))\n\ndef keygen(key, nkeys):\n    \"\"\"Generate randomness that JAX can use by splitting the JAX keys.\n    \n    Args:\n    key : the random.PRNGKey for JAX\n    nkeys : how many keys in key generator\n    \n    Returns:\n    2-tuple (new key for further generators, key generator)\n    \"\"\"\n    keys \u003d random.split(key, nkeys+1)\n    return keys[0], (k for k in keys[1:])",
      "metadata": {
        "pycharm": {
          "metadata": false,
          "name": "#%%\n"
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": "## Define LFADS functions\nThese functions are copied directly from the LFADS tutorial [here](https://github.com/google-research/computation-thru-dynamics/blob/master/lfads_tutorial/lfads.py),\nwith a bit of cleaning up and trimming comments for familiar things.",
      "metadata": {
        "pycharm": {
          "metadata": false,
          "name": "#%% md\n"
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": "### Methods for parameter random initialization",
      "metadata": {
        "pycharm": {
          "metadata": false,
          "name": "#%% md\n"
        }
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "outputs": [],
      "source": "# For linear/affine transforms,\n# y \u003d w x, y \u003d w x + b\n# with output size o, input size u, and keygen seed.\ndef linear_params(key, o, u, scale_factor\u003d1.0):\n    key, skeys \u003d keygen(key, 1)\n    scale_factor \u003d scale_factor / np.sqrt(u)\n    return {\u0027w\u0027: random.normal(next(skeys), (o, u)) * scale_factor}\n\n\ndef affine_params(key, o, u, scale_factor\u003d1.0):\n    key, skeys \u003d keygen(key, 1)\n    scale_factor \u003d scale_factor / np.sqrt(u)\n    return {\u0027w\u0027 : random.normal(next(skeys), (o, u)) * scale_factor,\n            \u0027b\u0027 : np.zeros((o,))}\n\n\ndef gru_params(key, n, u, ifactor\u003d1.0, hfactor\u003d1.0, hscale\u003d0.0):\n    \"\"\"Generate GRU parameters\n    \n    Arguments:\n    key: random.PRNGKey for random bits\n    n: hidden state size\n    u: input size\n    ifactor: scaling factor for input weights\n    hfactor: scaling factor for hidden -\u003e hidden weights\n    hscale: scale on h0 initial condition\n    \n    Returns:\n    a dictionary of parameters\n    \"\"\"\n    key, skeys \u003d keygen(key, 5)\n    ifactor \u003d ifactor / np.sqrt(u)\n    hfactor \u003d hfactor / np.sqrt(n)\n    \n    wRUH \u003d random.normal(next(skeys), (n+n,n)) * hfactor\n    wRUX \u003d random.normal(next(skeys), (n+n,u)) * ifactor\n    wRUHX \u003d np.concatenate([wRUH, wRUX], axis\u003d1)\n    \n    wCH \u003d random.normal(next(skeys), (n,n)) * hfactor\n    wCX \u003d random.normal(next(skeys), (n,u)) * ifactor\n    wCHX \u003d np.concatenate([wCH, wCX], axis\u003d1)\n    \n    return {\u0027h0\u0027 : random.normal(next(skeys), (n,)) * hscale,\n            \u0027wRUHX\u0027 : wRUHX,\n            \u0027wCHX\u0027 : wCHX,\n            \u0027bRU\u0027 : np.zeros((n+n,)),\n            \u0027bC\u0027 : np.zeros((n,))}\n",
      "metadata": {
        "pycharm": {
          "metadata": false,
          "name": "#%%\n"
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": "### Transformation implementations\nsigmoid $y\u003d\\frac{e^x}{1+e^x}\u003d\\frac{\\tanh(\\frac{x}{2}) + 1}{2}$\n\naffine transform $y \u003d w x + b$\n\n(batched) normed linear transform\n$y \u003d \\hat{w} x$,\nwhere $\\hat{w}_{ij} \u003d w_{ij} / |w_{i:}|$\n\nGRU\n\nDropout (batched)",
      "metadata": {
        "pycharm": {
          "metadata": false,
          "name": "#%% md\n"
        }
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "outputs": [],
      "source": "def sigmoid(vals):\n    return 0.5 * (np.tanh(vals / 2.) + 1)\n\n\ndef affine(params, x):\n    \"\"\"Implements y \u003d w x + b\"\"\"\n    return np.dot(params[\u0027w\u0027], x) + params[\u0027b\u0027]\nbatch_affine \u003d vmap(affine, in_axes\u003d(None, 0))\n\n\ndef normed_linear(params, x):\n    \"\"\"Implement y \u003d \\hat{w} x, where \\hat{w}_ij \u003d w_ij / |w_{i:}|, norm over j\n    \n    Arguments:\n    params: a dictionary of params\n    x: np array of input\n    \n    Returns:\n    np array of output\n    \"\"\"\n    w \u003d params[\u0027w\u0027]\n    w_row_norms \u003d np.sqrt(np.sum(w**2, axis\u003d1, keepdims\u003dTrue))\n    w \u003d w / w_row_norms\n    return np.dot(w, x)\nbatch_normed_linear \u003d vmap(normed_linear, in_axes\u003d(None, 0))\n\n\ndef gru(params, h, x, bfg\u003d0.5):\n    \"\"\"\n    Implement the GRU equations.\n    \n    Arguments:\n    params: dictionary of GRU parameters\n    h: np array of  hidden state\n    x: np array of input\n    bfg: bias on forget gate (useful for learning if \u003e 0.0)\n    \n    Returns:\n    np array of hidden state after GRU update\n    \"\"\"\n    hx \u003d np.concatenate([h, x], axis\u003d0)\n    ru \u003d sigmoid(np.dot(params[\u0027wRUHX\u0027], hx) + params[\u0027bRU\u0027])\n    r, u \u003d np.split(ru, 2, axis\u003d0)\n    rhx \u003d np.concatenate([r * h, x])\n    c \u003d np.tanh(np.dot(params[\u0027wCHX\u0027], rhx) + params[\u0027bC\u0027] + bfg)\n    return u * h + (1.0 - u) * c\n\n\ndef dropout(x, key, keep_rate):\n    \"\"\"Implement a dropout layer.\n    \n    Arguments:\n    x: np array to be dropped out\n    key: random.PRNGKey for random bits\n    keep_rate: dropout rate\n    \n    Returns:\n    np array of dropped out x\n    \"\"\"\n    # The shenanigans with np.where are to avoid having to re-jit if\n    # keep rate changes.\n    do_keep \u003d random.bernoulli(key, keep_rate, x.shape)\n    kept_rates \u003d np.where(do_keep, x / keep_rate, 0.0)\n    return np.where(keep_rate \u003c 1.0, kept_rates, x)\n\nbatch_dropout \u003d vmap(dropout, in_axes\u003d(0, 0, None))\n",
      "metadata": {
        "pycharm": {
          "metadata": false,
          "name": "#%%\n"
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": "### Runners\nThese functions run a layer/transformation over a sequence of data.",
      "metadata": {
        "pycharm": {
          "metadata": false,
          "name": "#%% md\n"
        }
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "outputs": [],
      "source": "def run_dropout(x_t, key, keep_rate):\n    \"\"\"\n    Run the dropout layer over additional dimensions, e.g. time.\n    \n    Arguments:\n    x_t: np array to be dropped out\n    key: random.PRNGKey for random bits\n    keep_rate: dropout rate\n    \n    Returns:\n    np array of dropped out x\n    \"\"\"\n    ntime \u003d x_t.shape[0]\n    keys \u003d random.split(key, ntime)\n    return batch_dropout(x_t, keys, keep_rate)\n\ndef run_rnn(params, rnn, x_t, h0\u003dNone):\n    \"\"\"\n    Run an RNN module forward in time.\n    \n    Arguments:\n    params: dictionary of RNN parameters\n    rnn: function for running RNN one step\n    x_t: np array data for RNN input with leading dim being time\n    h0: initial condition for running rnn, which overwrites param h0\n    \n    Returns:\n    np array of rnn applied to time data with leading dim being time\n    \"\"\"\n    h \u003d h0 if h0 is not None else params[\u0027h0\u0027]\n    h_t \u003d []\n    for x in x_t:\n        h \u003d rnn(params, h, x)\n        h_t.append(h)\n    return np.array(h_t)\n\n\ndef run_bidirectional_rnn(params, fwd_rnn, bwd_rnn, x_t):\n    \"\"\"\n    Run an RNN encoder backwards and forwards over some time series data.\n    \n    Arguments:\n    params: a dictionary of bidrectional RNN encoder parameters\n    fwd_rnn: function for running forward rnn encoding\n    bwd_rnn: function for running backward rnn encoding\n    x_t: np array data for RNN input with leading dim being time\n    \n    Returns:\n    tuple of np array concatenated forward, backward encoding, and\n      np array of concatenation of [forward_enc(T), backward_enc(1)]\n    \"\"\"\n    fwd_enc_t \u003d run_rnn(params[\u0027fwd_rnn\u0027], fwd_rnn, x_t)\n    bwd_enc_t \u003d np.flipud(run_rnn(params[\u0027bwd_rnn\u0027], bwd_rnn, np.flipud(x_t)))\n    full_enc \u003d np.concatenate([fwd_enc_t, bwd_enc_t], axis\u003d1)\n    enc_ends \u003d np.concatenate([bwd_enc_t[0], fwd_enc_t[-1]], axis\u003d1)\n    return full_enc, enc_ends\n",
      "metadata": {
        "pycharm": {
          "metadata": false,
          "name": "#%%\n"
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": "### Encoder Part + Decoder Part",
      "metadata": {
        "pycharm": {
          "metadata": false,
          "name": "#%% md\n"
        }
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "outputs": [],
      "source": "import indl.lfads.distributions as dists\n\ndef lfads_encode(params, lfads_hps, key, x_t, keep_rate):\n    \"\"\"Run the LFADS network from input to generator initial condition vars.\n    \n    Arguments:\n    params: a dictionary of LFADS parameters\n    lfads_hps: a dictionary of LFADS hyperparameters\n    key: random.PRNGKey for random bits\n    x_t: np array input for lfads with leading dimension being time\n    keep_rate: dropout keep rate\n    \n    Returns:\n    3-tuple of np arrays: generator initial condition mean, log variance\n      and also bidirectional encoding of x_t, with leading dim being time\n    \"\"\"\n    key, skeys \u003d keygen(key, 3)\n    \n    # Encode the input\n    x_t \u003d run_dropout(x_t, next(skeys), keep_rate)\n    con_ins_t, gen_pre_ics \u003d run_bidirectional_rnn(params[\u0027ic_enc\u0027], gru, gru, x_t)\n    # Push through to posterior mean and variance for initial conditions.\n    xenc_t \u003d dropout(con_ins_t, next(skeys), keep_rate)\n    gen_pre_ics \u003d dropout(gen_pre_ics, next(skeys), keep_rate)\n    ic_gauss_params \u003d affine(params[\u0027gen_ic\u0027], gen_pre_ics)\n    ic_mean, ic_logvar \u003d np.split(ic_gauss_params, 2, axis\u003d0)\n    return ic_mean, ic_logvar, xenc_t\n\n\ndef lfads_decode(params, lfads_hps, key, ic_mean, ic_logvar, xenc_t, keep_rate):\n    \"\"\"Run the LFADS network from latent variables to log rates.\n    \n    Arguments:\n    params: a dictionary of LFADS parameters\n    lfads_hps: a dictionary of LFADS hyperparameters\n    key: random.PRNGKey for random bits\n    ic_mean: np array of generator initial condition mean\n    ic_logvar: np array of generator initial condition log variance\n    xenc_t: np array bidirectional encoding of input (x_t) with leading dim\n      being time\n    keep_rate: dropout keep rate\n    \n    Returns:\n    7-tuple of np arrays all with leading dim being time,\n      controller hidden state, inferred input mean, inferred input log var,\n      generator hidden state, factors and log rates\n    \"\"\"\n    \n    ntime \u003d lfads_hps[\u0027ntimesteps\u0027]\n    key, skeys \u003d keygen(key, 1+2*ntime)\n    \n    # Since the factors feed back to the controller,\n    #    factors_{t-1} -\u003e controller_t -\u003e sample_t -\u003e generator_t -\u003e factors_t\n    # is really one big loop and therefor one RNN.\n    c \u003d c0 \u003d params[\u0027con\u0027][\u0027h0\u0027]\n    g \u003d g0 \u003d dists.diag_gaussian_sample(next(skeys), ic_mean,\n                                      ic_logvar, lfads_hps[\u0027var_min\u0027])\n    f \u003d f0 \u003d np.zeros((lfads_hps[\u0027factors_dim\u0027],))\n    c_t \u003d []\n    ii_mean_t \u003d []\n    ii_logvar_t \u003d []\n    ii_t \u003d []\n    gen_t \u003d []\n    factor_t \u003d []\n    for xenc in xenc_t:\n        cin \u003d np.concatenate([xenc, f], axis\u003d0)\n        c \u003d gru(params[\u0027con\u0027], c, cin)\n        cout \u003d affine(params[\u0027con_out\u0027], c)\n        ii_mean, ii_logvar \u003d np.split(cout, 2, axis\u003d0) # inferred input params\n        ii \u003d dists.diag_gaussian_sample(next(skeys), ii_mean,\n                                        ii_logvar, lfads_hps[\u0027var_min\u0027])\n        g \u003d gru(params[\u0027gen\u0027], g, ii)\n        g \u003d dropout(g, next(skeys), keep_rate)\n        f \u003d normed_linear(params[\u0027factors\u0027], g)\n        # Save everything.\n        c_t.append(c)\n        ii_t.append(ii)\n        gen_t.append(g)\n        ii_mean_t.append(ii_mean)\n        ii_logvar_t.append(ii_logvar)\n        factor_t.append(f)\n    \n    c_t \u003d np.array(c_t)\n    ii_t \u003d np.array(ii_t)\n    gen_t \u003d np.array(gen_t)\n    ii_mean_t \u003d np.array(ii_mean_t)\n    ii_logvar_t \u003d np.array(ii_logvar_t)\n    factor_t \u003d np.array(factor_t)\n    lograte_t \u003d batch_affine(params[\u0027logrates\u0027], factor_t)\n    \n    return c_t, ii_mean_t, ii_logvar_t, ii_t, gen_t, factor_t, lograte_t\n\n\ndef lfads(params, lfads_hps, key, x_t, keep_rate):\n    \"\"\"Run the LFADS network from input to output.\n    \n    Arguments:\n    params: a dictionary of LFADS parameters\n    lfads_hps: a dictionary of LFADS hyperparameters\n    key: random.PRNGKey for random bits\n    x_t: np array of input with leading dim being time\n    keep_rate: dropout keep rate\n    \n    Returns:\n    A dictionary of np arrays of all LFADS values of interest.\n    \"\"\"\n    \n    key, skeys \u003d keygen(key, 2)\n    \n    ic_mean, ic_logvar, xenc_t \u003d \\\n        lfads_encode(params, lfads_hps, next(skeys), x_t, keep_rate)\n    \n    c_t, ii_mean_t, ii_logvar_t, ii_t, gen_t, factor_t, lograte_t \u003d \\\n        lfads_decode(params, lfads_hps, next(skeys), ic_mean, ic_logvar,\n                     xenc_t, keep_rate)\n    \n    # As this is tutorial code, we\u0027re passing everything around.\n    return {\u0027xenc_t\u0027 : xenc_t, \u0027ic_mean\u0027 : ic_mean, \u0027ic_logvar\u0027 : ic_logvar,\n            \u0027ii_t\u0027 : ii_t, \u0027c_t\u0027 : c_t, \u0027ii_mean_t\u0027 : ii_mean_t,\n            \u0027ii_logvar_t\u0027 : ii_logvar_t, \u0027gen_t\u0027 : gen_t, \u0027factor_t\u0027 : factor_t,\n            \u0027lograte_t\u0027 : lograte_t}\n\n\nlfads_encode_jit \u003d jit(lfads_encode)\nlfads_decode_jit \u003d jit(lfads_decode, static_argnums\u003d(1,))\n\n# Batching accomplished by vectorized mapping.\n# We simultaneously map over random keys for forward-pass randomness\n# and inputs for batching.\nbatch_lfads \u003d vmap(lfads, in_axes\u003d(None, None, 0, 0, None))\n",
      "metadata": {
        "pycharm": {
          "metadata": false,
          "name": "#%%\n"
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": "### Training Functions\n",
      "metadata": {
        "pycharm": {
          "metadata": false,
          "name": "#%% md\n"
        }
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "outputs": [],
      "source": "def optimize_lfads_core(key, batch_idx_start, num_batches,\n                        update_fun, kl_warmup_fun,\n                        opt_state, lfads_hps, lfads_opt_hps, train_data):\n    \"\"\"Make gradient updates to the LFADS model.\n    \n    Uses lax.fori_loop instead of a Python loop to reduce JAX overhead. This \n    loop will be jit\u0027d and run on device.\n    \n    Arguments:\n    init_params: a dict of parameters to be trained\n    batch_idx_start: Where are we in the total number of batches\n    num_batches: how many batches to run\n    update_fun: the function that changes params based on grad of loss\n    kl_warmup_fun: function to compute the kl warmup\n    opt_state: the jax optimizer state, containing params and opt state\n    lfads_hps: dict of lfads model HPs\n    lfads_opt_hps: dict of optimization HPs\n    train_data: nexamples x time x ndims np array of data for training\n    \n    Returns:\n    opt_state: the jax optimizer state, containing params and optimizer state\"\"\"\n    \n    key, dkeyg \u003d keygen(key, num_batches) # data\n    key, fkeyg \u003d keygen(key, num_batches) # forward pass\n    \n    # Begin optimziation loop. Explicitly avoiding a python for-loop\n    # so that jax will not trace it for the sake of a gradient we will not use.  \n    def run_update(batch_idx, opt_state):\n        kl_warmup \u003d kl_warmup_fun(batch_idx)\n        didxs \u003d random.randint(next(dkeyg), [lfads_hps[\u0027batch_size\u0027]], 0,\n                               train_data.shape[0])\n        x_bxt \u003d train_data[didxs].astype(np.float32)\n        opt_state \u003d update_fun(batch_idx, opt_state, lfads_hps, lfads_opt_hps,\n                               next(fkeyg), x_bxt, kl_warmup)\n        return opt_state\n    \n    lower \u003d batch_idx_start\n    upper \u003d batch_idx_start + num_batches\n    return lax.fori_loop(lower, upper, run_update, opt_state)\n\n\noptimize_lfads_core_jit \u003d jit(optimize_lfads_core, static_argnums\u003d(2,3,4,6,7))\n\n\ndef optimize_lfads(key, init_params, lfads_hps, lfads_opt_hps,\n                   train_data, eval_data):\n    \"\"\"Optimize the LFADS model and print batch based optimization data.\n    \n    This loop is at the cpu nonjax-numpy level.\n    \n    Arguments:\n    init_params: a dict of parameters to be trained\n    lfads_hps: dict of lfads model HPs\n    lfads_opt_hps: dict of optimization HPs\n    train_data: nexamples x time x ndims np array of data for training\n    \n    Returns:\n    a dictionary of trained parameters\"\"\"\n    \n    # Begin optimziation loop.\n    all_tlosses \u003d []\n    all_elosses \u003d []\n    \n    # Build some functions used in optimization.\n    kl_warmup_fun \u003d get_kl_warmup_fun(lfads_opt_hps)\n    decay_fun \u003d optimizers.exponential_decay(lfads_opt_hps[\u0027step_size\u0027],\n                                           lfads_opt_hps[\u0027decay_steps\u0027],\n                                           lfads_opt_hps[\u0027decay_factor\u0027])\n    \n    opt_init, opt_update, get_params \u003d optimizers.adam(step_size\u003ddecay_fun,\n                                                     b1\u003dlfads_opt_hps[\u0027adam_b1\u0027],\n                                                     b2\u003dlfads_opt_hps[\u0027adam_b2\u0027],\n                                                     eps\u003dlfads_opt_hps[\u0027adam_eps\u0027])\n    opt_state \u003d opt_init(init_params)\n    \n    def update_w_gc(i, opt_state, lfads_hps, lfads_opt_hps, key, x_bxt,\n                  kl_warmup):\n        \"\"\"Update fun for gradients, includes gradient clipping.\"\"\"\n        params \u003d get_params(opt_state)\n        grads \u003d grad(lfads.lfads_training_loss)(params, lfads_hps, key, x_bxt,\n                                                kl_warmup,\n                                                lfads_opt_hps[\u0027keep_rate\u0027])\n        clipped_grads \u003d optimizers.clip_grads(grads, lfads_opt_hps[\u0027max_grad_norm\u0027])\n        return opt_update(i, clipped_grads, opt_state)\n    \n    # Run the optimization, pausing every so often to collect data and\n    # print status.\n    batch_size \u003d lfads_hps[\u0027batch_size\u0027]\n    num_batches \u003d lfads_opt_hps[\u0027num_batches\u0027]\n    print_every \u003d lfads_opt_hps[\u0027print_every\u0027]\n    num_opt_loops \u003d int(num_batches / print_every)\n    params \u003d get_params(opt_state)\n    for oidx in range(num_opt_loops):\n        batch_idx_start \u003d oidx * print_every\n        start_time \u003d time.time()\n        key, tkey, dtkey, dekey \u003d random.split(random.fold_in(key, oidx), 4)\n        opt_state \u003d optimize_lfads_core_jit(tkey, batch_idx_start,\n                                            print_every, update_w_gc, kl_warmup_fun,\n                                            opt_state, lfads_hps, lfads_opt_hps,\n                                            train_data)\n        batch_time \u003d time.time() - start_time\n        \n        # Losses\n        params \u003d get_params(opt_state)\n        batch_pidx \u003d batch_idx_start + print_every\n        kl_warmup \u003d kl_warmup_fun(batch_idx_start)\n        # Training loss\n        didxs \u003d onp.random.randint(0, train_data.shape[0], batch_size)\n        x_bxt \u003d train_data[didxs].astype(onp.float32)\n        tlosses \u003d lfads.lfads_losses_jit(params, lfads_hps, dtkey, x_bxt,\n                                         kl_warmup, 1.0)\n        \n        # Evaluation loss\n        didxs \u003d onp.random.randint(0, eval_data.shape[0], batch_size)\n        ex_bxt \u003d eval_data[didxs].astype(onp.float32)\n        elosses \u003d lfads.lfads_losses_jit(params, lfads_hps, dekey, ex_bxt,\n                                         kl_warmup, 1.0)\n        # Saving, printing.\n        all_tlosses.append(tlosses)\n        all_elosses.append(elosses)\n        s \u003d \"Batches {}-{} in {:0.2f} sec, Step size: {:0.5f}, Training loss {:0.0f}, Eval loss {:0.0f}\"\n        print(s.format(batch_idx_start+1, batch_pidx, batch_time,\n                       decay_fun(batch_pidx), tlosses[\u0027total\u0027], elosses[\u0027total\u0027]))\n        \n        tlosses_thru_training \u003d utils.merge_losses_dicts(all_tlosses)\n        elosses_thru_training \u003d utils.merge_losses_dicts(all_elosses)\n        optimizer_details \u003d {\u0027tlosses\u0027 : tlosses_thru_training,\n                             \u0027elosses\u0027 : elosses_thru_training}\n    \n    return params, optimizer_details\n",
      "metadata": {
        "pycharm": {
          "metadata": false,
          "name": "#%%\n"
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": "### Calculate Losses",
      "metadata": {
        "pycharm": {
          "metadata": false,
          "name": "#%% md\n"
        }
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "outputs": [],
      "source": "def lfads_losses(params, lfads_hps, key, x_bxt, kl_scale, keep_rate):\n    \"\"\"Compute the training loss of the LFADS autoencoder\n    \n    Arguments:\n    params: a dictionary of LFADS parameters\n    lfads_hps: a dictionary of LFADS hyperparameters\n    key: random.PRNGKey for random bits\n    x_bxt: np array of input with leading dims being batch and time\n    keep_rate: dropout keep rate\n    kl_scale: scale on KL\n    \n    Returns:\n    a dictionary of all losses, including the key \u0027total\u0027 used for optimization\n    \"\"\"\n    \n    B \u003d lfads_hps[\u0027batch_size\u0027]\n    key, skeys \u003d keygen(key, 2)\n    keys \u003d random.split(next(skeys), B)\n    lfads \u003d batch_lfads(params, lfads_hps, keys, x_bxt, keep_rate)\n    \n    # Sum over time and state dims, average over batch.\n    # KL - g0\n    ic_post_mean_b \u003d lfads[\u0027ic_mean\u0027]\n    ic_post_logvar_b \u003d lfads[\u0027ic_logvar\u0027]\n    kl_loss_g0_b \u003d dists.batch_kl_gauss_gauss(ic_post_mean_b, ic_post_logvar_b,\n                                            params[\u0027ic_prior\u0027],\n                                            lfads_hps[\u0027var_min\u0027])\n    kl_loss_g0_prescale \u003d np.sum(kl_loss_g0_b) / B  \n    kl_loss_g0 \u003d kl_scale * kl_loss_g0_prescale\n    \n    # KL - Inferred input\n    ii_post_mean_bxt \u003d lfads[\u0027ii_mean_t\u0027]\n    ii_post_var_bxt \u003d lfads[\u0027ii_logvar_t\u0027]\n    keys \u003d random.split(next(skeys), B)\n    kl_loss_ii_b \u003d dists.batch_kl_gauss_ar1(keys, ii_post_mean_bxt,\n                                          ii_post_var_bxt, params[\u0027ii_prior\u0027],\n                                          lfads_hps[\u0027var_min\u0027])\n    kl_loss_ii_prescale \u003d np.sum(kl_loss_ii_b) / B\n    kl_loss_ii \u003d kl_scale * kl_loss_ii_prescale\n    \n    # Log-likelihood of data given latents.\n    lograte_bxt \u003d lfads[\u0027lograte_t\u0027]\n    log_p_xgz \u003d np.sum(dists.poisson_log_likelihood(x_bxt, lograte_bxt)) / B\n    \n    # L2\n    l2reg \u003d lfads_hps[\u0027l2reg\u0027]\n    l2_loss \u003d l2reg * optimizers.l2_norm(params)**2\n    \n    loss \u003d -log_p_xgz + kl_loss_g0 + kl_loss_ii + l2_loss\n    all_losses \u003d {\u0027total\u0027 : loss, \u0027nlog_p_xgz\u0027 : -log_p_xgz,\n                \u0027kl_g0\u0027 : kl_loss_g0, \u0027kl_g0_prescale\u0027 : kl_loss_g0_prescale,\n                \u0027kl_ii\u0027 : kl_loss_ii, \u0027kl_ii_prescale\u0027 : kl_loss_ii_prescale,\n                \u0027l2\u0027 : l2_loss}\n    return all_losses\n\n\ndef lfads_training_loss(params, lfads_hps, key, x_bxt, kl_scale, keep_rate):\n    \"\"\"Pull out the total loss for training.\n    \n    Arguments:\n    params: a dictionary of LFADS parameters\n    lfads_hps: a dictionary of LFADS hyperparameters\n    key: random.PRNGKey for random bits\n    x_bxt: np array of input with leading dims being batch and time\n    keep_rate: dropout keep rate\n    kl_scale: scale on KL\n    \n    Returns:\n    return the total loss for optimization\n    \"\"\"\n    losses \u003d lfads_losses(params, lfads_hps, key, x_bxt, kl_scale, keep_rate)\n    return losses[\u0027total\u0027]\n\nfrom indl.lfads import utils\ndef posterior_sample_and_average(params, lfads_hps, key, x_txd):\n    \"\"\"Get the denoised lfad inferred values by posterior sample and average.\n    \n    Arguments:\n    params: dictionary of lfads parameters\n    lfads_hps: dict of LFADS hyperparameters\n    key: JAX random state\n    x_txd: 2d np.array time by dim trial to denoise\n    \n    Returns: \n    LFADS dictionary of inferred values, averaged over randomness.\n    \"\"\"\n    batch_size \u003d lfads_hps[\u0027batch_size\u0027]\n    skeys \u003d random.split(key, batch_size)  \n    x_bxtxd \u003d np.repeat(np.expand_dims(x_txd, axis\u003d0), batch_size, axis\u003d0)\n    lfads_dict \u003d batch_lfads(params, lfads_hps, skeys, x_bxtxd, 1.0)\n    return utils.average_lfads_batch(lfads_dict)\n",
      "metadata": {
        "pycharm": {
          "metadata": false,
          "name": "#%%\n"
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": "### Compilation\n\u003eJIT functions are orders of magnitude faster.  The first time you use them,\nthey will take a couple of minutes to compile, then the second time you use\nthem, they will be blindingly fast.\n",
      "metadata": {
        "pycharm": {
          "metadata": false,
          "name": "#%% md\n"
        }
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "outputs": [],
      "source": "# The static_argnums is telling JAX to ignore the lfads_hps dictionary,\n# which means you\u0027ll have to pay attention if you change the params.\n# How does one force a recompile?\nbatch_lfads_jit \u003d jit(batch_lfads, static_argnums\u003d(1,))\nlfads_losses_jit \u003d jit(lfads_losses, static_argnums\u003d(1,))\nlfads_training_loss_jit \u003d jit(lfads_training_loss, static_argnums\u003d(1,))\nposterior_sample_and_average_jit \u003d jit(posterior_sample_and_average, static_argnums\u003d(1,))\n",
      "metadata": {
        "pycharm": {
          "metadata": false,
          "name": "#%%\n"
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": "## Train LFADS",
      "metadata": {
        "pycharm": {
          "metadata": false,
          "name": "#%% md\n"
        }
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "outputs": [],
      "source": "key \u003d random.PRNGKey(onp.random.randint(0, MAX_SEED_INT))\ntrained_params, opt_details \u003d \\\n    optimize_lfads(key, init_params, lfads_hps, lfads_opt_hps, train_data, eval_data)\n",
      "metadata": {
        "pycharm": {
          "metadata": false,
          "name": "#%%\n"
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": "### Plot training details",
      "metadata": {
        "pycharm": {
          "metadata": false,
          "name": "#%% md\n"
        }
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "outputs": [],
      "source": "",
      "metadata": {
        "pycharm": {
          "metadata": false,
          "name": "#%%\n"
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": "### Save the result",
      "metadata": {
        "pycharm": {
          "metadata": false,
          "name": "#%% md\n"
        }
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "outputs": [],
      "source": "",
      "metadata": {
        "pycharm": {
          "metadata": false,
          "name": "#%%\n"
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": "## Inspect Model",
      "metadata": {
        "pycharm": {
          "metadata": false,
          "name": "#%% md\n"
        }
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "outputs": [],
      "source": "",
      "metadata": {
        "pycharm": {
          "metadata": false,
          "name": "#%%"
        }
      }
    }
  ]
}