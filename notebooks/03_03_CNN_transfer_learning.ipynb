{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Transfer Learning\n",
    "This notebook is part of the [SachsLab Workshop for Intracranial Neurophysiology and Deep Learning](https://github.com/SachsLab/IntracranialNeurophysDL).\n",
    "https://www.tensorflow.org/alpha/tutorials/images/transfer_learning\n",
    "* Freeze layers, update only first layer, then unfreeze and update\n",
    "\n",
    "Run the first two cells to normalize Local / Colab environments, then proceed below for the lesson."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import os\n",
    "try:\n",
    "    # See if we are running on google.colab\n",
    "    import google.colab\n",
    "    from google.colab import files\n",
    "    os.chdir('..')\n",
    "    if not (Path.home() / '.kaggle').is_dir():\n",
    "        # Configure kaggle\n",
    "        files.upload()  # Find the kaggle.json file in your ~/.kaggle directory.\n",
    "        !pip install -q kaggle\n",
    "        !mkdir -p ~/.kaggle\n",
    "        !mv kaggle.json ~/.kaggle/\n",
    "        !chmod 600 ~/.kaggle/kaggle.json\n",
    "    if Path.cwd().stem != 'IntracranialNeurophysDL':\n",
    "        if not (Path.cwd() / 'IntracranialNeurophysDL').is_dir():\n",
    "            # Download the workshop repo and change to its directory\n",
    "            !git clone --recursive https://github.com/SachsLab/IntracranialNeurophysDL.git\n",
    "        os.chdir('IntracranialNeurophysDL')\n",
    "    IN_COLAB = True\n",
    "    # Setup tensorflow 2.0\n",
    "    !pip install -q tensorflow-gpu==2.0.0-alpha0\n",
    "except ModuleNotFoundError:\n",
    "    IN_COLAB = False\n",
    "    import sys\n",
    "    if Path.cwd().stem == 'notebooks':\n",
    "        os.chdir(Path.cwd().parent)\n",
    "    # Make sure the kaggle executable is on the PATH\n",
    "    os.environ['PATH'] = os.environ['PATH'] + ';' + str(Path(sys.executable).parent / 'Scripts')\n",
    "\n",
    "# Try to clear any logs from previous runs\n",
    "if (Path.cwd() / 'logs').is_dir():\n",
    "    import shutil\n",
    "    try:\n",
    "        shutil.rmtree(str(Path.cwd() / 'logs'))\n",
    "    except PermissionError:\n",
    "        print(\"Unable to remove logs directory.\")\n",
    "\n",
    "# Additional imports\n",
    "import tensorflow as tf\n",
    "import datetime\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from indl import enable_plotly_in_cell\n",
    "%load_ext tensorboard.notebook\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "#Reset the model weights to what they were before training\n",
    "model.set_weights(initial_weights)\n",
    "\n",
    "def replace_input_layers(old_model, new_input_channels):\n",
    "    inputs = tf.keras.layers.Input(shape=(None, new_input_channels))\n",
    "    _y = tf.keras.layers.Conv1D(N_SOURCES, 1, use_bias=False)(inputs)  # Spatial filter.\n",
    "    for layer_ix, layer in enumerate(old_model.layers):\n",
    "        if layer_ix > 1:\n",
    "            _y = layer(_y)\n",
    "    return tf.keras.Model(inputs, _y)\n",
    "\n",
    "# Choose another participant, replace the input layers to match new input size, and retrain the model.\n",
    "X, Y, ax_info = load_faces_houses(datadir, 'de', feature_set='full')\n",
    "ds_train, ds_valid, n_train = get_ds_train_valid(X, Y, p_train=PTRAIN, batch_size=BATCH_SIZE, max_offset=100)\n",
    "xfer_model = replace_input_layers(model, X.shape[-1])\n",
    "xfer_model.compile(loss='sparse_categorical_crossentropy', optimizer='Nadam', metrics=['accuracy'])\n",
    "xfer_model.summary()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "history = xfer_model.fit(x=ds_train,  \n",
    "                         epochs=N_EPOCHS, \n",
    "                         validation_data=ds_valid,\n",
    "                         verbose=1)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Iterate through each participant\n",
    "participant_names = ['ja', 'ca', 'wc', 'de', 'zt', 'fp']  # , 'mv'\n",
    "INPUT_EPOCHS = 100  # Can use a lot here because it's quite fast.\n",
    "FINE_TUNE_EPOCHS = 50\n",
    "BATCH_SIZE = 20\n",
    "input_hists = []\n",
    "full_hists = []\n",
    "\n",
    "for p_ix, p_name in enumerate(participant_names):\n",
    "    \n",
    "    # Load their data\n",
    "    X, Y, ax_info = load_faces_houses(datadir, p_name, feature_set='full')\n",
    "    ds_train, ds_valid, n_train = get_ds_train_valid(X, Y, p_train=PTRAIN, batch_size=BATCH_SIZE, max_offset=100)\n",
    "    \n",
    "    # Make a new model with the proper input size\n",
    "    xfer_model = replace_input_layers(xfer_model, X.shape[-1])\n",
    "    \n",
    "    # Freeze layers other than input layers.\n",
    "    for layer_ix, layer in enumerate(xfer_model.layers):\n",
    "        if layer_ix > 1:\n",
    "            layer.trainable=False\n",
    "    \n",
    "    # Train for a INPUT_EPOCHS epochs to update input layers only\n",
    "    xfer_model.compile(loss='sparse_categorical_crossentropy', optimizer='Nadam', metrics=['accuracy'])\n",
    "    temp = xfer_model.fit(x=ds_train,  \n",
    "                          epochs=INPUT_EPOCHS, \n",
    "                          validation_data=ds_valid,\n",
    "                          verbose=1)\n",
    "    input_hists.append(temp)\n",
    "    \n",
    "    # Unfreeze all layers\n",
    "    for layer_ix, layer in enumerate(xfer_model.layers):\n",
    "        layer.trainable=True\n",
    "    \n",
    "    # Fine-tuning: Train for longer at a much lower rate\n",
    "    xfer_model.compile(loss='sparse_categorical_crossentropy', optimizer=tf.keras.optimizers.Nadam(lr=1e-5), metrics=['accuracy'])\n",
    "    temp = xfer_model.fit(x=ds_train,  \n",
    "                          epochs=FINE_TUNE_EPOCHS, \n",
    "                          validation_data=ds_valid,\n",
    "                          verbose=1)\n",
    "    full_hists.append(temp)\n",
    "\n",
    "# Save the model\n",
    "xfer_model.save(datadir / 'converted' / 'faces_basic' / 'xfer_model_full.h5')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Transfer the model to unseen data"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Save what we just trained\n",
    "if IN_COLAB:\n",
    "    files.download(datadir / 'converted' / 'faces_basic' / 'xfer_model_full.h5')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "X, Y, ax_info = load_faces_houses(datadir, 'mv', feature_set='full')\n",
    "ds_train, ds_valid, n_train = get_ds_train_valid(X, Y, p_train=0.5, batch_size=BATCH_SIZE, max_offset=100)\n",
    "\n",
    "# Make a new model with the proper input size\n",
    "xfer_model = replace_input_layers(xfer_model, X.shape[-1])\n",
    "xfer_model.compile(loss='sparse_categorical_crossentropy', optimizer='Nadam', metrics=['accuracy'])"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "log_dir = Path.cwd() / \"logs\" / datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "tensorboard_cb = tf.keras.callbacks.TensorBoard(str(log_dir), histogram_freq=1)\n",
    "history = xfer_model.fit(x=ds_train,  \n",
    "                         epochs=50,\n",
    "                         validation_data=ds_valid,\n",
    "                         callbacks=[tensorboard_cb],\n",
    "                         verbose=1)\n",
    "%tensorboard --logdir={str(log_dir)}\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  },
  "kernelspec": {
   "name": "python3",
   "language": "python",
   "display_name": "Python 3"
  },
  "pycharm": {
   "stem_cell": {
    "cell_type": "raw",
    "source": [],
    "metadata": {
     "collapsed": false
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}