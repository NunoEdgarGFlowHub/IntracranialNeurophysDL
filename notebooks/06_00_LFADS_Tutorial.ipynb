{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "06_00_LFADS_Tutorial.ipynb",
      "version": "0.3.2",
      "provenance": []
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 2
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython2",
      "version": "2.7.6"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "collapsed": false,
        "id": "jf0l50IDci45",
        "colab_type": "text"
      },
      "source": [
        "Note this is a copy of [the original tutorial](https://github.com/google-research/computation-thru-dynamics/blob/master/notebooks/LFADS%20Tutorial.ipynb)\n",
        "adapted to [run on Google Colab](https://colab.research.google.com/github/SachsLab/IntracranialNeurophysDL/blob/master/notebooks/06_00_LFADS_Tutorial.ipynb).\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lmCMfyHXcmUy",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 108
        },
        "outputId": "d2719b64-168a-4f1e-c24f-fdc409b17e62"
      },
      "source": [
        "!pip install --upgrade -q https://storage.googleapis.com/jax-wheels/cuda$(echo $CUDA_VERSION | sed -e 's/\\.//' -e 's/\\..*//')/jaxlib-latest-cp36-none-linux_x86_64.whl\n",
        "!pip install --upgrade -q jax"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\u001b[K     |████████████████████████████████| 39.6MB 49.5MB/s \n",
            "\u001b[K     |████████████████████████████████| 184kB 21.1MB/s \n",
            "\u001b[K     |████████████████████████████████| 61kB 24.2MB/s \n",
            "\u001b[?25h  Building wheel for jax (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for opt-einsum (setup.py) ... \u001b[?25l\u001b[?25hdone\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1MaZ5WsrcnAw",
        "colab_type": "text"
      },
      "source": [
        "# Run the LFADS algorithm on an RNN that integrates white noise.\n",
        "\n",
        "The goal of this tutorial is to learn about LFADS by running the algorithm on a simple data generator, a vanilla recurrent neural network (RNN) that was [trained to integrate a white noise input](https://github.com/google-research/computation-thru-dynamics/blob/master/notebooks/Integrator%20RNN%20Tutorial.ipynb).  Running LFADS on this integrator RNN will infer two things:\n",
        "1. the underlying hidden state of the integrator RNN\n",
        "2. the white noise input to the integrator RNN.\n",
        "\n",
        "Doing this will exercise the more complex LFADS architecture that is shown in Figure 5 of the [LFADS paper](https://rdcu.be/6Wji). It's pretty important that you have read at least the introduction of the paper, otherwise you won't understand *why* we are doing what we are doing.\n",
        "\n",
        "In this tutorial we do a few things:\n",
        "1. Load the integrator RNN data and \"spikify\" it by treating the hidden units as nonhomogeneous Poisson processes.\n",
        "2. Explain a bit of the of the LFADS architecture and highlight some of the relevant hyperparameters.\n",
        "3. Train the LFADS system on the spikified integrator RNN hidden states.\n",
        "4. Plot a whole bunch of training plots and LFADS outputs!\n",
        "\n",
        "If you make it through this tutorial and understand everything in it, it is *highly* likely you'll be able to run LFADS on your own data."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "collapsed": false,
        "id": "h1PY8jFici47",
        "colab_type": "text"
      },
      "source": [
        "#### Copyright 2019 Google LLC\n",
        "\n",
        "Licensed under the Apache License, Version 2.0 (the \"License\");\n",
        "you may not use this file except in compliance with the License.\n",
        "You may obtain a copy of the License at\n",
        "\n",
        "     https://www.apache.org/licenses/LICENSE-2.0\n",
        "\n",
        "Unless required by applicable law or agreed to in writing, software\n",
        "distributed under the License is distributed on an \"AS IS\" BASIS,\n",
        "WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
        "See the License for the specific language governing permissions and\n",
        "limitations under the License."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "collapsed": false,
        "id": "oqik3MMJci48",
        "colab_type": "text"
      },
      "source": [
        "### Import the tutorial code.\n",
        "\n",
        "If you are going to actually run the tutorial, you have to install JAX, download the computation thru dynamics github repo, and modify a path. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "pycharm": {
          "name": "#%%\n"
        },
        "id": "gObBt0KKci48",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Numpy, JAX, Matplotlib and h5py should all be correctly installed and on the python path.\n",
        "from __future__ import print_function, division, absolute_import\n",
        "\n",
        "import datetime\n",
        "import h5py\n",
        "import jax.numpy as np\n",
        "from jax import random\n",
        "from jax.experimental import optimizers\n",
        "from jax.config import config\n",
        "#config.update(\"jax_debug_nans\", True) # Useful for finding numerical errors\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as onp  # original CPU-backed NumPy\n",
        "import scipy.signal\n",
        "import scipy.stats\n",
        "import os\n",
        "import sys\n",
        "import time\n",
        "from pathlib import Path"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "pycharm": {
          "name": "#%%\n"
        },
        "id": "WZF2cAokci4_",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 145
        },
        "outputId": "851a6205-d10e-4561-cae4-f179adc96719"
      },
      "source": [
        "if not (Path.cwd().parent / 'repo').is_dir():\n",
        "    !git clone --recursive https://github.com/google-research/computation-thru-dynamics.git ../home/repo\n",
        "sys.path.append(os.path.join('..', 'home', 'repo'))\n",
        "import lfads_tutorial.lfads as lfads\n",
        "import lfads_tutorial.plotting as plotting\n",
        "import lfads_tutorial.utils as utils\n",
        "from lfads_tutorial.optimize import optimize_lfads, get_kl_warmup_fun"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Cloning into '../home/repo'...\n",
            "remote: Enumerating objects: 44, done.\u001b[K\n",
            "remote: Counting objects: 100% (44/44), done.\u001b[K\n",
            "remote: Compressing objects: 100% (31/31), done.\u001b[K\n",
            "remote: Total 248 (delta 16), reused 28 (delta 13), pack-reused 204\u001b[K\n",
            "Receiving objects: 100% (248/248), 12.87 MiB | 4.01 MiB/s, done.\n",
            "Resolving deltas: 100% (128/128), done.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "collapsed": false,
        "id": "0N5cKfMtci5B",
        "colab_type": "text"
      },
      "source": [
        "### Preliminaries - notes on using JAX\n",
        "\n",
        "JAX is amazing! It's really, really AMAZING! You program in Numpy/Python and then call a grad on your code, and it'll run speedy on GPUs! It does however have a few quirks and it uses a program deployment model you have to know about. The excited reader should definitely read the [JAX tutorial](https://github.com/google/jax), if they plan on programming with it. \n",
        "\n",
        "When using JAX for auto diff, auto batching or compiling, you should always have a two-level mental model in your mind: \n",
        "1. At the CPU level, like normal\n",
        "2. at the device level, for example a GPU. \n",
        "\n",
        "Since JAX compiles your code to device, it is very efficient but creates this split.  Thus, for example, we have two NumPY modules kicking around: 'onp' for 'original numpy', which is on the CPU, and np, which is the JAX modified version and runs 'on device'.  This latter version of numpy is enabled to compute gradients and run your code quickly.\n",
        "\n",
        "So the model then is: initialize variables, seeds, etc, at the CPU level, and *dispatch* a JAX based computation to the device. This all happens naturally whenever you call JAX enabled functions.\n",
        "\n",
        "Thus one of the first things we do initialize the onp random number generator at the CPU level."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "pycharm": {
          "name": "#%%\n"
        },
        "id": "V6jtDDBdci5B",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "onp_rng = onp.random.RandomState(seed=0) # For CPU-based numpy randomness"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "collapsed": false,
        "id": "1j2C9m2Qci5D",
        "colab_type": "text"
      },
      "source": [
        "----------"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "collapsed": false,
        "id": "CHsLBIEJci5D",
        "colab_type": "text"
      },
      "source": [
        "### Load the data\n",
        "\n",
        "You __must__ run through the the integrator RNN [tutorial notebook](https://github.com/google-research/computation-thru-dynamics/blob/master/notebooks/Integrator%20RNN%20Tutorial.ipynb) on your machine. Don't worry! It's much simpler than this tutorial! :) \n",
        "\n",
        "Point to the correct __data__  file for the integrator RNN. Note that the integrator rnn tutorial notebook creates two files, both the parameters file and the data file with examples."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "pycharm": {
          "name": "#%%\n"
        },
        "id": "IZ70tFM6ci5E",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "INTEGRATOR_RNN_DATA_FILE = \\\n",
        "     '/tmp/vrnn/pure_int/trained_data_vrnn_pure_int_0.00009_2019-05-08_18:53:02.h5'\n",
        "lfads_dir = '/tmp/lfads/'       # where to save lfads data and parameters to\n",
        "rnn_type = 'lfads'\n",
        "task_type = 'integrator'"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "pycharm": {
          "name": "#%%\n"
        },
        "id": "SBElLhI3ci5G",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Make directories\n",
        "data_dir = os.path.join(lfads_dir, 'data/')\n",
        "output_dir = os.path.join(lfads_dir, 'output/')\n",
        "figure_dir = os.path.join(lfads_dir, os.path.join(output_dir, 'figures/'))\n",
        "if not os.path.exists(output_dir):\n",
        "    os.makedirs(output_dir)\n",
        "if not os.path.exists(figure_dir):\n",
        "    os.makedirs(figure_dir)\n",
        "\n",
        "# Load synthetic data\n",
        "data_dict = utils.read_file(INTEGRATOR_RNN_DATA_FILE)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "collapsed": false,
        "id": "MzHfhmAMci5H",
        "colab_type": "text"
      },
      "source": [
        "### Plot examples and statistics about the integrator RNN data."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "pycharm": {
          "name": "#%%\n"
        },
        "id": "hbIgb-jbci5J",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "f = plotting.plot_data_pca(data_dict)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "collapsed": false,
        "id": "tnv1VH1Xci5L",
        "colab_type": "text"
      },
      "source": [
        "The goal of this tutorial is to infer the hiddens (blue), and input to the integrator RNN (umm... also blue)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "pycharm": {
          "name": "#%%\n"
        },
        "id": "AFGNV9Rwci5M",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "f = plotting.plot_data_example(data_dict['inputs'], \n",
        "                               data_dict['hiddens'],\n",
        "                               data_dict['outputs'], \n",
        "                               data_dict['targets'])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "collapsed": false,
        "id": "JOblvfoBci5O",
        "colab_type": "text"
      },
      "source": [
        "### Spikify the synthetic data\n",
        "The output of the integrator rnn is the continuous inputs,\n",
        "hidden states and outputs of the example.  LFADS is a tool \n",
        "to infer underlying factors in spiking neural data, so we \n",
        "are going to \"spikify\" the integrator rnn example hidden states.\n",
        "\n",
        "Data was generated w/ VRNN w/ tanh, thus $(\\mbox{data}+1) / 2 \\rightarrow [0,1]$. \n",
        "We put those activations between 0 and 1 here and then convert to spikes."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "pycharm": {
          "name": "#%%\n"
        },
        "id": "D72nhm3gci5P",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "data_dt = 1.0/25.0        # define our dt in a physiological range\n",
        "\n",
        "# If data is normed between 0 and 1, then a 1 yields this many \n",
        "# spikes per second. Pushing this downwards makes the problem harder.\n",
        "max_firing_rate = 40      \n",
        "train_fraction = 0.9      # Train with 90% of the synthetic data\n",
        "\n",
        "renormed_fun = lambda x : (x + 1) / 2.0\n",
        "\n",
        "renormed_data = renormed_fun(data_dict['hiddens'])\n",
        "\n",
        "# When dimensions are relevant, I use a variable naming scheme like\n",
        "# name_dim1xdim2x...  so below, here is the synthetic data with \n",
        "# 3 dimensions of batch, time and unit, in that order.\n",
        "data_bxtxn = utils.spikify_data(renormed_data, onp_rng, data_dt,\n",
        "                                max_firing_rate=max_firing_rate)\n",
        "nexamples, ntimesteps, data_dim = data_bxtxn.shape\n",
        "\n",
        "train_data, eval_data = utils.split_data(data_bxtxn,\n",
        "                                         train_fraction=train_fraction)\n",
        "eval_data_offset = int(train_fraction * data_bxtxn.shape[0])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "collapsed": false,
        "id": "SdJw-ukmci5S",
        "colab_type": "text"
      },
      "source": [
        "#### Plot the statistics of the data."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "pycharm": {
          "name": "#%%\n"
        },
        "id": "XAvFEOd-ci5S",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "f = plotting.plot_data_stats(data_dict, data_bxtxn, data_dt)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "collapsed": false,
        "id": "93mUQQhbci5U",
        "colab_type": "text"
      },
      "source": [
        "Let's study this single example of a single neuron's true firing rate (red) and the spikified version in the blue stem plot."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "pycharm": {
          "name": "#%%\n"
        },
        "id": "dUIP4UTeci5U",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "my_example_bidx = eval_data_offset + 1\n",
        "my_example_hidx = 17\n",
        "scale = max_firing_rate * data_dt\n",
        "my_signal = scale*renormed_data[my_example_bidx, :, my_example_hidx]\n",
        "my_signal_spikified = data_bxtxn[my_example_bidx, :, my_example_hidx]\n",
        "plt.plot(my_signal, 'r');\n",
        "plt.stem(my_signal_spikified);"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "collapsed": false,
        "id": "aX83cit4ci5W",
        "colab_type": "text"
      },
      "source": [
        "If you were to increase ```max_firing_rate``` to infinity, the stem plot would approach the red line. This plot gives you an idea of how challenging the data set is, at least on single trials. We can think about this a little bit.  If you were to simply filter the spikes, it definitely would not look like the red trace, at this low maximum firing rate. This means that if any technique were to have "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "pycharm": {
          "name": "#%%\n"
        },
        "id": "eZiyXAi1ci5W",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "nfilt = 3\n",
        "my_filtered_spikes = scipy.signal.filtfilt(onp.ones(nfilt)/nfilt, 1, my_signal_spikified)\n",
        "plt.plot(my_signal, 'r');\n",
        "plt.plot(my_filtered_spikes);\n",
        "plt.title(\"This looks terrible\");\n",
        "plt.legend(('True rate', 'Filtered spikes'));"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "collapsed": false,
        "id": "IG7OkFFdci5Y",
        "colab_type": "text"
      },
      "source": [
        "This would force us to think about ways in which the *population* can be filtered. The first idea is naturally PCA. Perhaps there is a low-d subspace of signal that can be found in the high-variance top PCs. Using the entire trial, it's likely this should do better."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "pycharm": {
          "name": "#%%\n"
        },
        "id": "YvJGwtBCci5Y",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import sklearn\n",
        "ncomponents = 100\n",
        "full_pca = sklearn.decomposition.PCA(ncomponents)\n",
        "full_pca.fit(onp.reshape(data_bxtxn, [-1, data_dim]))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "pycharm": {
          "name": "#%%\n"
        },
        "id": "OcquLPKXci5a",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "plt.stem(full_pca.explained_variance_)\n",
        "plt.title('Those top 2 PCs sure look promising!');"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "pycharm": {
          "name": "#%%\n"
        },
        "id": "2lJtEd6rci5b",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "ncomponents = 2\n",
        "pca = sklearn.decomposition.PCA(ncomponents)\n",
        "pca.fit(onp.reshape(data_bxtxn[0:eval_data_offset,:,:], [-1, data_dim]))\n",
        "my_example_pca = pca.transform(data_bxtxn[my_example_bidx,:,:])\n",
        "my_example_ipca = pca.inverse_transform(my_example_pca)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "pycharm": {
          "name": "#%%\n"
        },
        "id": "yOm7dxV2ci5d",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "plt.plot(my_signal, 'r')\n",
        "plt.plot(my_example_ipca[:,my_example_hidx])\n",
        "plt.legend(('True rate', 'PCA smoothed spikes'))\n",
        "plt.title('This a bit better.');"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "collapsed": false,
        "id": "buJdRs0aci5f",
        "colab_type": "text"
      },
      "source": [
        "So temporal filtering is not great, and spatial filtering helps only a bit.  What to do? The idea LFADS explores is that if you knew the system that generated the data, you would be able separate signal from noise, the signal being what a system can generate, the noise being the rest."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "collapsed": false,
        "id": "UQvOFn8rci5g",
        "colab_type": "text"
      },
      "source": [
        "----------------"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "collapsed": false,
        "id": "74gb1hxgci5h",
        "colab_type": "text"
      },
      "source": [
        "## LFADS - Latent Factor Analysis via Dynamical Systems\n",
        "\n",
        "\n",
        "[Link to paper readcube version of the LFADS Nature Methods 2018 paper](https://rdcu.be/6Wji)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "collapsed": false,
        "id": "MooV4BAxci5h",
        "colab_type": "text"
      },
      "source": [
        "### LFADS architecture with inferred inputs\n",
        "\n",
        "There are 3 variants of the LFADS architecture in the paper\n",
        "1. autonomous LFADS model (no inferred inputs), Fig. 1a\n",
        "2. stitched LFADS model for data recorded in different sessions, Fig. 4a\n",
        "3. non-autonomous LFADS model (with inferred inputs), Fig. 5a\n",
        "\n",
        "In this tutorial we deal with the non-autonomous model, which I believe is conceptually the most interesting, but also the most challenging to understand.  This tutorial (and the current code), does **NOT** handle stitched data.  Stitching data isn't conceptually hard, but it's a pain to code.  The Tensorflow version of the code handles that, if you need it. \n",
        "\n",
        "Here is the non-autonoumous LFADS model architecture: The full description of this model is given in the paper but briefly, the idea is that the data LFADS will 'denoise' or model data generated from a nonlinear, autonoumous system (we call it the data generator and the data generator in this tutorial is the integrator RNN) that receives an input through time. Based on the spiking observations, LFADS will try to pull apart the data into the dynamical system portion, and the input portion, thus the term *inferred inputs*.  I.e. we are trying to infer what inputs would drive a high-d nonlinear system to generate the data you've recorded. Doing this allows the system to model the dynamics much better for systems that are input driven. One final detail is that the model assumes that the spikes are poisson generated from an underlying continuous dynamical system. Of course, this is not true for spiking data from biological neural circuits, but the poisson approximation seems to be ok.\n",
        "\n",
        "So architecture infers a number of quantities of interest: \n",
        "1. initial state to generator (also called initial conditions)\n",
        "2. inferred inputs to generator - e.g. the LFADS component to learn the white noise in the integrator RNN example\n",
        "3. dynamical factors - these are like PCs underlying your data\n",
        "4. rates - a readout from the factors. The rates are really the most intuitive part, which are analogous to filtering your spiking data. \n",
        "\n",
        "![](https://raw.githubusercontent.com/google-research/computation-thru-dynamics/master/images/lfads_architecture_w_inferred_inputs_3.png)\n",
        "\n",
        "\n",
        "To begin, let's focus on the *autonomous* version of the architecture, which *excludes the controller RNN*.  The data is put through nonlinear, recurrent **encoders**, and this produces an **initial state distribtion**, which is a per-trial mean and variance to produce random vectors to encode that trial. The initial state of the generator is a randomly drawn vector from this distribution. The **generator** marches through time and at each time point produces **factors** and **rates**, ultimately producing outputs that learn to reproduce your data at the rate level.  \n",
        "\n",
        "\n",
        "From the perspective on information flow, the automous version of LFADS has a *bottleneck* between your data as inputted into LFADS, and the output, which also tries to learn your data. That bottleneck is the inital state of the generator, a potentially very low-bandwidth bottleneck, as a single vector has to encode a high-d time series. Such a system would be adequate for capturing systems that are (in approximation) autonomous.  For example, motor cortex dynamics during center-out reaches seem extremely well approximated by autonomous dynamics at the sub-second time scale (e.g. Fig 2). However, if you were to perturb the reach by messing with the cursor the animal was using, e.g perturbing cursor location mid-reach, then the motor cortical dynamics of a corrected reach couldn't possibly be autonomous.  In other words, some additional input must have come into the motor cortex and update the system with the information that the cursor had jumped unexpectedly.  This is the experimental setting we setup in Fig. 5.\n",
        "\n",
        "To compensate for such a scenario, we added a **controller** and **inferred inputs** to the generator portion of LFADS.  In particular, the controller runs in sync with the generator and receives the output of the generator from the last time step (the only \"backward\" loop in the architecture, aside from using backprop for training with gradient descent).  Thus it it knows what the generator output.  During training, the system learns that there are patterns in the data that cannot be created by the generator autonomously, so learns to compensate by emitting information from the data, through the encoders, through the controller to the generator.  We call this information an inferred input. In our experimental setup, this worked well on two examples: messing with the cursor of an animal making a reach and also for inferring oscillations in the local field potiential (LFP).\n",
        "\n",
        "Please note that the inferred input system is extremely powerful as it provides a leak from the your input data to the LFADS output on a per-time point basis. As such, one has to make sure that the system  does not pathologically leak all the information from the data trial through LFADS to generate the data trial. LFADS, like all auto-encoders, is at risk of creating a trivial identity function, $x = f(x)$, rather than finding stucture in the data. Thus, we utilize many tricks to avoid this (dropout, KL penalties, and even blocking out information given to the controller from time step t, when decoding time step t.)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "collapsed": false,
        "id": "E3MxUv8Pci5i",
        "colab_type": "text"
      },
      "source": [
        "### Hyperparameters"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "pycharm": {
          "name": "#%%\n"
        },
        "id": "Lrja8bDaci5j",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# LFADS Hyper parameters\n",
        "data_dim = train_data.shape[2]  # input to lfads should have dimensions:\n",
        "ntimesteps = train_data.shape[1] #   (batch_size x ntimesteps x data_dim)\n",
        "batch_size = 128      # batch size during optimization\n",
        "\n",
        "# LFADS architecture - The size of the numbers is rather arbitrary, \n",
        "# but relatively small because we know the integrator RNN isn't too high \n",
        "# dimensional in its activity.\n",
        "enc_dim = 128         # encoder dim\n",
        "con_dim = 128         # contoller dim\n",
        "ii_dim = 1            # inferred input dim, we know there is 1 dim in integrator RNN\n",
        "gen_dim = 128         # generator dim, should be large enough to generate integrator RNN dynamics\n",
        "factors_dim = 32      # factors dim, shoudld be large enough to capture most variance of dynamics\n",
        "\n",
        "# Numerical stability\n",
        "var_min = 0.001 # Minimal variance any gaussian can become.\n",
        "\n",
        "# Optimization HPs that percolates into model\n",
        "l2reg = 0.00002      # amount of l2 on weights"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "collapsed": false,
        "id": "EOq0lpY9ci5k",
        "colab_type": "text"
      },
      "source": [
        "### Hyperparameters for Priors\n",
        "\n",
        "As was mentioned above, LFADS is an auto-encoder and auto-encoders typically encode data through some kind of information bottleneck.  The idea is a lot like PCA, if one gets rid unimportant variation, then perhaps meaningful and interesting structure in the data will become apparent.  \n",
        "\n",
        "More precisely, LFADS is a **variational auto-encoder (VAE)**, which means that the bottleneck is achieved via probabilistic methods.  Namely, each trial intial state is encoded in a per-trial Gaussian distribution called the 'posterior', e.g. initial state parameter's mean and variance are given by $(\\mu(\\mathbf{x}), \\sigma^2(\\mathbf{x}))$, where $\\mathbf{x}$ is the data.  This then is compared to an **uninformative prior** $(\\mu_p, \\sigma^2_p)$, where uninformative means the prior is independent of the data, including that trial. A type of distance for distributions is used, called the KL-divergence, to force the initial state Gaussian distribution for each trial to be as close to as possible to a Gaussian that doesn't depend on the trial. This is a part of the **ELBO** - Evidence Lower BOund - that is used to train VAEs.\n",
        "\n",
        "In summary, one way of explaining VAEs is that they are auto-encoders, but they are attempting to limit the information flow from the input to the output using bottleneck based on probability distributions, basically forcing the generator to generate your data from white noise.  This is doomed to fail if training works, but in the process, it learns a probabilistic generative model of your data.\n",
        "\n",
        "In this LFADS architecture, there are two posterior distributions, based on the data, and two prior distributions, unrelated to the data.  They are distributions for the initial state, and the distributions for the inferred input."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "pycharm": {
          "name": "#%%\n"
        },
        "id": "EnEYiUZ_ci5l",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Initial state prior parameters\n",
        "# the mean is set to zero in the code\n",
        "ic_prior_var = 0.1 # this is $\\sigma^2_p$ in above paragraph"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "collapsed": false,
        "id": "E7v2RB26ci5n",
        "colab_type": "text"
      },
      "source": [
        "### Hyper parameters for inferred inputs\n",
        "\n",
        "The inferred inputs are also codes represented by posterior distributions, but now *each time point* is a Gaussian, so each inferred input time series is really a Gaussian process. A natural uninformative prior to compare the Gaussian process to is the [autoregressive-1](https://en.wikipedia.org/wiki/Autoregressive_model#Example:_An_AR(1)_process) process or AR-1 process for short. \n",
        "\n",
        "$s_t = c + \\phi s_{t-1} + \\epsilon_t, \\mbox{ with } \\epsilon_t \\in N(0, \\sigma^2_n) $\n",
        "\n",
        "with c the process mean, $\\phi$ giving dependence of process state at time $t-1$ to process state at time $t$ and $\\epsilon_t$ is the noise with variance $\\sigma^2_n$. In LFADS, $c$ is always $0$.\n",
        "\n",
        "So if you have 4 inferred inputs, then you have 4 AR-1 process priors. Utilizing an AR-1 process prior *over sequences* allows us to introduce another useful concept, **the auto-correlation** of each sequence. The auto-correlation is the correlation between values in the process at different time points.  We are interested in auto-correlation because we may want to penalize very jagged or very smooth inferred inputs on a task by task case, as well as for other technical reasons. As it turns out, the input to the integrator RNN in this tutorial is uncorrelated white noise, so this concept is not too important, but in general it may be very important.\n",
        "\n",
        "So just like the initial states, which introduced multi-variate gaussian distributions (the posteriors) for each data trial and an uninformative prior to which the per-trial posteriors are compared, we do the same thing with inferred inputs, now using the KL-divergence to compare the distribution of auto-regressive sequences to uninformative AR-1 priors. In this way, we aim to limit how informative the inferred inputs are by introducing a bottleneck between the encoder and the generator."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "pycharm": {
          "name": "#%%\n"
        },
        "id": "VaJ1P356ci5n",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Inferred input autoregressive prior parameters\n",
        "# Again, these hyper parameters are set \"in the ballpark\" but otherwise\n",
        "# pretty randomly.\n",
        "ar_mean = 0.0                 # process mean\n",
        "ar_autocorrelation_tau = 1.0  # seconds, how correlated each time point is, related to $\\phi$ above.\n",
        "ar_noise_variance = 0.1       # noise variance"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "pycharm": {
          "name": "#%%\n"
        },
        "id": "GXsoC-TPci5q",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "lfads_hps = {'data_dim' : data_dim, 'ntimesteps' : ntimesteps,\n",
        "             'enc_dim' : enc_dim, 'con_dim' : con_dim, 'var_min' : var_min,\n",
        "             'ic_prior_var' : ic_prior_var, 'ar_mean' : ar_mean,\n",
        "             'ar_autocorrelation_tau' : ar_autocorrelation_tau,\n",
        "             'ar_noise_variance' : ar_noise_variance,\n",
        "             'ii_dim' : ii_dim, 'gen_dim' : gen_dim,\n",
        "             'factors_dim' : factors_dim,\n",
        "             'l2reg' : l2reg,\n",
        "             'batch_size' : batch_size}"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "collapsed": false,
        "id": "8Nw7eACRci5s",
        "colab_type": "text"
      },
      "source": [
        "#### LFADS Optimization hyperparameters"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "pycharm": {
          "name": "#%%\n"
        },
        "id": "J_Loj1Q0ci5s",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "num_batches = 15000         # how many batches do we train\n",
        "print_every = 100            # give information every so often\n",
        "\n",
        "# Learning rate HPs\n",
        "step_size = 0.05            # initial learning rate\n",
        "decay_factor = 0.99985      # learning rate decay param\n",
        "decay_steps = 1             # learning rate decay param\n",
        "\n",
        "# Regularization HPs\n",
        "keep_rate = 0.98            # dropout keep rate during training\n",
        "l2reg = 0.00002     # amount of l2 on weights (in lfads_hps)\n",
        "\n",
        "# Numerical stability HPs\n",
        "max_grad_norm = 10.0        # gradient clipping above this value"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "collapsed": false,
        "id": "2EHciHbFci5u",
        "colab_type": "text"
      },
      "source": [
        "### Warming up the KL penalties \n",
        "The optimization of a VAE optimizes the ELBO, which is\n",
        "\n",
        "$L(\\theta) = -\\mathop{\\mathbb{E}}_x \\left(\\log p_\\theta(x|z) + KL(q_\\theta(z|x) \\;\\;|| \\;\\;p(z))\\right)$\n",
        "\n",
        "* $p_\\theta(x|z)$ - the reconstruction given the initial state and inferred inputs distributions (collectively denoted $z$  here)\n",
        "\n",
        "* $q_\\theta(z|x)$ - represents the latent variable posterior distributions (the data encoders that ultimately yield the intial state and inferred input codes).\n",
        "\n",
        "* $p(z)$ - the prior that does not know about the data\n",
        "\n",
        "where $\\theta$ are all the trainable parameters. This is an expectation over all your data, $x$, of the quality of the data generation $p_\\theta(x|z)$, plus the KL diverengence penality mentioned above that compares the distributions for the initial state and inferred inputs to uninformative priors.\n",
        "\n",
        "**All the hacks in hacksville:** It turns out that the KL term can be a lot easier to optimize initially than learning how to reconstruct your data. This results in a pathological stoppage of training where the KL goes to nearly zero and training is broken there on out (as you cannot represent any a given trial from uninformative priors).  One way out of this is to warmup the KL penality, starting it off with a weight term of 0 and then slowly building to 1, giving the reconstruction a chance to train a bit without the KL penalty messing things up."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "pycharm": {
          "name": "#%%\n"
        },
        "id": "D8fhbEH6ci5u",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# The fact that the start and end values are required to be floats is something I need to fix.\n",
        "kl_warmup_start = 500.0 # batch number to start kl warmup, explicitly float\n",
        "kl_warmup_end = 1000.0  # batch number to be finished with kl warmup, explicitly float\n",
        "kl_min = 0.01 # The minimum KL value, non-zero to make sure KL doesn't grow crazy before kicking in."
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "collapsed": false,
        "id": "zofx0G6Aci5w",
        "colab_type": "text"
      },
      "source": [
        "Note, there is currently a HUGE amount of debate about what the correct parameter value here is for the KL penalty. kl_max = 1 is what creates a lower bound on the (marginal) log likelihood of the data, but folks argue it could be higher or lower than 1. Myself, I have never played around with this HP, but I have the idea that LFADS may benefit from < 1 values, as LFADS is not really being used for random sampling from the distribution of spiking data.\n",
        "\n",
        "See [$\\beta$-VAE: LEARNING BASIC VISUAL CONCEPTS WITH A\n",
        "CONSTRAINED VARIATIONAL FRAMEWORK](https://openreview.net/pdf?id=Sy2fzU9gl)\n",
        "\n",
        "See [Fixing a Broken ELBO](https://arxiv.org/pdf/1711.00464.pdf) as to why you might choose a particular kl maximum value.  I found this article pretty clarifying."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "pycharm": {
          "name": "#%%\n"
        },
        "id": "6_xeXhdWci5w",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "kl_max = 1.0"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "pycharm": {
          "name": "#%%\n"
        },
        "id": "6Y7eoP7cci5y",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "lfads_opt_hps = {'num_batches' : num_batches, 'step_size' : step_size,\n",
        "                 'decay_steps' : decay_steps, 'decay_factor' : decay_factor,\n",
        "                 'kl_min' : kl_min, 'kl_max' : kl_max, 'kl_warmup_start' : kl_warmup_start,\n",
        "                 'kl_warmup_end' : kl_warmup_end, 'keep_rate' : keep_rate,\n",
        "                 'max_grad_norm' : max_grad_norm, 'print_every' : print_every,\n",
        "                 'adam_b1' : 0.9, 'adam_b2' : 0.999, 'adam_eps' : 1e-1}\n",
        "\n",
        "assert num_batches >= print_every and num_batches % print_every == 0"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "pycharm": {
          "name": "#%%\n"
        },
        "id": "EQEPNj7Wci50",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Plot the warmup function and the learning rate decay function.\n",
        "plt.figure(figsize=(16,4))\n",
        "plt.subplot(121)\n",
        "x = onp.arange(0, num_batches, print_every)\n",
        "kl_warmup_fun = get_kl_warmup_fun(lfads_opt_hps)\n",
        "plt.plot(x, [kl_warmup_fun(i) for i in onp.arange(1,lfads_opt_hps['num_batches'], print_every)]);\n",
        "plt.title('KL warmup function')\n",
        "plt.xlabel('Training batch');\n",
        "\n",
        "plt.subplot(122)\n",
        "decay_fun = optimizers.exponential_decay(lfads_opt_hps['step_size'],                                                             \n",
        "                                         lfads_opt_hps['decay_steps'],                                                           \n",
        "                                         lfads_opt_hps['decay_factor'])                                                          \n",
        "plt.plot(x, [decay_fun(i) for i in range(1, lfads_opt_hps['num_batches'], print_every)]);\n",
        "plt.title('learning rate function')\n",
        "plt.xlabel('Training batch');                                                                                     "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "collapsed": false,
        "id": "eyd0EQOUci51",
        "colab_type": "text"
      },
      "source": [
        "### Train the LFADS model\n",
        "\n",
        "Note that JAX uses it's own setup to handle randomness and seeding the pseudo-random number generators.  You can read about it [here](https://github.com/google/jax/blob/master/README.md#random-numbers-are-different). If you want to modify the LFADS tutorial you *NEED* to understand this. Otherwise, not so big a deal if you are just messing around with LFADS hyperparameters or applying the tutorial to new data."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "pycharm": {
          "name": "#%%\n"
        },
        "id": "N_uD-OoKci51",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Initialize parameters for LFADS\n",
        "key = random.PRNGKey(onp.random.randint(0, utils.MAX_SEED_INT))\n",
        "init_params = lfads.lfads_params(key, lfads_hps)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "collapsed": false,
        "id": "IEBHs3oKci53",
        "colab_type": "text"
      },
      "source": [
        "Note that the first loop could take a few minutes to run, because the LFADS model is unrolled, and therefor the JIT (just in time) compilation is slow, and happens \"just in time\", which is the first training loop iteration. On my computer, the JIT compilation takes a few minutes.\n",
        "\n",
        "You'll see the loss go up when the KL warmup starts turning on.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "pycharm": {
          "name": "#%%\n"
        },
        "id": "02ue0L65ci54",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "key = random.PRNGKey(onp.random.randint(0, utils.MAX_SEED_INT))\n",
        "trained_params, opt_details = \\\n",
        "    optimize_lfads(key, init_params, lfads_hps, lfads_opt_hps,\n",
        "                   train_data, eval_data)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "pycharm": {
          "name": "#%%\n"
        },
        "id": "5i2Uulozci56",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Plot the training details\n",
        "x = onp.arange(0, num_batches, print_every)\n",
        "plt.figure(figsize=(20,6))\n",
        "plt.subplot(251)\n",
        "plt.plot(x, opt_details['tlosses']['total'], 'k')\n",
        "plt.ylabel('Training')\n",
        "plt.title('Total loss')\n",
        "plt.subplot(252)\n",
        "plt.plot(x, opt_details['tlosses']['nlog_p_xgz'], 'b')\n",
        "plt.title('Negative log p(z|x)')\n",
        "plt.subplot(253)\n",
        "plt.plot(x, opt_details['tlosses']['kl_ii'], 'r')\n",
        "plt.title('KL inferred inputs')\n",
        "plt.subplot(254)\n",
        "plt.plot(x, opt_details['tlosses']['kl_g0'], 'g')\n",
        "plt.title('KL initial state')\n",
        "plt.subplot(255)\n",
        "plt.plot(x, opt_details['tlosses']['l2'], 'c')\n",
        "plt.xlabel('Training batch')\n",
        "plt.title('L2 loss')\n",
        "plt.subplot(256)\n",
        "plt.plot(x, opt_details['elosses']['total'], 'k')\n",
        "plt.xlabel('Training batch')\n",
        "plt.ylabel('Evaluation')\n",
        "plt.subplot(257)\n",
        "plt.plot(x, opt_details['tlosses']['nlog_p_xgz'], 'b')\n",
        "plt.xlabel('Training batch')\n",
        "plt.subplot(258)\n",
        "plt.plot(x, opt_details['elosses']['kl_ii'], 'r')\n",
        "plt.xlabel('Training batch')\n",
        "plt.subplot(259)\n",
        "plt.plot(x, opt_details['elosses']['kl_g0'], 'g')\n",
        "plt.xlabel('Training batch');"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "pycharm": {
          "name": "#%%\n"
        },
        "id": "6H4vlTVCci57",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# See the effect of the KL warmup, which is shown \n",
        "# by the KL penalities without the warmup scaling. \n",
        "plt.figure(figsize=(7,4))\n",
        "plt.subplot(221)\n",
        "plt.plot(x, opt_details['tlosses']['kl_ii_prescale'], 'r--')\n",
        "plt.ylabel('Training')\n",
        "plt.subplot(222)\n",
        "plt.plot(x, opt_details['tlosses']['kl_g0_prescale'], 'g--')\n",
        "plt.subplot(223)\n",
        "plt.plot(x, opt_details['elosses']['kl_ii_prescale'], 'r--')\n",
        "plt.ylabel('Evaluation')\n",
        "plt.xlabel('Training batch')\n",
        "plt.subplot(224)\n",
        "plt.plot(x, opt_details['elosses']['kl_g0_prescale'], 'g--')\n",
        "plt.xlabel('Trainign batch');"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "collapsed": false,
        "id": "1JYaBBY_ci5-",
        "colab_type": "text"
      },
      "source": [
        "### Save the LFADS model parameters"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "pycharm": {
          "name": "#%%\n"
        },
        "id": "0xCswkZGci5-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "fname_uniquifier = datetime.datetime.now().strftime(\"%Y-%m-%d_%H:%M:%S\")\n",
        "network_fname = ('trained_params_' + rnn_type + '_' + task_type + '_' + \\\n",
        "                 fname_uniquifier + '.npz')\n",
        "network_path = os.path.join(output_dir, network_fname)\n",
        "\n",
        "# Note we are just using numpy save instead of h5 because the LFADS parameter \n",
        "# is nested dictionaries, something I couldn't get h5 to save down easily.\n",
        "print(\"Saving parameters: \", network_path)\n",
        "onp.savez(network_path, trained_params)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "pycharm": {
          "name": "#%%\n"
        },
        "id": "J2-yonahci5_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# After training, you can load these up, after locating the save file.\n",
        "if False:\n",
        "    loaded_params = onp.load(network_path)\n",
        "    trained_params = loaded_params['arr_0'].item()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "collapsed": false,
        "id": "8Y85oiNgci6B",
        "colab_type": "text"
      },
      "source": [
        "### LFADS Visualization\n",
        "To plot the results of LFADS, namely the inferred quantities such as the inferred inputs, factors, or rates, we have to do a sample-and-average operation.  Remember, the latent variables for LFADS are the initial state and the inferred inputs, and they are per-trial *stochastic* codes, even for a *single trial*.  To get good inference for a given trial, we sample a large number of times from these per-trial stochastic latent variables, run the generator forward, and then average all the quantities of interest over the samples. \n",
        "\n",
        "If LFADS were linear a linear model, it would be equivalent to do the *much more efficient decode* of the posterior means, that is, just take the mean of the initial state distribution and the mean of the inferred input distribution, and then run the decoder one time. (This, btw, is a great exercise to the tutorial reader: implement posterior-mean decoding in this tutorial.)\n",
        "\n",
        "Here we use batching and take the 'posterior average' using batch number of samples from the latent variable distributions.\n",
        "\n",
        "So the main result of this tutorial, the moment you've been waiting for, is the comparison between the true rates of the integrator RNN, and the inferred rates by LFADS, and the true input to the integrator RNN and the inferred inputs given by LFADS.  You can see how well we did by generating lots of trials here."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "pycharm": {
          "name": "#%%\n"
        },
        "id": "KhNsEYIqci6B",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Plot a bunch of examples of eval trials run through LFADS.\n",
        "reload(plotting)\n",
        "\n",
        "def plot_rescale_fun(a): \n",
        "    fac = max_firing_rate * data_dt\n",
        "    return renormed_fun(a) * fac\n",
        "\n",
        "bidx = my_example_bidx - eval_data_offset\n",
        "\n",
        "nexamples_to_save = 1\n",
        "for eidx in range(nexamples_to_save):\n",
        "    fkey = random.fold_in(key, eidx)\n",
        "    psa_example = eval_data[bidx,:,:].astype(np.float32)\n",
        "    psa_dict = lfads.posterior_sample_and_average_jit(trained_params, lfads_hps, fkey, psa_example)\n",
        "\n",
        "    # The inferred input and true input are rescaled and shifted via \n",
        "    # linear regression to match, as there is an identifiability issue. there.\n",
        "    plotting.plot_lfads(psa_example, psa_dict,\n",
        "                        data_dict, eval_data_offset+bidx, plot_rescale_fun)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "collapsed": false,
        "id": "A-yk0yoGci6C",
        "colab_type": "text"
      },
      "source": [
        "And coming back to our example signal, how well does LFADS do on it, compared to the other *much easier to implement* methods? A noticable improvement on inferring the underlying rate."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "pycharm": {
          "name": "#%%\n"
        },
        "id": "JTBptnuoci6D",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "plt.figure(figsize=(16,4))\n",
        "\n",
        "plt.subplot(141)\n",
        "plt.plot(my_signal, 'r');\n",
        "plt.stem(my_signal_spikified);\n",
        "_, _, r2_spike, _, _ = scipy.stats.linregress(my_signal_spikified, my_signal)\n",
        "plt.title('Raw spikes R^2=%.3f' % (r2_spike))\n",
        "plt.legend(('True rate', 'Spikes'));\n",
        "\n",
        "\n",
        "plt.subplot(142)\n",
        "plt.plot(my_signal, 'r');\n",
        "plt.plot(my_filtered_spikes);\n",
        "_, _, r2_tfilt, _, _ = scipy.stats.linregress(my_filtered_spikes, my_signal)\n",
        "plt.title(\"Temporal filtering  R^2=%.3f\" % (r2_tfilt));\n",
        "plt.legend(('True rate', 'Filtered spikes'));\n",
        "\n",
        "plt.subplot(143)\n",
        "plt.plot(my_signal, 'r')\n",
        "plt.plot(my_example_ipca[:,my_example_hidx])\n",
        "_, _, r2_sfilt, _, _ = scipy.stats.linregress(my_example_ipca[:,my_example_hidx], my_signal)\n",
        "plt.legend(('True rate', 'PCA smoothed spikes'))\n",
        "plt.title('Spatial filtering R^2=%.3f' % (r2_sfilt));\n",
        "\n",
        "plt.subplot(144)\n",
        "plt.plot(my_signal, 'r')\n",
        "my_lfads_rate = onp.exp(psa_dict['lograte_t'][:,my_example_hidx])\n",
        "plt.plot(my_lfads_rate)\n",
        "_, _, r2_lfads, _, _ = scipy.stats.linregress(my_lfads_rate, my_signal)\n",
        "plt.legend(('True rate', 'LFADS rate'))\n",
        "plt.title('LFADS \"filtering\" R^2=%.3f' % (r2_lfads));"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "collapsed": false,
        "id": "XMVhbaGpci6E",
        "colab_type": "text"
      },
      "source": [
        "That single example can't tell the whole story so let us look at the average. LFADS is much better than spatial averaging across a large set of trials.\n",
        "\n",
        "Take an average over all the hidden units in 100 evaluation trials."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "pycharm": {
          "name": "#%%\n"
        },
        "id": "-1zqZKeNci6E",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "r2_sfilts = []\n",
        "r2_lfadss = []\n",
        "for bidx in range(100):\n",
        "    ebidx = eval_data_offset + bidx\n",
        "    \n",
        "    # Get the LFADS decode for this trial.\n",
        "    fkey = random.fold_in(key, bidx)\n",
        "    psa_example = eval_data[bidx,:,:].astype(np.float32)\n",
        "    psa_dict = lfads.posterior_sample_and_average_jit(trained_params, lfads_hps, fkey, psa_example)\n",
        "    \n",
        "    # Get the spatially smoothed trial.\n",
        "    trial_rates = scale*renormed_data[ebidx, :, :]\n",
        "    trial_spikes = data_bxtxn[ebidx, :, :]\n",
        "    spikes_pca = pca.transform(trial_spikes)\n",
        "    spikes_ipca = pca.inverse_transform(spikes_pca)\n",
        "    \n",
        "    for hidx in range(data_dim):\n",
        "        sig = trial_rates[:, hidx]\n",
        "        _, _, r2_sfilt, _, _ = scipy.stats.linregress(spikes_ipca[:,hidx], sig)\n",
        "\n",
        "        lr = onp.exp(psa_dict['lograte_t'][:,hidx])\n",
        "        _, _, r2_lfads, _, _ = scipy.stats.linregress(lr, sig)\n",
        "\n",
        "        r2_sfilts.append(r2_sfilt)\n",
        "        r2_lfadss.append(r2_lfads)\n",
        "    \n",
        "r2_sfilts = onp.array(r2_sfilts)\n",
        "r2_lfadss = onp.array(r2_lfadss)\n",
        "\n",
        "plt.figure(figsize=(8,4))\n",
        "plt.subplot(121)\n",
        "plt.hist(r2_sfilts, 50)\n",
        "plt.title('Spatial filtering, hist of R^2, <%.3f>' % (onp.mean(r2_sfilts)))\n",
        "plt.xlim([-.5, 1.0])\n",
        "\n",
        "plt.subplot(122)\n",
        "plt.hist(r2_lfadss, 50);\n",
        "plt.title('LFADS filtering, hist of R^2, <%.3f>' % (onp.mean(r2_lfadss)));\n",
        "plt.xlim([-.5, 1.0]);"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "collapsed": false,
        "id": "EGse517cci6G",
        "colab_type": "text"
      },
      "source": [
        "### Compare the inferred inputs learned by LFADS to the actual inputs to the integrator RNN. \n",
        "\n",
        "Finally, we can look at the average correlation between the inferred inputs and the true inputs to the integrator RNN. The inferred input can be arbitrarily scaled or rotated, so we first compute a linear regression, to scale the inferred input correctly, and then get the $R^2$."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "pycharm": {
          "name": "#%%\n"
        },
        "id": "hqCTU8vWci6H",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "r2_iis = []\n",
        "for bidx in range(200):\n",
        "    ebidx = eval_data_offset + bidx\n",
        "    \n",
        "    # Get the LFADS decode for this trial.\n",
        "    psa_example = eval_data[bidx,:,:].astype(np.float32)\n",
        "    fkey = random.fold_in(key, bidx)\n",
        "    psa_dict = lfads.posterior_sample_and_average_jit(trained_params, lfads_hps, fkey, psa_example)\n",
        "    \n",
        "    # Get the true input and inferred input\n",
        "    true_input = onp.squeeze(data_dict['inputs'][ebidx])\n",
        "    inferred_input = onp.squeeze(psa_dict['ii_t'])\n",
        "    slope, intercept, _, _, _ = scipy.stats.linregress(inferred_input, true_input)\n",
        "    _, _, r2_ii, _, _ = scipy.stats.linregress(slope * inferred_input + intercept, true_input)\n",
        "    \n",
        "    r2_iis.append(r2_ii)\n",
        "    \n",
        "r2_iis = onp.array(r2_iis)\n",
        "\n",
        "plt.hist(r2_iis, 20)\n",
        "plt.title('Correlation between rescaled inferrred inputs and true inputs, hist of R^2, <%.3f>' % (onp.mean(r2_iis)))\n",
        "plt.xlim([0.0, 1.0]);"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "collapsed": false,
        "id": "pS0xETEBci6J",
        "colab_type": "text"
      },
      "source": [
        "### Compare the inferred initial state for the LFADS generator to the actual initial state of the integrator RNN.\n",
        "\n",
        "To finish, we can examine the relationship between the initial condition (h0) of the integrator RNN and the inferred initial condition of the LFADS generator.\n",
        "The color we use is the readout of the integrator RNN's initial state, so basically the state of the line attractor before further information is presented.  In the integrator RNN example, we made sure to seed these intial states with a various values along the line attractor, so we expect a line of coloration."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "pycharm": {
          "name": "#%%\n"
        },
        "id": "CF-x8SUlci6K",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "ntrials = 1000\n",
        "true_h0s = onp.zeros([ntrials, data_dim])\n",
        "ic_means = onp.zeros([ntrials, gen_dim])\n",
        "colors = onp.zeros(ntrials)\n",
        "for bidx in range(ntrials):\n",
        "    ebidx = eval_data_offset + bidx\n",
        "    \n",
        "    # Get the LFADS decode for this trial.\n",
        "    psa_example = eval_data[bidx,:,:].astype(np.float32)\n",
        "    fkey = random.fold_in(key, bidx)\n",
        "    psa_dict = lfads.posterior_sample_and_average_jit(trained_params, lfads_hps, fkey, psa_example)\n",
        "    \n",
        "    # Get the true initial condition (and the readout of the true h0 for color)\n",
        "    # Get the inferred input from LFADS\n",
        "    true_h0s[bidx,:] = data_dict['h0s'][ebidx]\n",
        "    colors[bidx] = data_dict['outputs'][ebidx][0]\n",
        "    ic_means[bidx,:] = psa_dict['ic_mean']\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "pycharm": {
          "name": "#%%\n"
        },
        "id": "kwBIsIYTci6L",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.manifold import TSNE\n",
        "plt.figure(figsize=(16,8))\n",
        "plt.subplot(121)\n",
        "h0s_embedded = TSNE(n_components=2).fit_transform(true_h0s)\n",
        "plt.scatter(h0s_embedded[:,0], h0s_embedded[:,1], c=colors)\n",
        "plt.title('TSNE visualization of integrator RNN intial state')\n",
        "plt.subplot(122)\n",
        "ic_means_embedded = TSNE(n_components=2).fit_transform(ic_means)\n",
        "plt.scatter(ic_means_embedded[:,0], ic_means_embedded[:,1], c=colors);\n",
        "plt.title('TSNE visualziation of LFADS inferred intial generator state.')"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}