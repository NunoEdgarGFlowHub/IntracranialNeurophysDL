{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "06_00_LFADS_Tutorial.ipynb",
      "version": "0.3.2",
      "provenance": []
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 2
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython2",
      "version": "2.7.6"
    },
    "kernelspec": {
      "name": "python3",
      "language": "python",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "collapsed": false,
        "id": "jf0l50IDci45",
        "colab_type": "text",
        "pycharm": {}
      },
      "source": [
        "Note this is a copy of [the original tutorial](https://github.com/google-research/computation-thru-dynamics/blob/master/notebooks/LFADS%20Tutorial.ipynb)\n",
        "adapted to [run on Google Colab](https://colab.research.google.com/github/SachsLab/IntracranialNeurophysDL/blob/master/notebooks/06_00_LFADS_Tutorial.ipynb).\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lmCMfyHXcmUy",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 108
        },
        "outputId": "d2719b64-168a-4f1e-c24f-fdc409b17e62",
        "pycharm": {}
      },
      "source": "try:\n    # See if we are running on google.colab\n    import google.colab\n    import sys\n    if sys.version_info \u003e (3, 0):\n        from importlib import reload\n        !pip install --upgrade -q https://storage.googleapis.com/jax-wheels/cuda$(echo $CUDA_VERSION | sed -e \u0027s/\\.//\u0027 -e \u0027s/\\..*//\u0027)/jaxlib-latest-cp36-none-linux_x86_64.whl\n    else:\n        !pip install --upgrade -q https://storage.googleapis.com/jax-wheels/cuda$(echo $CUDA_VERSION | sed -e \u0027s/\\.//\u0027 -e \u0027s/\\..*//\u0027)/jaxlib-latest-cp27-none-linux_x86_64.whl\n    !pip install --upgrade -q git+https://github.com/google/jax.git\nexcept ModuleNotFoundError:\n    pass",
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\u001b[K     |████████████████████████████████| 39.6MB 49.5MB/s \n",
            "\u001b[K     |████████████████████████████████| 184kB 21.1MB/s \n",
            "\u001b[K     |████████████████████████████████| 61kB 24.2MB/s \n",
            "\u001b[?25h  Building wheel for jax (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for opt-einsum (setup.py) ... \u001b[?25l\u001b[?25hdone\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1MaZ5WsrcnAw",
        "colab_type": "text",
        "pycharm": {}
      },
      "source": [
        "# Run the LFADS algorithm on an RNN that integrates white noise.\n",
        "\n",
        "The goal of this tutorial is to learn about LFADS by running the algorithm on a simple data generator, a vanilla recurrent neural network (RNN) that was [trained to integrate a white noise input](https://github.com/google-research/computation-thru-dynamics/blob/master/notebooks/Integrator%20RNN%20Tutorial.ipynb).  Running LFADS on this integrator RNN will infer two things:\n",
        "1. the underlying hidden state of the integrator RNN\n",
        "2. the white noise input to the integrator RNN.\n",
        "\n",
        "Doing this will exercise the more complex LFADS architecture that is shown in Figure 5 of the [LFADS paper](https://rdcu.be/6Wji). It\u0027s pretty important that you have read at least the introduction of the paper, otherwise you won\u0027t understand *why* we are doing what we are doing.\n",
        "\n",
        "In this tutorial we do a few things:\n",
        "1. Load the integrator RNN data and \"spikify\" it by treating the hidden units as nonhomogeneous Poisson processes.\n",
        "2. Explain a bit of the of the LFADS architecture and highlight some of the relevant hyperparameters.\n",
        "3. Train the LFADS system on the spikified integrator RNN hidden states.\n",
        "4. Plot a whole bunch of training plots and LFADS outputs!\n",
        "\n",
        "If you make it through this tutorial and understand everything in it, it is *highly* likely you\u0027ll be able to run LFADS on your own data."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "collapsed": false,
        "id": "h1PY8jFici47",
        "colab_type": "text",
        "pycharm": {}
      },
      "source": [
        "#### Copyright 2019 Google LLC\n",
        "\n",
        "Licensed under the Apache License, Version 2.0 (the \"License\");\n",
        "you may not use this file except in compliance with the License.\n",
        "You may obtain a copy of the License at\n",
        "\n",
        "     https://www.apache.org/licenses/LICENSE-2.0\n",
        "\n",
        "Unless required by applicable law or agreed to in writing, software\n",
        "distributed under the License is distributed on an \"AS IS\" BASIS,\n",
        "WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
        "See the License for the specific language governing permissions and\n",
        "limitations under the License."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "collapsed": false,
        "id": "oqik3MMJci48",
        "colab_type": "text",
        "pycharm": {}
      },
      "source": [
        "### Import the tutorial code.\n",
        "\n",
        "If you are going to actually run the tutorial, you have to install JAX, download the computation thru dynamics github repo, and modify a path. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "pycharm": {
          "name": "#%%\n"
        },
        "id": "gObBt0KKci48",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Numpy, JAX, Matplotlib and h5py should all be correctly installed and on the python path.\n",
        "from __future__ import print_function, division, absolute_import\n",
        "\n",
        "import datetime\n",
        "import h5py\n",
        "import jax.numpy as np\n",
        "from jax import random\n",
        "from jax.experimental import optimizers\n",
        "from jax.config import config\n",
        "#config.update(\"jax_debug_nans\", True) # Useful for finding numerical errors\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as onp  # original CPU-backed NumPy\n",
        "import scipy.signal\n",
        "import scipy.stats\n",
        "import os\n",
        "import sys\n",
        "import time\n",
        "from pathlib import Path"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "pycharm": {
          "name": "#%%\n"
        },
        "id": "WZF2cAokci4_",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 145
        },
        "outputId": "851a6205-d10e-4561-cae4-f179adc96719"
      },
      "source": "if not (Path.cwd().parent / \u0027repo\u0027).is_dir():\n    !git clone --recursive https://github.com/google-research/computation-thru-dynamics.git\nsys.path.append(os.path.join(\u0027.\u0027, \u0027computation-thru-dynamics\u0027))\nimport lfads_tutorial.lfads as lfads\nimport lfads_tutorial.plotting as plotting\nimport lfads_tutorial.utils as utils\nfrom lfads_tutorial.optimize import optimize_lfads, get_kl_warmup_fun",
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Cloning into \u0027../home/repo\u0027...\n",
            "remote: Enumerating objects: 44, done.\u001b[K\n",
            "remote: Counting objects: 100% (44/44), done.\u001b[K\n",
            "remote: Compressing objects: 100% (31/31), done.\u001b[K\n",
            "remote: Total 248 (delta 16), reused 28 (delta 13), pack-reused 204\u001b[K\n",
            "Receiving objects: 100% (248/248), 12.87 MiB | 4.01 MiB/s, done.\n",
            "Resolving deltas: 100% (128/128), done.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "collapsed": false,
        "id": "0N5cKfMtci5B",
        "colab_type": "text",
        "pycharm": {}
      },
      "source": [
        "### Preliminaries - notes on using JAX\n",
        "\n",
        "JAX is amazing! It\u0027s really, really AMAZING! You program in Numpy/Python and then call a grad on your code, and it\u0027ll run speedy on GPUs! It does however have a few quirks and it uses a program deployment model you have to know about. The excited reader should definitely read the [JAX tutorial](https://github.com/google/jax), if they plan on programming with it. \n",
        "\n",
        "When using JAX for auto diff, auto batching or compiling, you should always have a two-level mental model in your mind: \n",
        "1. At the CPU level, like normal\n",
        "2. at the device level, for example a GPU. \n",
        "\n",
        "Since JAX compiles your code to device, it is very efficient but creates this split.  Thus, for example, we have two NumPY modules kicking around: \u0027onp\u0027 for \u0027original numpy\u0027, which is on the CPU, and np, which is the JAX modified version and runs \u0027on device\u0027.  This latter version of numpy is enabled to compute gradients and run your code quickly.\n",
        "\n",
        "So the model then is: initialize variables, seeds, etc, at the CPU level, and *dispatch* a JAX based computation to the device. This all happens naturally whenever you call JAX enabled functions.\n",
        "\n",
        "Thus one of the first things we do initialize the onp random number generator at the CPU level."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "pycharm": {
          "name": "#%%\n"
        },
        "id": "V6jtDDBdci5B",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "onp_rng \u003d onp.random.RandomState(seed\u003d0) # For CPU-based numpy randomness"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "collapsed": false,
        "id": "1j2C9m2Qci5D",
        "colab_type": "text",
        "pycharm": {}
      },
      "source": [
        "----------"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "collapsed": false,
        "id": "CHsLBIEJci5D",
        "colab_type": "text",
        "pycharm": {}
      },
      "source": [
        "### Load the data\n",
        "\n",
        "You __must__ run through the the integrator RNN [tutorial notebook](https://github.com/google-research/computation-thru-dynamics/blob/master/notebooks/Integrator%20RNN%20Tutorial.ipynb) on your machine. Don\u0027t worry! It\u0027s much simpler than this tutorial! :) \n",
        "\n",
        "Point to the correct __data__  file for the integrator RNN. Note that the integrator rnn tutorial notebook creates two files, both the parameters file and the data file with examples."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "pycharm": {
          "name": "#%%\n"
        },
        "id": "IZ70tFM6ci5E",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "INTEGRATOR_RNN_DATA_FILE \u003d \\\n",
        "     \u0027/tmp/vrnn/pure_int/trained_data_vrnn_pure_int_0.00009_2019-05-08_18:53:02.h5\u0027\n",
        "lfads_dir \u003d \u0027/tmp/lfads/\u0027       # where to save lfads data and parameters to\n",
        "rnn_type \u003d \u0027lfads\u0027\n",
        "task_type \u003d \u0027integrator\u0027"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "pycharm": {
          "name": "#%%\n"
        },
        "id": "SBElLhI3ci5G",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Make directories\n",
        "data_dir \u003d os.path.join(lfads_dir, \u0027data/\u0027)\n",
        "output_dir \u003d os.path.join(lfads_dir, \u0027output/\u0027)\n",
        "figure_dir \u003d os.path.join(lfads_dir, os.path.join(output_dir, \u0027figures/\u0027))\n",
        "if not os.path.exists(output_dir):\n",
        "    os.makedirs(output_dir)\n",
        "if not os.path.exists(figure_dir):\n",
        "    os.makedirs(figure_dir)\n",
        "\n",
        "# Load synthetic data\n",
        "data_dict \u003d utils.read_file(INTEGRATOR_RNN_DATA_FILE)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "collapsed": false,
        "id": "MzHfhmAMci5H",
        "colab_type": "text",
        "pycharm": {}
      },
      "source": [
        "### Plot examples and statistics about the integrator RNN data."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "pycharm": {
          "name": "#%%\n"
        },
        "id": "hbIgb-jbci5J",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "f \u003d plotting.plot_data_pca(data_dict)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "collapsed": false,
        "id": "tnv1VH1Xci5L",
        "colab_type": "text",
        "pycharm": {}
      },
      "source": [
        "The goal of this tutorial is to infer the hiddens (blue), and input to the integrator RNN (umm... also blue)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "pycharm": {
          "name": "#%%\n"
        },
        "id": "AFGNV9Rwci5M",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "f \u003d plotting.plot_data_example(data_dict[\u0027inputs\u0027], \n",
        "                               data_dict[\u0027hiddens\u0027],\n",
        "                               data_dict[\u0027outputs\u0027], \n",
        "                               data_dict[\u0027targets\u0027])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "collapsed": false,
        "id": "JOblvfoBci5O",
        "colab_type": "text",
        "pycharm": {}
      },
      "source": [
        "### Spikify the synthetic data\n",
        "The output of the integrator rnn is the continuous inputs,\n",
        "hidden states and outputs of the example.  LFADS is a tool \n",
        "to infer underlying factors in spiking neural data, so we \n",
        "are going to \"spikify\" the integrator rnn example hidden states.\n",
        "\n",
        "Data was generated w/ VRNN w/ tanh, thus $(\\mbox{data}+1) / 2 \\rightarrow [0,1]$. \n",
        "We put those activations between 0 and 1 here and then convert to spikes."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "pycharm": {
          "name": "#%%\n"
        },
        "id": "D72nhm3gci5P",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "data_dt \u003d 1.0/25.0        # define our dt in a physiological range\n",
        "\n",
        "# If data is normed between 0 and 1, then a 1 yields this many \n",
        "# spikes per second. Pushing this downwards makes the problem harder.\n",
        "max_firing_rate \u003d 40      \n",
        "train_fraction \u003d 0.9      # Train with 90% of the synthetic data\n",
        "\n",
        "renormed_fun \u003d lambda x : (x + 1) / 2.0\n",
        "\n",
        "renormed_data \u003d renormed_fun(data_dict[\u0027hiddens\u0027])\n",
        "\n",
        "# When dimensions are relevant, I use a variable naming scheme like\n",
        "# name_dim1xdim2x...  so below, here is the synthetic data with \n",
        "# 3 dimensions of batch, time and unit, in that order.\n",
        "data_bxtxn \u003d utils.spikify_data(renormed_data, onp_rng, data_dt,\n",
        "                                max_firing_rate\u003dmax_firing_rate)\n",
        "nexamples, ntimesteps, data_dim \u003d data_bxtxn.shape\n",
        "\n",
        "train_data, eval_data \u003d utils.split_data(data_bxtxn,\n",
        "                                         train_fraction\u003dtrain_fraction)\n",
        "eval_data_offset \u003d int(train_fraction * data_bxtxn.shape[0])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "collapsed": false,
        "id": "SdJw-ukmci5S",
        "colab_type": "text",
        "pycharm": {}
      },
      "source": [
        "#### Plot the statistics of the data."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "pycharm": {
          "name": "#%%\n"
        },
        "id": "XAvFEOd-ci5S",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "f \u003d plotting.plot_data_stats(data_dict, data_bxtxn, data_dt)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "collapsed": false,
        "id": "93mUQQhbci5U",
        "colab_type": "text",
        "pycharm": {}
      },
      "source": [
        "Let\u0027s study this single example of a single neuron\u0027s true firing rate (red) and the spikified version in the blue stem plot."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "pycharm": {
          "name": "#%%\n"
        },
        "id": "dUIP4UTeci5U",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "my_example_bidx \u003d eval_data_offset + 1\n",
        "my_example_hidx \u003d 17\n",
        "scale \u003d max_firing_rate * data_dt\n",
        "my_signal \u003d scale*renormed_data[my_example_bidx, :, my_example_hidx]\n",
        "my_signal_spikified \u003d data_bxtxn[my_example_bidx, :, my_example_hidx]\n",
        "plt.plot(my_signal, \u0027r\u0027);\n",
        "plt.stem(my_signal_spikified);"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "collapsed": false,
        "id": "aX83cit4ci5W",
        "colab_type": "text",
        "pycharm": {}
      },
      "source": [
        "If you were to increase ```max_firing_rate``` to infinity, the stem plot would approach the red line. This plot gives you an idea of how challenging the data set is, at least on single trials. We can think about this a little bit.  If you were to simply filter the spikes, it definitely would not look like the red trace, at this low maximum firing rate. This means that if any technique were to have "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "pycharm": {
          "name": "#%%\n"
        },
        "id": "eZiyXAi1ci5W",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "nfilt \u003d 3\n",
        "my_filtered_spikes \u003d scipy.signal.filtfilt(onp.ones(nfilt)/nfilt, 1, my_signal_spikified)\n",
        "plt.plot(my_signal, \u0027r\u0027);\n",
        "plt.plot(my_filtered_spikes);\n",
        "plt.title(\"This looks terrible\");\n",
        "plt.legend((\u0027True rate\u0027, \u0027Filtered spikes\u0027));"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "collapsed": false,
        "id": "IG7OkFFdci5Y",
        "colab_type": "text",
        "pycharm": {}
      },
      "source": [
        "This would force us to think about ways in which the *population* can be filtered. The first idea is naturally PCA. Perhaps there is a low-d subspace of signal that can be found in the high-variance top PCs. Using the entire trial, it\u0027s likely this should do better."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "pycharm": {
          "name": "#%%\n"
        },
        "id": "YvJGwtBCci5Y",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import sklearn\n",
        "ncomponents \u003d 100\n",
        "full_pca \u003d sklearn.decomposition.PCA(ncomponents)\n",
        "full_pca.fit(onp.reshape(data_bxtxn, [-1, data_dim]))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "pycharm": {
          "name": "#%%\n"
        },
        "id": "OcquLPKXci5a",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "plt.stem(full_pca.explained_variance_)\n",
        "plt.title(\u0027Those top 2 PCs sure look promising!\u0027);"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "pycharm": {
          "name": "#%%\n"
        },
        "id": "2lJtEd6rci5b",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "ncomponents \u003d 2\n",
        "pca \u003d sklearn.decomposition.PCA(ncomponents)\n",
        "pca.fit(onp.reshape(data_bxtxn[0:eval_data_offset,:,:], [-1, data_dim]))\n",
        "my_example_pca \u003d pca.transform(data_bxtxn[my_example_bidx,:,:])\n",
        "my_example_ipca \u003d pca.inverse_transform(my_example_pca)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "pycharm": {
          "name": "#%%\n"
        },
        "id": "yOm7dxV2ci5d",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "plt.plot(my_signal, \u0027r\u0027)\n",
        "plt.plot(my_example_ipca[:,my_example_hidx])\n",
        "plt.legend((\u0027True rate\u0027, \u0027PCA smoothed spikes\u0027))\n",
        "plt.title(\u0027This a bit better.\u0027);"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "collapsed": false,
        "id": "buJdRs0aci5f",
        "colab_type": "text",
        "pycharm": {}
      },
      "source": [
        "So temporal filtering is not great, and spatial filtering helps only a bit.  What to do? The idea LFADS explores is that if you knew the system that generated the data, you would be able separate signal from noise, the signal being what a system can generate, the noise being the rest."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "collapsed": false,
        "id": "UQvOFn8rci5g",
        "colab_type": "text",
        "pycharm": {}
      },
      "source": [
        "----------------"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "collapsed": false,
        "id": "74gb1hxgci5h",
        "colab_type": "text",
        "pycharm": {}
      },
      "source": [
        "## LFADS - Latent Factor Analysis via Dynamical Systems\n",
        "\n",
        "\n",
        "[Link to paper readcube version of the LFADS Nature Methods 2018 paper](https://rdcu.be/6Wji)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "collapsed": false,
        "id": "MooV4BAxci5h",
        "colab_type": "text",
        "pycharm": {}
      },
      "source": [
        "### LFADS architecture with inferred inputs\n",
        "\n",
        "There are 3 variants of the LFADS architecture in the paper\n",
        "1. autonomous LFADS model (no inferred inputs), Fig. 1a\n",
        "2. stitched LFADS model for data recorded in different sessions, Fig. 4a\n",
        "3. non-autonomous LFADS model (with inferred inputs), Fig. 5a\n",
        "\n",
        "In this tutorial we deal with the non-autonomous model, which I believe is conceptually the most interesting, but also the most challenging to understand.  This tutorial (and the current code), does **NOT** handle stitched data.  Stitching data isn\u0027t conceptually hard, but it\u0027s a pain to code.  The Tensorflow version of the code handles that, if you need it. \n",
        "\n",
        "Here is the non-autonoumous LFADS model architecture: The full description of this model is given in the paper but briefly, the idea is that the data LFADS will \u0027denoise\u0027 or model data generated from a nonlinear, autonoumous system (we call it the data generator and the data generator in this tutorial is the integrator RNN) that receives an input through time. Based on the spiking observations, LFADS will try to pull apart the data into the dynamical system portion, and the input portion, thus the term *inferred inputs*.  I.e. we are trying to infer what inputs would drive a high-d nonlinear system to generate the data you\u0027ve recorded. Doing this allows the system to model the dynamics much better for systems that are input driven. One final detail is that the model assumes that the spikes are poisson generated from an underlying continuous dynamical system. Of course, this is not true for spiking data from biological neural circuits, but the poisson approximation seems to be ok.\n",
        "\n",
        "So architecture infers a number of quantities of interest: \n",
        "1. initial state to generator (also called initial conditions)\n",
        "2. inferred inputs to generator - e.g. the LFADS component to learn the white noise in the integrator RNN example\n",
        "3. dynamical factors - these are like PCs underlying your data\n",
        "4. rates - a readout from the factors. The rates are really the most intuitive part, which are analogous to filtering your spiking data. \n",
        "\n",
        "![](https://raw.githubusercontent.com/google-research/computation-thru-dynamics/master/images/lfads_architecture_w_inferred_inputs_3.png)\n",
        "\n",
        "\n",
        "To begin, let\u0027s focus on the *autonomous* version of the architecture, which *excludes the controller RNN*.  The data is put through nonlinear, recurrent **encoders**, and this produces an **initial state distribtion**, which is a per-trial mean and variance to produce random vectors to encode that trial. The initial state of the generator is a randomly drawn vector from this distribution. The **generator** marches through time and at each time point produces **factors** and **rates**, ultimately producing outputs that learn to reproduce your data at the rate level.  \n",
        "\n",
        "\n",
        "From the perspective on information flow, the automous version of LFADS has a *bottleneck* between your data as inputted into LFADS, and the output, which also tries to learn your data. That bottleneck is the inital state of the generator, a potentially very low-bandwidth bottleneck, as a single vector has to encode a high-d time series. Such a system would be adequate for capturing systems that are (in approximation) autonomous.  For example, motor cortex dynamics during center-out reaches seem extremely well approximated by autonomous dynamics at the sub-second time scale (e.g. Fig 2). However, if you were to perturb the reach by messing with the cursor the animal was using, e.g perturbing cursor location mid-reach, then the motor cortical dynamics of a corrected reach couldn\u0027t possibly be autonomous.  In other words, some additional input must have come into the motor cortex and update the system with the information that the cursor had jumped unexpectedly.  This is the experimental setting we setup in Fig. 5.\n",
        "\n",
        "To compensate for such a scenario, we added a **controller** and **inferred inputs** to the generator portion of LFADS.  In particular, the controller runs in sync with the generator and receives the output of the generator from the last time step (the only \"backward\" loop in the architecture, aside from using backprop for training with gradient descent).  Thus it it knows what the generator output.  During training, the system learns that there are patterns in the data that cannot be created by the generator autonomously, so learns to compensate by emitting information from the data, through the encoders, through the controller to the generator.  We call this information an inferred input. In our experimental setup, this worked well on two examples: messing with the cursor of an animal making a reach and also for inferring oscillations in the local field potiential (LFP).\n",
        "\n",
        "Please note that the inferred input system is extremely powerful as it provides a leak from the your input data to the LFADS output on a per-time point basis. As such, one has to make sure that the system  does not pathologically leak all the information from the data trial through LFADS to generate the data trial. LFADS, like all auto-encoders, is at risk of creating a trivial identity function, $x \u003d f(x)$, rather than finding stucture in the data. Thus, we utilize many tricks to avoid this (dropout, KL penalties, and even blocking out information given to the controller from time step t, when decoding time step t.)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "collapsed": false,
        "id": "E3MxUv8Pci5i",
        "colab_type": "text",
        "pycharm": {}
      },
      "source": [
        "### Hyperparameters"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "pycharm": {
          "name": "#%%\n"
        },
        "id": "Lrja8bDaci5j",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# LFADS Hyper parameters\n",
        "data_dim \u003d train_data.shape[2]  # input to lfads should have dimensions:\n",
        "ntimesteps \u003d train_data.shape[1] #   (batch_size x ntimesteps x data_dim)\n",
        "batch_size \u003d 128      # batch size during optimization\n",
        "\n",
        "# LFADS architecture - The size of the numbers is rather arbitrary, \n",
        "# but relatively small because we know the integrator RNN isn\u0027t too high \n",
        "# dimensional in its activity.\n",
        "enc_dim \u003d 128         # encoder dim\n",
        "con_dim \u003d 128         # contoller dim\n",
        "ii_dim \u003d 1            # inferred input dim, we know there is 1 dim in integrator RNN\n",
        "gen_dim \u003d 128         # generator dim, should be large enough to generate integrator RNN dynamics\n",
        "factors_dim \u003d 32      # factors dim, shoudld be large enough to capture most variance of dynamics\n",
        "\n",
        "# Numerical stability\n",
        "var_min \u003d 0.001 # Minimal variance any gaussian can become.\n",
        "\n",
        "# Optimization HPs that percolates into model\n",
        "l2reg \u003d 0.00002      # amount of l2 on weights"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "collapsed": false,
        "id": "EOq0lpY9ci5k",
        "colab_type": "text",
        "pycharm": {}
      },
      "source": [
        "### Hyperparameters for Priors\n",
        "\n",
        "As was mentioned above, LFADS is an auto-encoder and auto-encoders typically encode data through some kind of information bottleneck.  The idea is a lot like PCA, if one gets rid unimportant variation, then perhaps meaningful and interesting structure in the data will become apparent.  \n",
        "\n",
        "More precisely, LFADS is a **variational auto-encoder (VAE)**, which means that the bottleneck is achieved via probabilistic methods.  Namely, each trial intial state is encoded in a per-trial Gaussian distribution called the \u0027posterior\u0027, e.g. initial state parameter\u0027s mean and variance are given by $(\\mu(\\mathbf{x}), \\sigma^2(\\mathbf{x}))$, where $\\mathbf{x}$ is the data.  This then is compared to an **uninformative prior** $(\\mu_p, \\sigma^2_p)$, where uninformative means the prior is independent of the data, including that trial. A type of distance for distributions is used, called the KL-divergence, to force the initial state Gaussian distribution for each trial to be as close to as possible to a Gaussian that doesn\u0027t depend on the trial. This is a part of the **ELBO** - Evidence Lower BOund - that is used to train VAEs.\n",
        "\n",
        "In summary, one way of explaining VAEs is that they are auto-encoders, but they are attempting to limit the information flow from the input to the output using bottleneck based on probability distributions, basically forcing the generator to generate your data from white noise.  This is doomed to fail if training works, but in the process, it learns a probabilistic generative model of your data.\n",
        "\n",
        "In this LFADS architecture, there are two posterior distributions, based on the data, and two prior distributions, unrelated to the data.  They are distributions for the initial state, and the distributions for the inferred input."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "pycharm": {
          "name": "#%%\n"
        },
        "id": "EnEYiUZ_ci5l",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Initial state prior parameters\n",
        "# the mean is set to zero in the code\n",
        "ic_prior_var \u003d 0.1 # this is $\\sigma^2_p$ in above paragraph"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "collapsed": false,
        "id": "E7v2RB26ci5n",
        "colab_type": "text",
        "pycharm": {}
      },
      "source": [
        "### Hyper parameters for inferred inputs\n",
        "\n",
        "The inferred inputs are also codes represented by posterior distributions, but now *each time point* is a Gaussian, so each inferred input time series is really a Gaussian process. A natural uninformative prior to compare the Gaussian process to is the [autoregressive-1](https://en.wikipedia.org/wiki/Autoregressive_model#Example:_An_AR(1)_process) process or AR-1 process for short. \n",
        "\n",
        "$s_t \u003d c + \\phi s_{t-1} + \\epsilon_t, \\mbox{ with } \\epsilon_t \\in N(0, \\sigma^2_n) $\n",
        "\n",
        "with c the process mean, $\\phi$ giving dependence of process state at time $t-1$ to process state at time $t$ and $\\epsilon_t$ is the noise with variance $\\sigma^2_n$. In LFADS, $c$ is always $0$.\n",
        "\n",
        "So if you have 4 inferred inputs, then you have 4 AR-1 process priors. Utilizing an AR-1 process prior *over sequences* allows us to introduce another useful concept, **the auto-correlation** of each sequence. The auto-correlation is the correlation between values in the process at different time points.  We are interested in auto-correlation because we may want to penalize very jagged or very smooth inferred inputs on a task by task case, as well as for other technical reasons. As it turns out, the input to the integrator RNN in this tutorial is uncorrelated white noise, so this concept is not too important, but in general it may be very important.\n",
        "\n",
        "So just like the initial states, which introduced multi-variate gaussian distributions (the posteriors) for each data trial and an uninformative prior to which the per-trial posteriors are compared, we do the same thing with inferred inputs, now using the KL-divergence to compare the distribution of auto-regressive sequences to uninformative AR-1 priors. In this way, we aim to limit how informative the inferred inputs are by introducing a bottleneck between the encoder and the generator."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "pycharm": {
          "name": "#%%\n"
        },
        "id": "VaJ1P356ci5n",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Inferred input autoregressive prior parameters\n",
        "# Again, these hyper parameters are set \"in the ballpark\" but otherwise\n",
        "# pretty randomly.\n",
        "ar_mean \u003d 0.0                 # process mean\n",
        "ar_autocorrelation_tau \u003d 1.0  # seconds, how correlated each time point is, related to $\\phi$ above.\n",
        "ar_noise_variance \u003d 0.1       # noise variance"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "pycharm": {
          "name": "#%%\n"
        },
        "id": "GXsoC-TPci5q",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "lfads_hps \u003d {\u0027data_dim\u0027 : data_dim, \u0027ntimesteps\u0027 : ntimesteps,\n",
        "             \u0027enc_dim\u0027 : enc_dim, \u0027con_dim\u0027 : con_dim, \u0027var_min\u0027 : var_min,\n",
        "             \u0027ic_prior_var\u0027 : ic_prior_var, \u0027ar_mean\u0027 : ar_mean,\n",
        "             \u0027ar_autocorrelation_tau\u0027 : ar_autocorrelation_tau,\n",
        "             \u0027ar_noise_variance\u0027 : ar_noise_variance,\n",
        "             \u0027ii_dim\u0027 : ii_dim, \u0027gen_dim\u0027 : gen_dim,\n",
        "             \u0027factors_dim\u0027 : factors_dim,\n",
        "             \u0027l2reg\u0027 : l2reg,\n",
        "             \u0027batch_size\u0027 : batch_size}"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "collapsed": false,
        "id": "8Nw7eACRci5s",
        "colab_type": "text",
        "pycharm": {}
      },
      "source": [
        "#### LFADS Optimization hyperparameters"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "pycharm": {
          "name": "#%%\n"
        },
        "id": "J_Loj1Q0ci5s",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "num_batches \u003d 15000         # how many batches do we train\n",
        "print_every \u003d 100            # give information every so often\n",
        "\n",
        "# Learning rate HPs\n",
        "step_size \u003d 0.05            # initial learning rate\n",
        "decay_factor \u003d 0.99985      # learning rate decay param\n",
        "decay_steps \u003d 1             # learning rate decay param\n",
        "\n",
        "# Regularization HPs\n",
        "keep_rate \u003d 0.98            # dropout keep rate during training\n",
        "l2reg \u003d 0.00002     # amount of l2 on weights (in lfads_hps)\n",
        "\n",
        "# Numerical stability HPs\n",
        "max_grad_norm \u003d 10.0        # gradient clipping above this value"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "collapsed": false,
        "id": "2EHciHbFci5u",
        "colab_type": "text",
        "pycharm": {}
      },
      "source": [
        "### Warming up the KL penalties \n",
        "The optimization of a VAE optimizes the ELBO, which is\n",
        "\n",
        "$L(\\theta) \u003d -\\mathop{\\mathbb{E}}_x \\left(\\log p_\\theta(x|z) + KL(q_\\theta(z|x) \\;\\;|| \\;\\;p(z))\\right)$\n",
        "\n",
        "* $p_\\theta(x|z)$ - the reconstruction given the initial state and inferred inputs distributions (collectively denoted $z$  here)\n",
        "\n",
        "* $q_\\theta(z|x)$ - represents the latent variable posterior distributions (the data encoders that ultimately yield the intial state and inferred input codes).\n",
        "\n",
        "* $p(z)$ - the prior that does not know about the data\n",
        "\n",
        "where $\\theta$ are all the trainable parameters. This is an expectation over all your data, $x$, of the quality of the data generation $p_\\theta(x|z)$, plus the KL diverengence penality mentioned above that compares the distributions for the initial state and inferred inputs to uninformative priors.\n",
        "\n",
        "**All the hacks in hacksville:** It turns out that the KL term can be a lot easier to optimize initially than learning how to reconstruct your data. This results in a pathological stoppage of training where the KL goes to nearly zero and training is broken there on out (as you cannot represent any a given trial from uninformative priors).  One way out of this is to warmup the KL penality, starting it off with a weight term of 0 and then slowly building to 1, giving the reconstruction a chance to train a bit without the KL penalty messing things up."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "pycharm": {
          "name": "#%%\n"
        },
        "id": "D8fhbEH6ci5u",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# The fact that the start and end values are required to be floats is something I need to fix.\n",
        "kl_warmup_start \u003d 500.0 # batch number to start kl warmup, explicitly float\n",
        "kl_warmup_end \u003d 1000.0  # batch number to be finished with kl warmup, explicitly float\n",
        "kl_min \u003d 0.01 # The minimum KL value, non-zero to make sure KL doesn\u0027t grow crazy before kicking in."
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "collapsed": false,
        "id": "zofx0G6Aci5w",
        "colab_type": "text",
        "pycharm": {}
      },
      "source": [
        "Note, there is currently a HUGE amount of debate about what the correct parameter value here is for the KL penalty. kl_max \u003d 1 is what creates a lower bound on the (marginal) log likelihood of the data, but folks argue it could be higher or lower than 1. Myself, I have never played around with this HP, but I have the idea that LFADS may benefit from \u003c 1 values, as LFADS is not really being used for random sampling from the distribution of spiking data.\n",
        "\n",
        "See [$\\beta$-VAE: LEARNING BASIC VISUAL CONCEPTS WITH A\n",
        "CONSTRAINED VARIATIONAL FRAMEWORK](https://openreview.net/pdf?id\u003dSy2fzU9gl)\n",
        "\n",
        "See [Fixing a Broken ELBO](https://arxiv.org/pdf/1711.00464.pdf) as to why you might choose a particular kl maximum value.  I found this article pretty clarifying."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "pycharm": {
          "name": "#%%\n"
        },
        "id": "6_xeXhdWci5w",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "kl_max \u003d 1.0"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "pycharm": {
          "name": "#%%\n"
        },
        "id": "6Y7eoP7cci5y",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "lfads_opt_hps \u003d {\u0027num_batches\u0027 : num_batches, \u0027step_size\u0027 : step_size,\n",
        "                 \u0027decay_steps\u0027 : decay_steps, \u0027decay_factor\u0027 : decay_factor,\n",
        "                 \u0027kl_min\u0027 : kl_min, \u0027kl_max\u0027 : kl_max, \u0027kl_warmup_start\u0027 : kl_warmup_start,\n",
        "                 \u0027kl_warmup_end\u0027 : kl_warmup_end, \u0027keep_rate\u0027 : keep_rate,\n",
        "                 \u0027max_grad_norm\u0027 : max_grad_norm, \u0027print_every\u0027 : print_every,\n",
        "                 \u0027adam_b1\u0027 : 0.9, \u0027adam_b2\u0027 : 0.999, \u0027adam_eps\u0027 : 1e-1}\n",
        "\n",
        "assert num_batches \u003e\u003d print_every and num_batches % print_every \u003d\u003d 0"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "pycharm": {
          "name": "#%%\n"
        },
        "id": "EQEPNj7Wci50",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Plot the warmup function and the learning rate decay function.\n",
        "plt.figure(figsize\u003d(16,4))\n",
        "plt.subplot(121)\n",
        "x \u003d onp.arange(0, num_batches, print_every)\n",
        "kl_warmup_fun \u003d get_kl_warmup_fun(lfads_opt_hps)\n",
        "plt.plot(x, [kl_warmup_fun(i) for i in onp.arange(1,lfads_opt_hps[\u0027num_batches\u0027], print_every)]);\n",
        "plt.title(\u0027KL warmup function\u0027)\n",
        "plt.xlabel(\u0027Training batch\u0027);\n",
        "\n",
        "plt.subplot(122)\n",
        "decay_fun \u003d optimizers.exponential_decay(lfads_opt_hps[\u0027step_size\u0027],                                                             \n",
        "                                         lfads_opt_hps[\u0027decay_steps\u0027],                                                           \n",
        "                                         lfads_opt_hps[\u0027decay_factor\u0027])                                                          \n",
        "plt.plot(x, [decay_fun(i) for i in range(1, lfads_opt_hps[\u0027num_batches\u0027], print_every)]);\n",
        "plt.title(\u0027learning rate function\u0027)\n",
        "plt.xlabel(\u0027Training batch\u0027);                                                                                     "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "collapsed": false,
        "id": "eyd0EQOUci51",
        "colab_type": "text",
        "pycharm": {}
      },
      "source": [
        "### Train the LFADS model\n",
        "\n",
        "Note that JAX uses it\u0027s own setup to handle randomness and seeding the pseudo-random number generators.  You can read about it [here](https://github.com/google/jax/blob/master/README.md#random-numbers-are-different). If you want to modify the LFADS tutorial you *NEED* to understand this. Otherwise, not so big a deal if you are just messing around with LFADS hyperparameters or applying the tutorial to new data."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "pycharm": {
          "name": "#%%\n"
        },
        "id": "N_uD-OoKci51",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Initialize parameters for LFADS\n",
        "key \u003d random.PRNGKey(onp.random.randint(0, utils.MAX_SEED_INT))\n",
        "init_params \u003d lfads.lfads_params(key, lfads_hps)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "collapsed": false,
        "id": "IEBHs3oKci53",
        "colab_type": "text",
        "pycharm": {}
      },
      "source": [
        "Note that the first loop could take a few minutes to run, because the LFADS model is unrolled, and therefor the JIT (just in time) compilation is slow, and happens \"just in time\", which is the first training loop iteration. On my computer, the JIT compilation takes a few minutes.\n",
        "\n",
        "You\u0027ll see the loss go up when the KL warmup starts turning on.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "pycharm": {
          "name": "#%%\n"
        },
        "id": "02ue0L65ci54",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "key \u003d random.PRNGKey(onp.random.randint(0, utils.MAX_SEED_INT))\n",
        "trained_params, opt_details \u003d \\\n",
        "    optimize_lfads(key, init_params, lfads_hps, lfads_opt_hps,\n",
        "                   train_data, eval_data)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "pycharm": {
          "name": "#%%\n"
        },
        "id": "5i2Uulozci56",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Plot the training details\n",
        "x \u003d onp.arange(0, num_batches, print_every)\n",
        "plt.figure(figsize\u003d(20,6))\n",
        "plt.subplot(251)\n",
        "plt.plot(x, opt_details[\u0027tlosses\u0027][\u0027total\u0027], \u0027k\u0027)\n",
        "plt.ylabel(\u0027Training\u0027)\n",
        "plt.title(\u0027Total loss\u0027)\n",
        "plt.subplot(252)\n",
        "plt.plot(x, opt_details[\u0027tlosses\u0027][\u0027nlog_p_xgz\u0027], \u0027b\u0027)\n",
        "plt.title(\u0027Negative log p(z|x)\u0027)\n",
        "plt.subplot(253)\n",
        "plt.plot(x, opt_details[\u0027tlosses\u0027][\u0027kl_ii\u0027], \u0027r\u0027)\n",
        "plt.title(\u0027KL inferred inputs\u0027)\n",
        "plt.subplot(254)\n",
        "plt.plot(x, opt_details[\u0027tlosses\u0027][\u0027kl_g0\u0027], \u0027g\u0027)\n",
        "plt.title(\u0027KL initial state\u0027)\n",
        "plt.subplot(255)\n",
        "plt.plot(x, opt_details[\u0027tlosses\u0027][\u0027l2\u0027], \u0027c\u0027)\n",
        "plt.xlabel(\u0027Training batch\u0027)\n",
        "plt.title(\u0027L2 loss\u0027)\n",
        "plt.subplot(256)\n",
        "plt.plot(x, opt_details[\u0027elosses\u0027][\u0027total\u0027], \u0027k\u0027)\n",
        "plt.xlabel(\u0027Training batch\u0027)\n",
        "plt.ylabel(\u0027Evaluation\u0027)\n",
        "plt.subplot(257)\n",
        "plt.plot(x, opt_details[\u0027tlosses\u0027][\u0027nlog_p_xgz\u0027], \u0027b\u0027)\n",
        "plt.xlabel(\u0027Training batch\u0027)\n",
        "plt.subplot(258)\n",
        "plt.plot(x, opt_details[\u0027elosses\u0027][\u0027kl_ii\u0027], \u0027r\u0027)\n",
        "plt.xlabel(\u0027Training batch\u0027)\n",
        "plt.subplot(259)\n",
        "plt.plot(x, opt_details[\u0027elosses\u0027][\u0027kl_g0\u0027], \u0027g\u0027)\n",
        "plt.xlabel(\u0027Training batch\u0027);"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "pycharm": {
          "name": "#%%\n"
        },
        "id": "6H4vlTVCci57",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# See the effect of the KL warmup, which is shown \n",
        "# by the KL penalities without the warmup scaling. \n",
        "plt.figure(figsize\u003d(7,4))\n",
        "plt.subplot(221)\n",
        "plt.plot(x, opt_details[\u0027tlosses\u0027][\u0027kl_ii_prescale\u0027], \u0027r--\u0027)\n",
        "plt.ylabel(\u0027Training\u0027)\n",
        "plt.subplot(222)\n",
        "plt.plot(x, opt_details[\u0027tlosses\u0027][\u0027kl_g0_prescale\u0027], \u0027g--\u0027)\n",
        "plt.subplot(223)\n",
        "plt.plot(x, opt_details[\u0027elosses\u0027][\u0027kl_ii_prescale\u0027], \u0027r--\u0027)\n",
        "plt.ylabel(\u0027Evaluation\u0027)\n",
        "plt.xlabel(\u0027Training batch\u0027)\n",
        "plt.subplot(224)\n",
        "plt.plot(x, opt_details[\u0027elosses\u0027][\u0027kl_g0_prescale\u0027], \u0027g--\u0027)\n",
        "plt.xlabel(\u0027Trainign batch\u0027);"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "collapsed": false,
        "id": "1JYaBBY_ci5-",
        "colab_type": "text",
        "pycharm": {}
      },
      "source": [
        "### Save the LFADS model parameters"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "pycharm": {
          "name": "#%%\n"
        },
        "id": "0xCswkZGci5-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "fname_uniquifier \u003d datetime.datetime.now().strftime(\"%Y-%m-%d_%H:%M:%S\")\n",
        "network_fname \u003d (\u0027trained_params_\u0027 + rnn_type + \u0027_\u0027 + task_type + \u0027_\u0027 + \\\n",
        "                 fname_uniquifier + \u0027.npz\u0027)\n",
        "network_path \u003d os.path.join(output_dir, network_fname)\n",
        "\n",
        "# Note we are just using numpy save instead of h5 because the LFADS parameter \n",
        "# is nested dictionaries, something I couldn\u0027t get h5 to save down easily.\n",
        "print(\"Saving parameters: \", network_path)\n",
        "onp.savez(network_path, trained_params)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "pycharm": {
          "name": "#%%\n"
        },
        "id": "J2-yonahci5_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# After training, you can load these up, after locating the save file.\n",
        "if False:\n",
        "    loaded_params \u003d onp.load(network_path)\n",
        "    trained_params \u003d loaded_params[\u0027arr_0\u0027].item()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "collapsed": false,
        "id": "8Y85oiNgci6B",
        "colab_type": "text",
        "pycharm": {}
      },
      "source": [
        "### LFADS Visualization\n",
        "To plot the results of LFADS, namely the inferred quantities such as the inferred inputs, factors, or rates, we have to do a sample-and-average operation.  Remember, the latent variables for LFADS are the initial state and the inferred inputs, and they are per-trial *stochastic* codes, even for a *single trial*.  To get good inference for a given trial, we sample a large number of times from these per-trial stochastic latent variables, run the generator forward, and then average all the quantities of interest over the samples. \n",
        "\n",
        "If LFADS were linear a linear model, it would be equivalent to do the *much more efficient decode* of the posterior means, that is, just take the mean of the initial state distribution and the mean of the inferred input distribution, and then run the decoder one time. (This, btw, is a great exercise to the tutorial reader: implement posterior-mean decoding in this tutorial.)\n",
        "\n",
        "Here we use batching and take the \u0027posterior average\u0027 using batch number of samples from the latent variable distributions.\n",
        "\n",
        "So the main result of this tutorial, the moment you\u0027ve been waiting for, is the comparison between the true rates of the integrator RNN, and the inferred rates by LFADS, and the true input to the integrator RNN and the inferred inputs given by LFADS.  You can see how well we did by generating lots of trials here."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "pycharm": {
          "name": "#%%\n"
        },
        "id": "KhNsEYIqci6B",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Plot a bunch of examples of eval trials run through LFADS.\n",
        "reload(plotting)\n",
        "\n",
        "def plot_rescale_fun(a): \n",
        "    fac \u003d max_firing_rate * data_dt\n",
        "    return renormed_fun(a) * fac\n",
        "\n",
        "bidx \u003d my_example_bidx - eval_data_offset\n",
        "\n",
        "nexamples_to_save \u003d 1\n",
        "for eidx in range(nexamples_to_save):\n",
        "    fkey \u003d random.fold_in(key, eidx)\n",
        "    psa_example \u003d eval_data[bidx,:,:].astype(np.float32)\n",
        "    psa_dict \u003d lfads.posterior_sample_and_average_jit(trained_params, lfads_hps, fkey, psa_example)\n",
        "\n",
        "    # The inferred input and true input are rescaled and shifted via \n",
        "    # linear regression to match, as there is an identifiability issue. there.\n",
        "    plotting.plot_lfads(psa_example, psa_dict,\n",
        "                        data_dict, eval_data_offset+bidx, plot_rescale_fun)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "collapsed": false,
        "id": "A-yk0yoGci6C",
        "colab_type": "text",
        "pycharm": {}
      },
      "source": [
        "And coming back to our example signal, how well does LFADS do on it, compared to the other *much easier to implement* methods? A noticable improvement on inferring the underlying rate."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "pycharm": {
          "name": "#%%\n"
        },
        "id": "JTBptnuoci6D",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "plt.figure(figsize\u003d(16,4))\n",
        "\n",
        "plt.subplot(141)\n",
        "plt.plot(my_signal, \u0027r\u0027);\n",
        "plt.stem(my_signal_spikified);\n",
        "_, _, r2_spike, _, _ \u003d scipy.stats.linregress(my_signal_spikified, my_signal)\n",
        "plt.title(\u0027Raw spikes R^2\u003d%.3f\u0027 % (r2_spike))\n",
        "plt.legend((\u0027True rate\u0027, \u0027Spikes\u0027));\n",
        "\n",
        "\n",
        "plt.subplot(142)\n",
        "plt.plot(my_signal, \u0027r\u0027);\n",
        "plt.plot(my_filtered_spikes);\n",
        "_, _, r2_tfilt, _, _ \u003d scipy.stats.linregress(my_filtered_spikes, my_signal)\n",
        "plt.title(\"Temporal filtering  R^2\u003d%.3f\" % (r2_tfilt));\n",
        "plt.legend((\u0027True rate\u0027, \u0027Filtered spikes\u0027));\n",
        "\n",
        "plt.subplot(143)\n",
        "plt.plot(my_signal, \u0027r\u0027)\n",
        "plt.plot(my_example_ipca[:,my_example_hidx])\n",
        "_, _, r2_sfilt, _, _ \u003d scipy.stats.linregress(my_example_ipca[:,my_example_hidx], my_signal)\n",
        "plt.legend((\u0027True rate\u0027, \u0027PCA smoothed spikes\u0027))\n",
        "plt.title(\u0027Spatial filtering R^2\u003d%.3f\u0027 % (r2_sfilt));\n",
        "\n",
        "plt.subplot(144)\n",
        "plt.plot(my_signal, \u0027r\u0027)\n",
        "my_lfads_rate \u003d onp.exp(psa_dict[\u0027lograte_t\u0027][:,my_example_hidx])\n",
        "plt.plot(my_lfads_rate)\n",
        "_, _, r2_lfads, _, _ \u003d scipy.stats.linregress(my_lfads_rate, my_signal)\n",
        "plt.legend((\u0027True rate\u0027, \u0027LFADS rate\u0027))\n",
        "plt.title(\u0027LFADS \"filtering\" R^2\u003d%.3f\u0027 % (r2_lfads));"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "collapsed": false,
        "id": "XMVhbaGpci6E",
        "colab_type": "text",
        "pycharm": {}
      },
      "source": [
        "That single example can\u0027t tell the whole story so let us look at the average. LFADS is much better than spatial averaging across a large set of trials.\n",
        "\n",
        "Take an average over all the hidden units in 100 evaluation trials."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "pycharm": {
          "name": "#%%\n"
        },
        "id": "-1zqZKeNci6E",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "r2_sfilts \u003d []\n",
        "r2_lfadss \u003d []\n",
        "for bidx in range(100):\n",
        "    ebidx \u003d eval_data_offset + bidx\n",
        "    \n",
        "    # Get the LFADS decode for this trial.\n",
        "    fkey \u003d random.fold_in(key, bidx)\n",
        "    psa_example \u003d eval_data[bidx,:,:].astype(np.float32)\n",
        "    psa_dict \u003d lfads.posterior_sample_and_average_jit(trained_params, lfads_hps, fkey, psa_example)\n",
        "    \n",
        "    # Get the spatially smoothed trial.\n",
        "    trial_rates \u003d scale*renormed_data[ebidx, :, :]\n",
        "    trial_spikes \u003d data_bxtxn[ebidx, :, :]\n",
        "    spikes_pca \u003d pca.transform(trial_spikes)\n",
        "    spikes_ipca \u003d pca.inverse_transform(spikes_pca)\n",
        "    \n",
        "    for hidx in range(data_dim):\n",
        "        sig \u003d trial_rates[:, hidx]\n",
        "        _, _, r2_sfilt, _, _ \u003d scipy.stats.linregress(spikes_ipca[:,hidx], sig)\n",
        "\n",
        "        lr \u003d onp.exp(psa_dict[\u0027lograte_t\u0027][:,hidx])\n",
        "        _, _, r2_lfads, _, _ \u003d scipy.stats.linregress(lr, sig)\n",
        "\n",
        "        r2_sfilts.append(r2_sfilt)\n",
        "        r2_lfadss.append(r2_lfads)\n",
        "    \n",
        "r2_sfilts \u003d onp.array(r2_sfilts)\n",
        "r2_lfadss \u003d onp.array(r2_lfadss)\n",
        "\n",
        "plt.figure(figsize\u003d(8,4))\n",
        "plt.subplot(121)\n",
        "plt.hist(r2_sfilts, 50)\n",
        "plt.title(\u0027Spatial filtering, hist of R^2, \u003c%.3f\u003e\u0027 % (onp.mean(r2_sfilts)))\n",
        "plt.xlim([-.5, 1.0])\n",
        "\n",
        "plt.subplot(122)\n",
        "plt.hist(r2_lfadss, 50);\n",
        "plt.title(\u0027LFADS filtering, hist of R^2, \u003c%.3f\u003e\u0027 % (onp.mean(r2_lfadss)));\n",
        "plt.xlim([-.5, 1.0]);"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "collapsed": false,
        "id": "EGse517cci6G",
        "colab_type": "text",
        "pycharm": {}
      },
      "source": [
        "### Compare the inferred inputs learned by LFADS to the actual inputs to the integrator RNN. \n",
        "\n",
        "Finally, we can look at the average correlation between the inferred inputs and the true inputs to the integrator RNN. The inferred input can be arbitrarily scaled or rotated, so we first compute a linear regression, to scale the inferred input correctly, and then get the $R^2$."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "pycharm": {
          "name": "#%%\n"
        },
        "id": "hqCTU8vWci6H",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "r2_iis \u003d []\n",
        "for bidx in range(200):\n",
        "    ebidx \u003d eval_data_offset + bidx\n",
        "    \n",
        "    # Get the LFADS decode for this trial.\n",
        "    psa_example \u003d eval_data[bidx,:,:].astype(np.float32)\n",
        "    fkey \u003d random.fold_in(key, bidx)\n",
        "    psa_dict \u003d lfads.posterior_sample_and_average_jit(trained_params, lfads_hps, fkey, psa_example)\n",
        "    \n",
        "    # Get the true input and inferred input\n",
        "    true_input \u003d onp.squeeze(data_dict[\u0027inputs\u0027][ebidx])\n",
        "    inferred_input \u003d onp.squeeze(psa_dict[\u0027ii_t\u0027])\n",
        "    slope, intercept, _, _, _ \u003d scipy.stats.linregress(inferred_input, true_input)\n",
        "    _, _, r2_ii, _, _ \u003d scipy.stats.linregress(slope * inferred_input + intercept, true_input)\n",
        "    \n",
        "    r2_iis.append(r2_ii)\n",
        "    \n",
        "r2_iis \u003d onp.array(r2_iis)\n",
        "\n",
        "plt.hist(r2_iis, 20)\n",
        "plt.title(\u0027Correlation between rescaled inferrred inputs and true inputs, hist of R^2, \u003c%.3f\u003e\u0027 % (onp.mean(r2_iis)))\n",
        "plt.xlim([0.0, 1.0]);"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "collapsed": false,
        "id": "pS0xETEBci6J",
        "colab_type": "text",
        "pycharm": {}
      },
      "source": [
        "### Compare the inferred initial state for the LFADS generator to the actual initial state of the integrator RNN.\n",
        "\n",
        "To finish, we can examine the relationship between the initial condition (h0) of the integrator RNN and the inferred initial condition of the LFADS generator.\n",
        "The color we use is the readout of the integrator RNN\u0027s initial state, so basically the state of the line attractor before further information is presented.  In the integrator RNN example, we made sure to seed these intial states with a various values along the line attractor, so we expect a line of coloration."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "pycharm": {
          "name": "#%%\n"
        },
        "id": "CF-x8SUlci6K",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "ntrials \u003d 1000\n",
        "true_h0s \u003d onp.zeros([ntrials, data_dim])\n",
        "ic_means \u003d onp.zeros([ntrials, gen_dim])\n",
        "colors \u003d onp.zeros(ntrials)\n",
        "for bidx in range(ntrials):\n",
        "    ebidx \u003d eval_data_offset + bidx\n",
        "    \n",
        "    # Get the LFADS decode for this trial.\n",
        "    psa_example \u003d eval_data[bidx,:,:].astype(np.float32)\n",
        "    fkey \u003d random.fold_in(key, bidx)\n",
        "    psa_dict \u003d lfads.posterior_sample_and_average_jit(trained_params, lfads_hps, fkey, psa_example)\n",
        "    \n",
        "    # Get the true initial condition (and the readout of the true h0 for color)\n",
        "    # Get the inferred input from LFADS\n",
        "    true_h0s[bidx,:] \u003d data_dict[\u0027h0s\u0027][ebidx]\n",
        "    colors[bidx] \u003d data_dict[\u0027outputs\u0027][ebidx][0]\n",
        "    ic_means[bidx,:] \u003d psa_dict[\u0027ic_mean\u0027]\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "pycharm": {
          "name": "#%%\n"
        },
        "id": "kwBIsIYTci6L",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.manifold import TSNE\n",
        "plt.figure(figsize\u003d(16,8))\n",
        "plt.subplot(121)\n",
        "h0s_embedded \u003d TSNE(n_components\u003d2).fit_transform(true_h0s)\n",
        "plt.scatter(h0s_embedded[:,0], h0s_embedded[:,1], c\u003dcolors)\n",
        "plt.title(\u0027TSNE visualization of integrator RNN intial state\u0027)\n",
        "plt.subplot(122)\n",
        "ic_means_embedded \u003d TSNE(n_components\u003d2).fit_transform(ic_means)\n",
        "plt.scatter(ic_means_embedded[:,0], ic_means_embedded[:,1], c\u003dcolors);\n",
        "plt.title(\u0027TSNE visualziation of LFADS inferred intial generator state.\u0027)"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}