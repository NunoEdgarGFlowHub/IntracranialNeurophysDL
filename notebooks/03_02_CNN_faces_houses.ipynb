{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# CNNs to Decode ECoG Data\n",
    "Run the first few cells to normalize Local / Colab environments, then proceed below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import os\n",
    "try:\n",
    "    # See if we are running on google.colab\n",
    "    import google.colab\n",
    "    from google.colab import files\n",
    "    os.chdir('..')\n",
    "    if not (Path.cwd() / '.kaggle').is_dir():\n",
    "        # Configure kaggle\n",
    "        files.upload()  # Find the kaggle.json file in your ~/.kaggle directory.\n",
    "        !pip install -q kaggle\n",
    "        !mkdir -p ~/.kaggle\n",
    "        !mv kaggle.json ~/.kaggle/\n",
    "        !chmod 600 ~/.kaggle/kaggle.json\n",
    "    if not (Path.cwd() / 'repo').is_dir():\n",
    "        # Download the workshop repo and change to its directory\n",
    "        # For now edit the username/password. This requirement will be removed when the repo is made public.\n",
    "        !git clone https://github.com/SachsLab/IntracranialNeurophysDL.git\n",
    "        os.chdir('IntracranialNeurophysDL')\n",
    "    IN_COLAB = True\n",
    "    # Setup tensorflow 2.0\n",
    "    !pip install -q tensorflow-gpu==2.0.0-alpha0\n",
    "    import tensorflow as tf\n",
    "    # Setup tensorboard callback\n",
    "    !pip install tensorboardcolab\n",
    "    from tensorboardcolab import TensorBoardColab, TensorBoardColabCallback\n",
    "    tbc=TensorBoardColab(startup_waiting_time=30)\n",
    "    tensorboard_callback = TensorBoardColabCallback(tbc)\n",
    "except ModuleNotFoundError:\n",
    "    IN_COLAB = False\n",
    "    import sys\n",
    "    if Path.cwd().stem == 'notebooks':\n",
    "        os.chdir(Path.cwd().parent)\n",
    "    # Make sure the kaggle executable is on the PATH\n",
    "    os.environ['PATH'] = os.environ['PATH'] + ';' + str(Path(sys.executable).parent / 'Scripts')\n",
    "    # Clear any logs from previous runs\n",
    "    if (Path.cwd() / 'logs').is_dir():\n",
    "        import platform\n",
    "        if platform.system() == 'Windows':\n",
    "            !rmdir /S /Q logs\n",
    "        else:\n",
    "            !rm -Rf logs\n",
    "    # Setup tensorboard callback\n",
    "    import tensorflow as tf\n",
    "    import datetime\n",
    "    %load_ext tensorboard.notebook\n",
    "    log_dir = Path.cwd() / \"logs\" / datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "    tensorboard_callback = tf.keras.callbacks.TensorBoard(log_dir=log_dir, histogram_freq=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Download and unzip data\n",
    "datadir = Path.cwd() / 'data' / 'kjm_ecog'\n",
    "if not (datadir / 'converted').is_dir():\n",
    "    !kaggle datasets download -d cboulay/kjm-ecog-faces-basic\n",
    "    print(\"Finished downloading. Now extracting contents...\")\n",
    "    data_path = Path('kjm-ecog-faces-basic.zip')\n",
    "    import zipfile\n",
    "    with zipfile.ZipFile(data_path, 'r') as zip_ref:\n",
    "        zip_ref.extractall(datadir / 'converted' / 'faces_basic')\n",
    "    data_path.unlink()\n",
    "    print(\"Finished extracting data.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Prepare data from one participant\n",
    "See 02_02 for an explanation.\n",
    "However, this time we are loading the full-band data time-domain data\n",
    "at the original sampling rate.\n",
    "### Load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 603 trials, 801 timestamps (-0.2 to 0.6 at 1000 Hz), 58 channels\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from data.utils.fileio import from_neuropype_h5\n",
    "\n",
    "# define a helper function to load the data\n",
    "def load_faces_houses(sub_id):\n",
    "    test_file = datadir / 'converted' / 'faces_basic' / (sub_id + '_full.h5')\n",
    "    chunks = from_neuropype_h5(test_file)\n",
    "    chunk_names = [_[0] for _ in chunks]\n",
    "    chunk = chunks[chunk_names.index('signals')][1]\n",
    "    ax_types = [_['type'] for _ in chunk['axes']]\n",
    "    instance_axis = chunk['axes'][ax_types.index('instance')]\n",
    "    n_trials = len(instance_axis['data'])\n",
    "    X = chunk['data']\n",
    "    Y = instance_axis['data']['Marker'].values.reshape(-1, 1)\n",
    "    ax_info = {'instance_data': instance_axis['data'],\n",
    "               'fs': chunk['axes'][ax_types.index('time')]['nominal_rate'],\n",
    "               'timestamps': chunk['axes'][ax_types.index('time')]['times'],\n",
    "               'channel_names': chunk['axes'][ax_types.index('space')]['names']\n",
    "              }\n",
    "    return X, Y, ax_info\n",
    "\n",
    "# Load data from one participant.\n",
    "SUB_ID = 'mv'\n",
    "X, Y, ax_info = load_faces_houses(SUB_ID)\n",
    "n_trials = X.shape[0]\n",
    "\n",
    "print(\"Found {} trials, {} timestamps ({} to {} at {} Hz), {} channels\".format(\n",
    "    n_trials, len(ax_info['timestamps']), ax_info['timestamps'][0], ax_info['timestamps'][-1],\n",
    "    ax_info['fs'], X.shape[-1]))\n",
    "# print(ax_info['channel_names'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get baseline accuracy\n",
    "Remember, the signals haven't been processed much. The only features we can expect LDA to pick out are stimulus-evoked time-domain signals. Also with 801 * 58 features, over-fitting is likely. Unfortunately, the LDA 'eigen' solver can't handle this ~600 x ~4800 matrix so we can't use its shrinkage. So we'll try regularized logistic regression instead. (Though it has 'regression' in the name, it's actually a classification algorithm)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3-class accuracy: 92.20563847429518\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "\n",
    "args = {\n",
    "    'solver': 'lbfgs',\n",
    "    'C': 10.0,  # inverse regularization strength\n",
    "    'penalty': 'l2',\n",
    "    'multi_class': 'ovr',\n",
    "    'max_iter': 200\n",
    "}\n",
    "model = LogisticRegression(**args)\n",
    "splitter = StratifiedKFold(n_splits=10)\n",
    "\n",
    "y_preds = []\n",
    "y_true = []\n",
    "for trn, tst in splitter.split(X, Y):\n",
    "    model.fit(X[trn].reshape(-1, np.prod(X.shape[1:])), Y[trn].ravel())\n",
    "    y_preds.append(model.predict(X[tst].reshape(-1, np.prod(X.shape[1:]))))\n",
    "    y_true.append(Y[tst].ravel())\n",
    "\n",
    "y_preds = np.hstack(y_preds)\n",
    "y_true = np.hstack(y_true)\n",
    "\n",
    "pcnt_corr = 100 * np.sum(y_preds == y_true) / len(y_preds)\n",
    "print(\"3-class accuracy: {}\".format(pcnt_corr))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "So even with way more features and without signal processing, we're getting fairly high accuracy at 92%. Our goal is for the deep model to do at least as well and to hopefully provide some insight along the way."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prepare data for deep learning\n",
    "See 02_02 notebook for explanation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def get_ds_train_valid(X, Y, p_train=0.8, batch_size=5):\n",
    "    # Convert Y from strings to integers.\n",
    "    classes, y = np.unique(Y, return_inverse=True)\n",
    "    n_classes = len(classes)\n",
    "    n_trials = len(y)\n",
    "    \n",
    "    def preprocess_fn(x_dat, y_dat):\n",
    "        x_dat = tf.cast(x_dat, tf.float32)\n",
    "        y_dat = tf.one_hot(tf.cast(y_dat, tf.uint8), n_classes)\n",
    "        return x_dat, y_dat\n",
    "    \n",
    "    X_train, X_valid, y_train, y_valid = train_test_split(X, y, train_size=p_train)\n",
    "    n_train = len(y_train)\n",
    "    n_valid = len(y_valid)\n",
    "    ds_train = tf.data.Dataset.from_tensor_slices((X_train, y_train))\n",
    "    ds_valid = tf.data.Dataset.from_tensor_slices((X_valid, y_valid))\n",
    "    \n",
    "    ds_train = ds_train.map(preprocess_fn)\n",
    "    ds_valid = ds_valid.map(preprocess_fn)\n",
    "    ds_train = ds_train.shuffle(int(n_trials * p_train) + 1).batch(batch_size, drop_remainder=True).repeat()  # , drop_remainder=True?\n",
    "    ds_valid = ds_valid.batch(batch_size).repeat()\n",
    "    \n",
    "    return ds_train, ds_valid, n_train\n",
    "    \n",
    "PTRAIN = 0.8\n",
    "BATCH_SIZE = 5\n",
    "ds_train, ds_valid, n_train = get_ds_train_valid(X, Y, p_train=PTRAIN, batch_size=BATCH_SIZE)\n",
    "\n",
    "# TODO: proper cross-validation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Create our model\n",
    "As in 02_02, our objective is to decode the stimulus class ('ISI', 'face', or 'house')\n",
    "from the ECoG data.\n",
    "Let's start off with a model that someone else developed for similar purposes.\n",
    "\n",
    "* [One for EEG](https://iopscience.iop.org/article/10.1088/1741-2552/aaf3f6)\n",
    "* [ECoG CNNs with LSTMs](https://iopscience.iop.org/article/10.1088/1741-2552/aa9dbe/meta)\n",
    "    1. CNN for spatial unmixing matrix [77] initialized with fast ICA\n",
    "    1. 3 temporal CNN layers, with [2, 4, 8] filters of kernel size 1 x 17 per layer,\n",
    "    initialized with biorthogonal wavelet 6.8.\n",
    "* [ECoG CNNs with autoencoders](https://iopscience.iop.org/article/10.1088/1741-2552/aaf13f/pdf)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_1 (InputLayer)         [(None, 801, 58)]         0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 801, 64)           3776      \n",
      "_________________________________________________________________\n",
      "batch_normalization_v2 (Batc (None, 801, 64)           256       \n",
      "_________________________________________________________________\n",
      "conv1d (Conv1D)              (None, 790, 64)           49216     \n",
      "_________________________________________________________________\n",
      "max_pooling1d (MaxPooling1D) (None, 263, 64)           0         \n",
      "_________________________________________________________________\n",
      "dropout (Dropout)            (None, 263, 64)           0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_v2_1 (Ba (None, 263, 64)           256       \n",
      "_________________________________________________________________\n",
      "conv1d_1 (Conv1D)            (None, 252, 64)           49216     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_1 (MaxPooling1 (None, 84, 64)            0         \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 84, 64)            0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_v2_2 (Ba (None, 84, 64)            256       \n",
      "_________________________________________________________________\n",
      "conv1d_2 (Conv1D)            (None, 77, 32)            16416     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_2 (MaxPooling1 (None, 25, 32)            0         \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 25, 32)            0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_v2_3 (Ba (None, 25, 32)            128       \n",
      "_________________________________________________________________\n",
      "conv1d_3 (Conv1D)            (None, 23, 16)            1552      \n",
      "_________________________________________________________________\n",
      "max_pooling1d_3 (MaxPooling1 (None, 7, 16)             0         \n",
      "_________________________________________________________________\n",
      "dropout_3 (Dropout)          (None, 7, 16)             0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_v2_4 (Ba (None, 7, 16)             64        \n",
      "_________________________________________________________________\n",
      "conv1d_4 (Conv1D)            (None, 5, 8)              392       \n",
      "_________________________________________________________________\n",
      "max_pooling1d_4 (MaxPooling1 (None, 1, 8)              0         \n",
      "_________________________________________________________________\n",
      "dropout_4 (Dropout)          (None, 1, 8)              0         \n",
      "_________________________________________________________________\n",
      "flatten (Flatten)            (None, 8)                 0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 3)                 27        \n",
      "=================================================================\n",
      "Total params: 121,555\n",
      "Trainable params: 121,075\n",
      "Non-trainable params: 480\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# Create a convenience function for adding a multilayer block\n",
    "def cnn_block(inputs, n_filters=16, filter_length=8, activation='relu', pool_size=1, strides=None, dropout_rate=0.0):\n",
    "    y_ = inputs\n",
    "    y_ = tf.keras.layers.BatchNormalization()(y_)\n",
    "    y_ = tf.keras.layers.Conv1D(n_filters, filter_length, activation=activation)(y_)\n",
    "    if pool_size > 1:\n",
    "        y_ = tf.keras.layers.MaxPooling1D(pool_size=pool_size, strides=strides)(y_)\n",
    "    if dropout_rate > 0.0:\n",
    "        y_ = tf.keras.layers.Dropout(rate=dropout_rate)(y_)\n",
    "    return y_\n",
    "\n",
    "\n",
    "N_FILTERS = [64, 64, 32, 16, 8]\n",
    "FILTER_LENGTHS = [12, 12, 8, 3, 3]\n",
    "\n",
    "inputs = tf.keras.layers.Input(shape=X.shape[1:])\n",
    "y_ = tf.keras.layers.Dense(64, activation='linear')(inputs)  # spat filt\n",
    "# y_ = tf.keras.layers.Conv1D(64, 1)(inputs)  # spat filt.\n",
    "for n_filts, filt_length in zip(N_FILTERS, FILTER_LENGTHS):\n",
    "    y_ = cnn_block(y_, n_filters=n_filts, filter_length=filt_length, pool_size=3, dropout_rate=0.2)\n",
    "y_ = tf.keras.layers.Flatten()(y_)\n",
    "outputs = tf.keras.layers.Dense(3, activation='softmax')(y_)\n",
    "model = tf.keras.Model(inputs, outputs)\n",
    "\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "model.summary()\n",
    "\n",
    "Wsave = model.get_weights()  # Save the weights for later"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      " 1/96 [..............................] - ETA: 12:23 - loss: 0.7930 - accuracy: 0.6000"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Logging before flag parsing goes to stderr.\n",
      "W0526 00:47:45.588757  6044 callbacks.py:236] Method (on_train_batch_end) is slow compared to the batch update (0.231905). Check your callbacks.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 2/96 [..............................] - ETA: 6:22 - loss: 1.6342 - accuracy: 0.5000 "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0526 00:47:45.612051  6044 callbacks.py:236] Method (on_train_batch_end) is slow compared to the batch update (0.187480). Check your callbacks.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "96/96 [==============================] - ETA: 2:28 - loss: 1.3963 - accuracy: 0.52 - ETA: 1:10 - loss: 1.3199 - accuracy: 0.42 - ETA: 48s - loss: 1.4504 - accuracy: 0.3714 - ETA: 36s - loss: 1.3882 - accuracy: 0.411 - ETA: 28s - loss: 1.3562 - accuracy: 0.436 - ETA: 22s - loss: 1.3281 - accuracy: 0.453 - ETA: 17s - loss: 1.2637 - accuracy: 0.483 - ETA: 14s - loss: 1.2648 - accuracy: 0.468 - ETA: 12s - loss: 1.2471 - accuracy: 0.471 - ETA: 10s - loss: 1.2382 - accuracy: 0.469 - ETA: 8s - loss: 1.2248 - accuracy: 0.475 - ETA: 7s - loss: 1.2249 - accuracy: 0.46 - ETA: 6s - loss: 1.2263 - accuracy: 0.47 - ETA: 5s - loss: 1.1793 - accuracy: 0.49 - ETA: 4s - loss: 1.1706 - accuracy: 0.49 - ETA: 3s - loss: 1.1465 - accuracy: 0.50 - ETA: 2s - loss: 1.1457 - accuracy: 0.50 - ETA: 2s - loss: 1.1278 - accuracy: 0.51 - ETA: 1s - loss: 1.1386 - accuracy: 0.50 - ETA: 1s - loss: 1.1389 - accuracy: 0.50 - ETA: 0s - loss: 1.1432 - accuracy: 0.50 - ETA: 0s - loss: 1.1223 - accuracy: 0.50 - 10s 107ms/step - loss: 1.1167 - accuracy: 0.5042 - val_loss: 0.6403 - val_accuracy: 0.6583\n",
      "Epoch 2/20\n",
      "96/96 [==============================] - ETA: 8s - loss: 0.7490 - accuracy: 0.60 - ETA: 2s - loss: 1.1199 - accuracy: 0.60 - ETA: 1s - loss: 1.0912 - accuracy: 0.60 - ETA: 1s - loss: 0.9592 - accuracy: 0.63 - ETA: 1s - loss: 0.9802 - accuracy: 0.58 - ETA: 1s - loss: 0.9355 - accuracy: 0.60 - ETA: 1s - loss: 0.9259 - accuracy: 0.60 - ETA: 1s - loss: 0.8969 - accuracy: 0.60 - ETA: 0s - loss: 0.8748 - accuracy: 0.60 - ETA: 0s - loss: 0.8867 - accuracy: 0.59 - ETA: 0s - loss: 0.8838 - accuracy: 0.60 - ETA: 0s - loss: 0.8685 - accuracy: 0.60 - ETA: 0s - loss: 0.8721 - accuracy: 0.60 - ETA: 0s - loss: 0.8557 - accuracy: 0.60 - ETA: 0s - loss: 0.8731 - accuracy: 0.58 - ETA: 0s - loss: 0.8814 - accuracy: 0.58 - ETA: 0s - loss: 0.8675 - accuracy: 0.58 - ETA: 0s - loss: 0.8651 - accuracy: 0.58 - ETA: 0s - loss: 0.8853 - accuracy: 0.57 - ETA: 0s - loss: 0.8845 - accuracy: 0.56 - ETA: 0s - loss: 0.8767 - accuracy: 0.57 - ETA: 0s - loss: 0.8673 - accuracy: 0.57 - ETA: 0s - loss: 0.8577 - accuracy: 0.57 - ETA: 0s - loss: 0.8457 - accuracy: 0.58 - 2s 20ms/step - loss: 0.8343 - accuracy: 0.5896 - val_loss: 0.5238 - val_accuracy: 0.7500\n",
      "Epoch 3/20\n",
      "96/96 [==============================] - ETA: 7s - loss: 0.7859 - accuracy: 0.80 - ETA: 3s - loss: 0.6503 - accuracy: 0.75 - ETA: 2s - loss: 0.7431 - accuracy: 0.70 - ETA: 1s - loss: 0.7146 - accuracy: 0.70 - ETA: 1s - loss: 0.7439 - accuracy: 0.66 - ETA: 1s - loss: 0.7802 - accuracy: 0.65 - ETA: 1s - loss: 0.7563 - accuracy: 0.65 - ETA: 1s - loss: 0.7829 - accuracy: 0.65 - ETA: 0s - loss: 0.7480 - accuracy: 0.65 - ETA: 0s - loss: 0.7549 - accuracy: 0.64 - ETA: 0s - loss: 0.7506 - accuracy: 0.64 - ETA: 0s - loss: 0.7588 - accuracy: 0.63 - ETA: 0s - loss: 0.7460 - accuracy: 0.64 - ETA: 0s - loss: 0.7418 - accuracy: 0.65 - ETA: 0s - loss: 0.7464 - accuracy: 0.64 - ETA: 0s - loss: 0.7426 - accuracy: 0.65 - ETA: 0s - loss: 0.7401 - accuracy: 0.64 - ETA: 0s - loss: 0.7303 - accuracy: 0.64 - ETA: 0s - loss: 0.7288 - accuracy: 0.64 - ETA: 0s - loss: 0.7275 - accuracy: 0.64 - ETA: 0s - loss: 0.7287 - accuracy: 0.63 - ETA: 0s - loss: 0.7181 - accuracy: 0.63 - ETA: 0s - loss: 0.7146 - accuracy: 0.63 - 2s 21ms/step - loss: 0.7084 - accuracy: 0.6438 - val_loss: 0.5032 - val_accuracy: 0.7000\n",
      "Epoch 4/20\n",
      "96/96 [==============================] - ETA: 8s - loss: 0.7312 - accuracy: 0.60 - ETA: 2s - loss: 0.6856 - accuracy: 0.68 - ETA: 1s - loss: 0.6806 - accuracy: 0.64 - ETA: 1s - loss: 0.7351 - accuracy: 0.58 - ETA: 1s - loss: 0.7479 - accuracy: 0.58 - ETA: 1s - loss: 0.7137 - accuracy: 0.60 - ETA: 1s - loss: 0.7272 - accuracy: 0.60 - ETA: 1s - loss: 0.7063 - accuracy: 0.61 - ETA: 0s - loss: 0.6697 - accuracy: 0.63 - ETA: 0s - loss: 0.6669 - accuracy: 0.64 - ETA: 0s - loss: 0.6420 - accuracy: 0.66 - ETA: 0s - loss: 0.6347 - accuracy: 0.67 - ETA: 0s - loss: 0.6434 - accuracy: 0.66 - ETA: 0s - loss: 0.6257 - accuracy: 0.67 - ETA: 0s - loss: 0.6182 - accuracy: 0.68 - ETA: 0s - loss: 0.6254 - accuracy: 0.67 - ETA: 0s - loss: 0.6202 - accuracy: 0.67 - ETA: 0s - loss: 0.6132 - accuracy: 0.67 - ETA: 0s - loss: 0.6029 - accuracy: 0.69 - ETA: 0s - loss: 0.6035 - accuracy: 0.69 - ETA: 0s - loss: 0.6157 - accuracy: 0.68 - ETA: 0s - loss: 0.6269 - accuracy: 0.68 - ETA: 0s - loss: 0.6213 - accuracy: 0.68 - ETA: 0s - loss: 0.6254 - accuracy: 0.68 - 2s 20ms/step - loss: 0.6272 - accuracy: 0.6854 - val_loss: 0.4768 - val_accuracy: 0.7250\n",
      "Epoch 5/20\n",
      "96/96 [==============================] - ETA: 8s - loss: 0.4704 - accuracy: 0.80 - ETA: 2s - loss: 0.5620 - accuracy: 0.84 - ETA: 1s - loss: 0.5410 - accuracy: 0.77 - ETA: 1s - loss: 0.6173 - accuracy: 0.72 - ETA: 1s - loss: 0.6128 - accuracy: 0.73 - ETA: 1s - loss: 0.6314 - accuracy: 0.70 - ETA: 1s - loss: 0.5925 - accuracy: 0.71 - ETA: 1s - loss: 0.5715 - accuracy: 0.72 - ETA: 0s - loss: 0.5542 - accuracy: 0.73 - ETA: 0s - loss: 0.5390 - accuracy: 0.73 - ETA: 0s - loss: 0.5482 - accuracy: 0.73 - ETA: 0s - loss: 0.5609 - accuracy: 0.72 - ETA: 0s - loss: 0.5700 - accuracy: 0.71 - ETA: 0s - loss: 0.5561 - accuracy: 0.72 - ETA: 0s - loss: 0.5582 - accuracy: 0.72 - ETA: 0s - loss: 0.5585 - accuracy: 0.72 - ETA: 0s - loss: 0.5632 - accuracy: 0.72 - ETA: 0s - loss: 0.5580 - accuracy: 0.72 - ETA: 0s - loss: 0.5624 - accuracy: 0.72 - ETA: 0s - loss: 0.5588 - accuracy: 0.72 - ETA: 0s - loss: 0.5579 - accuracy: 0.71 - ETA: 0s - loss: 0.5530 - accuracy: 0.72 - ETA: 0s - loss: 0.5515 - accuracy: 0.72 - ETA: 0s - loss: 0.5570 - accuracy: 0.72 - 2s 20ms/step - loss: 0.5571 - accuracy: 0.7208 - val_loss: 0.4348 - val_accuracy: 0.8083\n",
      "Epoch 6/20\n",
      "96/96 [==============================] - ETA: 9s - loss: 0.9160 - accuracy: 0.40 - ETA: 2s - loss: 0.7707 - accuracy: 0.60 - ETA: 1s - loss: 0.6691 - accuracy: 0.70 - ETA: 1s - loss: 0.6683 - accuracy: 0.67 - ETA: 1s - loss: 0.6071 - accuracy: 0.68 - ETA: 1s - loss: 0.6171 - accuracy: 0.69 - ETA: 1s - loss: 0.6176 - accuracy: 0.70 - ETA: 1s - loss: 0.6165 - accuracy: 0.70 - ETA: 0s - loss: 0.6121 - accuracy: 0.69 - ETA: 0s - loss: 0.5989 - accuracy: 0.71 - ETA: 0s - loss: 0.5934 - accuracy: 0.71 - ETA: 0s - loss: 0.5952 - accuracy: 0.71 - ETA: 0s - loss: 0.5758 - accuracy: 0.72 - ETA: 0s - loss: 0.5691 - accuracy: 0.73 - ETA: 0s - loss: 0.5534 - accuracy: 0.74 - ETA: 0s - loss: 0.5465 - accuracy: 0.73 - ETA: 0s - loss: 0.5454 - accuracy: 0.74 - ETA: 0s - loss: 0.5456 - accuracy: 0.74 - ETA: 0s - loss: 0.5548 - accuracy: 0.74 - ETA: 0s - loss: 0.5535 - accuracy: 0.73 - ETA: 0s - loss: 0.5386 - accuracy: 0.74 - ETA: 0s - loss: 0.5296 - accuracy: 0.74 - ETA: 0s - loss: 0.5414 - accuracy: 0.73 - 2s 20ms/step - loss: 0.5464 - accuracy: 0.7354 - val_loss: 0.4537 - val_accuracy: 0.7667\n",
      "Epoch 7/20\n",
      "96/96 [==============================] - ETA: 8s - loss: 0.7612 - accuracy: 0.60 - ETA: 2s - loss: 0.4342 - accuracy: 0.72 - ETA: 1s - loss: 0.4907 - accuracy: 0.73 - ETA: 1s - loss: 0.5518 - accuracy: 0.69 - ETA: 1s - loss: 0.5242 - accuracy: 0.71 - ETA: 1s - loss: 0.4887 - accuracy: 0.73 - ETA: 1s - loss: 0.4878 - accuracy: 0.73 - ETA: 1s - loss: 0.4750 - accuracy: 0.74 - ETA: 0s - loss: 0.4772 - accuracy: 0.74 - ETA: 0s - loss: 0.5012 - accuracy: 0.73 - ETA: 0s - loss: 0.4904 - accuracy: 0.73 - ETA: 0s - loss: 0.4975 - accuracy: 0.72 - ETA: 0s - loss: 0.5107 - accuracy: 0.72 - ETA: 0s - loss: 0.5061 - accuracy: 0.73 - ETA: 0s - loss: 0.4991 - accuracy: 0.73 - ETA: 0s - loss: 0.4959 - accuracy: 0.73 - ETA: 0s - loss: 0.4977 - accuracy: 0.74 - ETA: 0s - loss: 0.4929 - accuracy: 0.74 - ETA: 0s - loss: 0.4899 - accuracy: 0.73 - ETA: 0s - loss: 0.4965 - accuracy: 0.73 - ETA: 0s - loss: 0.5083 - accuracy: 0.73 - ETA: 0s - loss: 0.5098 - accuracy: 0.73 - ETA: 0s - loss: 0.5061 - accuracy: 0.73 - ETA: 0s - loss: 0.5036 - accuracy: 0.73 - 2s 20ms/step - loss: 0.5018 - accuracy: 0.7396 - val_loss: 0.4614 - val_accuracy: 0.7500\n",
      "Epoch 8/20\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "96/96 [==============================] - ETA: 8s - loss: 0.8583 - accuracy: 0.40 - ETA: 2s - loss: 0.5562 - accuracy: 0.68 - ETA: 1s - loss: 0.5438 - accuracy: 0.71 - ETA: 1s - loss: 0.6422 - accuracy: 0.67 - ETA: 1s - loss: 0.6092 - accuracy: 0.68 - ETA: 1s - loss: 0.5852 - accuracy: 0.69 - ETA: 1s - loss: 0.6305 - accuracy: 0.69 - ETA: 1s - loss: 0.6310 - accuracy: 0.69 - ETA: 0s - loss: 0.6256 - accuracy: 0.69 - ETA: 0s - loss: 0.6080 - accuracy: 0.69 - ETA: 0s - loss: 0.6281 - accuracy: 0.68 - ETA: 0s - loss: 0.6242 - accuracy: 0.68 - ETA: 0s - loss: 0.6072 - accuracy: 0.69 - ETA: 0s - loss: 0.5939 - accuracy: 0.71 - ETA: 0s - loss: 0.5973 - accuracy: 0.71 - ETA: 0s - loss: 0.6016 - accuracy: 0.70 - ETA: 0s - loss: 0.6074 - accuracy: 0.70 - ETA: 0s - loss: 0.5917 - accuracy: 0.71 - ETA: 0s - loss: 0.5855 - accuracy: 0.71 - ETA: 0s - loss: 0.5775 - accuracy: 0.71 - ETA: 0s - loss: 0.5718 - accuracy: 0.71 - ETA: 0s - loss: 0.5782 - accuracy: 0.71 - ETA: 0s - loss: 0.5811 - accuracy: 0.72 - ETA: 0s - loss: 0.5824 - accuracy: 0.71 - 2s 20ms/step - loss: 0.5706 - accuracy: 0.7250 - val_loss: 0.4186 - val_accuracy: 0.8000\n",
      "Epoch 9/20\n",
      "96/96 [==============================] - ETA: 7s - loss: 0.8898 - accuracy: 0.40 - ETA: 2s - loss: 0.7219 - accuracy: 0.64 - ETA: 1s - loss: 0.5675 - accuracy: 0.77 - ETA: 1s - loss: 0.5101 - accuracy: 0.78 - ETA: 1s - loss: 0.5243 - accuracy: 0.77 - ETA: 1s - loss: 0.4946 - accuracy: 0.79 - ETA: 1s - loss: 0.5019 - accuracy: 0.76 - ETA: 1s - loss: 0.5020 - accuracy: 0.77 - ETA: 0s - loss: 0.4995 - accuracy: 0.76 - ETA: 0s - loss: 0.4780 - accuracy: 0.78 - ETA: 0s - loss: 0.4983 - accuracy: 0.77 - ETA: 0s - loss: 0.4992 - accuracy: 0.77 - ETA: 0s - loss: 0.4806 - accuracy: 0.77 - ETA: 0s - loss: 0.4832 - accuracy: 0.78 - ETA: 0s - loss: 0.4890 - accuracy: 0.77 - ETA: 0s - loss: 0.4848 - accuracy: 0.78 - ETA: 0s - loss: 0.4837 - accuracy: 0.77 - ETA: 0s - loss: 0.4819 - accuracy: 0.77 - ETA: 0s - loss: 0.4796 - accuracy: 0.77 - ETA: 0s - loss: 0.4723 - accuracy: 0.77 - ETA: 0s - loss: 0.4770 - accuracy: 0.77 - ETA: 0s - loss: 0.4695 - accuracy: 0.78 - ETA: 0s - loss: 0.4612 - accuracy: 0.78 - ETA: 0s - loss: 0.4579 - accuracy: 0.79 - 2s 20ms/step - loss: 0.4573 - accuracy: 0.7917 - val_loss: 0.3949 - val_accuracy: 0.8250\n",
      "Epoch 10/20\n",
      "96/96 [==============================] - ETA: 7s - loss: 0.7140 - accuracy: 0.40 - ETA: 3s - loss: 0.5690 - accuracy: 0.80 - ETA: 2s - loss: 0.4807 - accuracy: 0.80 - ETA: 1s - loss: 0.4672 - accuracy: 0.81 - ETA: 1s - loss: 0.4768 - accuracy: 0.80 - ETA: 1s - loss: 0.4349 - accuracy: 0.82 - ETA: 1s - loss: 0.4486 - accuracy: 0.81 - ETA: 1s - loss: 0.4625 - accuracy: 0.80 - ETA: 1s - loss: 0.4552 - accuracy: 0.80 - ETA: 0s - loss: 0.4522 - accuracy: 0.80 - ETA: 0s - loss: 0.4438 - accuracy: 0.81 - ETA: 0s - loss: 0.4313 - accuracy: 0.82 - ETA: 0s - loss: 0.4191 - accuracy: 0.82 - ETA: 0s - loss: 0.4342 - accuracy: 0.81 - ETA: 0s - loss: 0.4373 - accuracy: 0.81 - ETA: 0s - loss: 0.4350 - accuracy: 0.82 - ETA: 0s - loss: 0.4294 - accuracy: 0.81 - ETA: 0s - loss: 0.4393 - accuracy: 0.80 - ETA: 0s - loss: 0.4336 - accuracy: 0.80 - ETA: 0s - loss: 0.4338 - accuracy: 0.81 - ETA: 0s - loss: 0.4346 - accuracy: 0.81 - ETA: 0s - loss: 0.4295 - accuracy: 0.81 - ETA: 0s - loss: 0.4199 - accuracy: 0.82 - ETA: 0s - loss: 0.4256 - accuracy: 0.81 - 2s 20ms/step - loss: 0.4242 - accuracy: 0.8125 - val_loss: 0.3578 - val_accuracy: 0.8500\n",
      "Epoch 11/20\n",
      "96/96 [==============================] - ETA: 8s - loss: 0.3610 - accuracy: 0.80 - ETA: 2s - loss: 0.4124 - accuracy: 0.72 - ETA: 1s - loss: 0.3790 - accuracy: 0.82 - ETA: 1s - loss: 0.3439 - accuracy: 0.86 - ETA: 1s - loss: 0.3611 - accuracy: 0.83 - ETA: 1s - loss: 0.3940 - accuracy: 0.80 - ETA: 1s - loss: 0.3963 - accuracy: 0.80 - ETA: 1s - loss: 0.4253 - accuracy: 0.78 - ETA: 0s - loss: 0.4332 - accuracy: 0.77 - ETA: 0s - loss: 0.4390 - accuracy: 0.76 - ETA: 0s - loss: 0.4404 - accuracy: 0.77 - ETA: 0s - loss: 0.4419 - accuracy: 0.77 - ETA: 0s - loss: 0.4486 - accuracy: 0.77 - ETA: 0s - loss: 0.4547 - accuracy: 0.76 - ETA: 0s - loss: 0.4481 - accuracy: 0.77 - ETA: 0s - loss: 0.4394 - accuracy: 0.78 - ETA: 0s - loss: 0.4439 - accuracy: 0.78 - ETA: 0s - loss: 0.4497 - accuracy: 0.77 - ETA: 0s - loss: 0.4528 - accuracy: 0.77 - ETA: 0s - loss: 0.4556 - accuracy: 0.77 - ETA: 0s - loss: 0.4534 - accuracy: 0.77 - ETA: 0s - loss: 0.4683 - accuracy: 0.76 - ETA: 0s - loss: 0.4607 - accuracy: 0.76 - ETA: 0s - loss: 0.4532 - accuracy: 0.77 - 2s 20ms/step - loss: 0.4529 - accuracy: 0.7750 - val_loss: 0.3388 - val_accuracy: 0.8500\n",
      "Epoch 12/20\n",
      "96/96 [==============================] - ETA: 8s - loss: 0.2614 - accuracy: 1.00 - ETA: 2s - loss: 0.3765 - accuracy: 0.88 - ETA: 1s - loss: 0.4806 - accuracy: 0.82 - ETA: 1s - loss: 0.4963 - accuracy: 0.77 - ETA: 1s - loss: 0.4776 - accuracy: 0.78 - ETA: 1s - loss: 0.4414 - accuracy: 0.81 - ETA: 1s - loss: 0.4663 - accuracy: 0.80 - ETA: 1s - loss: 0.4521 - accuracy: 0.80 - ETA: 0s - loss: 0.4536 - accuracy: 0.80 - ETA: 0s - loss: 0.4455 - accuracy: 0.81 - ETA: 0s - loss: 0.4411 - accuracy: 0.81 - ETA: 0s - loss: 0.4294 - accuracy: 0.81 - ETA: 0s - loss: 0.4169 - accuracy: 0.82 - ETA: 0s - loss: 0.4004 - accuracy: 0.83 - ETA: 0s - loss: 0.3908 - accuracy: 0.84 - ETA: 0s - loss: 0.4019 - accuracy: 0.83 - ETA: 0s - loss: 0.4039 - accuracy: 0.83 - ETA: 0s - loss: 0.3931 - accuracy: 0.83 - ETA: 0s - loss: 0.3971 - accuracy: 0.83 - ETA: 0s - loss: 0.4059 - accuracy: 0.82 - ETA: 0s - loss: 0.4063 - accuracy: 0.82 - ETA: 0s - loss: 0.4068 - accuracy: 0.81 - ETA: 0s - loss: 0.4010 - accuracy: 0.82 - ETA: 0s - loss: 0.4000 - accuracy: 0.82 - 2s 20ms/step - loss: 0.3947 - accuracy: 0.8250 - val_loss: 0.3156 - val_accuracy: 0.8083\n",
      "Epoch 13/20\n",
      "96/96 [==============================] - ETA: 8s - loss: 0.2092 - accuracy: 1.00 - ETA: 2s - loss: 0.2141 - accuracy: 0.96 - ETA: 1s - loss: 0.2565 - accuracy: 0.91 - ETA: 1s - loss: 0.2515 - accuracy: 0.92 - ETA: 1s - loss: 0.3101 - accuracy: 0.88 - ETA: 1s - loss: 0.3041 - accuracy: 0.89 - ETA: 1s - loss: 0.2837 - accuracy: 0.90 - ETA: 1s - loss: 0.2975 - accuracy: 0.88 - ETA: 0s - loss: 0.3030 - accuracy: 0.87 - ETA: 0s - loss: 0.3017 - accuracy: 0.86 - ETA: 0s - loss: 0.2940 - accuracy: 0.87 - ETA: 0s - loss: 0.2911 - accuracy: 0.87 - ETA: 0s - loss: 0.2969 - accuracy: 0.87 - ETA: 0s - loss: 0.3016 - accuracy: 0.87 - ETA: 0s - loss: 0.2965 - accuracy: 0.87 - ETA: 0s - loss: 0.3009 - accuracy: 0.87 - ETA: 0s - loss: 0.3071 - accuracy: 0.87 - ETA: 0s - loss: 0.3039 - accuracy: 0.87 - ETA: 0s - loss: 0.3073 - accuracy: 0.87 - ETA: 0s - loss: 0.3000 - accuracy: 0.87 - ETA: 0s - loss: 0.2968 - accuracy: 0.88 - ETA: 0s - loss: 0.2943 - accuracy: 0.88 - ETA: 0s - loss: 0.2934 - accuracy: 0.88 - ETA: 0s - loss: 0.2963 - accuracy: 0.88 - 2s 20ms/step - loss: 0.2968 - accuracy: 0.8854 - val_loss: 0.2917 - val_accuracy: 0.8667\n",
      "Epoch 14/20\n",
      "96/96 [==============================] - ETA: 9s - loss: 0.2902 - accuracy: 0.80 - ETA: 2s - loss: 0.3408 - accuracy: 0.84 - ETA: 1s - loss: 0.4756 - accuracy: 0.77 - ETA: 1s - loss: 0.4387 - accuracy: 0.78 - ETA: 1s - loss: 0.4244 - accuracy: 0.81 - ETA: 1s - loss: 0.4076 - accuracy: 0.80 - ETA: 1s - loss: 0.3783 - accuracy: 0.82 - ETA: 1s - loss: 0.3790 - accuracy: 0.82 - ETA: 0s - loss: 0.3607 - accuracy: 0.83 - ETA: 0s - loss: 0.3624 - accuracy: 0.83 - ETA: 0s - loss: 0.3508 - accuracy: 0.83 - ETA: 0s - loss: 0.3524 - accuracy: 0.83 - ETA: 0s - loss: 0.3435 - accuracy: 0.84 - ETA: 0s - loss: 0.3293 - accuracy: 0.84 - ETA: 0s - loss: 0.3397 - accuracy: 0.84 - ETA: 0s - loss: 0.3252 - accuracy: 0.85 - ETA: 0s - loss: 0.3401 - accuracy: 0.84 - ETA: 0s - loss: 0.3391 - accuracy: 0.84 - ETA: 0s - loss: 0.3366 - accuracy: 0.84 - ETA: 0s - loss: 0.3456 - accuracy: 0.84 - ETA: 0s - loss: 0.3622 - accuracy: 0.84 - ETA: 0s - loss: 0.3643 - accuracy: 0.84 - ETA: 0s - loss: 0.3596 - accuracy: 0.84 - ETA: 0s - loss: 0.3541 - accuracy: 0.84 - 2s 20ms/step - loss: 0.3496 - accuracy: 0.8521 - val_loss: 0.2768 - val_accuracy: 0.8667\n",
      "Epoch 15/20\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "96/96 [==============================] - ETA: 9s - loss: 0.1253 - accuracy: 1.00 - ETA: 2s - loss: 0.3138 - accuracy: 0.92 - ETA: 1s - loss: 0.3268 - accuracy: 0.86 - ETA: 1s - loss: 0.2709 - accuracy: 0.89 - ETA: 1s - loss: 0.2598 - accuracy: 0.89 - ETA: 1s - loss: 0.2460 - accuracy: 0.91 - ETA: 1s - loss: 0.2497 - accuracy: 0.91 - ETA: 1s - loss: 0.2616 - accuracy: 0.90 - ETA: 0s - loss: 0.2615 - accuracy: 0.90 - ETA: 0s - loss: 0.2781 - accuracy: 0.90 - ETA: 0s - loss: 0.2818 - accuracy: 0.89 - ETA: 0s - loss: 0.2786 - accuracy: 0.90 - ETA: 0s - loss: 0.2742 - accuracy: 0.90 - ETA: 0s - loss: 0.2750 - accuracy: 0.89 - ETA: 0s - loss: 0.2740 - accuracy: 0.89 - ETA: 0s - loss: 0.2795 - accuracy: 0.88 - ETA: 0s - loss: 0.2722 - accuracy: 0.89 - ETA: 0s - loss: 0.2744 - accuracy: 0.88 - ETA: 0s - loss: 0.2860 - accuracy: 0.87 - ETA: 0s - loss: 0.2913 - accuracy: 0.87 - ETA: 0s - loss: 0.2913 - accuracy: 0.87 - ETA: 0s - loss: 0.2868 - accuracy: 0.87 - ETA: 0s - loss: 0.2846 - accuracy: 0.87 - ETA: 0s - loss: 0.2868 - accuracy: 0.87 - 2s 20ms/step - loss: 0.2885 - accuracy: 0.8687 - val_loss: 0.2426 - val_accuracy: 0.8667\n",
      "Epoch 16/20\n",
      "96/96 [==============================] - ETA: 8s - loss: 0.2609 - accuracy: 1.00 - ETA: 2s - loss: 0.3329 - accuracy: 0.80 - ETA: 1s - loss: 0.3026 - accuracy: 0.84 - ETA: 1s - loss: 0.3209 - accuracy: 0.86 - ETA: 1s - loss: 0.2894 - accuracy: 0.87 - ETA: 1s - loss: 0.3085 - accuracy: 0.85 - ETA: 1s - loss: 0.2891 - accuracy: 0.88 - ETA: 1s - loss: 0.2608 - accuracy: 0.89 - ETA: 0s - loss: 0.2812 - accuracy: 0.88 - ETA: 0s - loss: 0.2690 - accuracy: 0.89 - ETA: 0s - loss: 0.2777 - accuracy: 0.89 - ETA: 0s - loss: 0.2818 - accuracy: 0.88 - ETA: 0s - loss: 0.2714 - accuracy: 0.89 - ETA: 0s - loss: 0.2699 - accuracy: 0.89 - ETA: 0s - loss: 0.2656 - accuracy: 0.90 - ETA: 0s - loss: 0.2764 - accuracy: 0.89 - ETA: 0s - loss: 0.2787 - accuracy: 0.89 - ETA: 0s - loss: 0.2770 - accuracy: 0.89 - ETA: 0s - loss: 0.2839 - accuracy: 0.88 - ETA: 0s - loss: 0.2933 - accuracy: 0.87 - ETA: 0s - loss: 0.2881 - accuracy: 0.88 - ETA: 0s - loss: 0.2820 - accuracy: 0.88 - ETA: 0s - loss: 0.2811 - accuracy: 0.88 - ETA: 0s - loss: 0.2846 - accuracy: 0.87 - 2s 20ms/step - loss: 0.2856 - accuracy: 0.8771 - val_loss: 0.2069 - val_accuracy: 0.9000\n",
      "Epoch 17/20\n",
      "96/96 [==============================] - ETA: 8s - loss: 0.2066 - accuracy: 1.00 - ETA: 2s - loss: 0.1800 - accuracy: 0.96 - ETA: 1s - loss: 0.2543 - accuracy: 0.91 - ETA: 1s - loss: 0.3154 - accuracy: 0.89 - ETA: 1s - loss: 0.3122 - accuracy: 0.89 - ETA: 1s - loss: 0.2895 - accuracy: 0.89 - ETA: 1s - loss: 0.2923 - accuracy: 0.88 - ETA: 1s - loss: 0.2644 - accuracy: 0.90 - ETA: 0s - loss: 0.2617 - accuracy: 0.90 - ETA: 0s - loss: 0.2788 - accuracy: 0.88 - ETA: 0s - loss: 0.2620 - accuracy: 0.89 - ETA: 0s - loss: 0.2589 - accuracy: 0.89 - ETA: 0s - loss: 0.2456 - accuracy: 0.90 - ETA: 0s - loss: 0.2398 - accuracy: 0.90 - ETA: 0s - loss: 0.2342 - accuracy: 0.90 - ETA: 0s - loss: 0.2417 - accuracy: 0.90 - ETA: 0s - loss: 0.2346 - accuracy: 0.90 - ETA: 0s - loss: 0.2349 - accuracy: 0.89 - ETA: 0s - loss: 0.2302 - accuracy: 0.90 - ETA: 0s - loss: 0.2260 - accuracy: 0.90 - ETA: 0s - loss: 0.2219 - accuracy: 0.90 - ETA: 0s - loss: 0.2238 - accuracy: 0.90 - ETA: 0s - loss: 0.2256 - accuracy: 0.90 - ETA: 0s - loss: 0.2176 - accuracy: 0.90 - 2s 20ms/step - loss: 0.2193 - accuracy: 0.9062 - val_loss: 0.1870 - val_accuracy: 0.9417\n",
      "Epoch 18/20\n",
      "96/96 [==============================] - ETA: 8s - loss: 0.0093 - accuracy: 1.00 - ETA: 2s - loss: 0.1691 - accuracy: 0.92 - ETA: 1s - loss: 0.3006 - accuracy: 0.88 - ETA: 1s - loss: 0.3524 - accuracy: 0.86 - ETA: 1s - loss: 0.3076 - accuracy: 0.88 - ETA: 1s - loss: 0.2800 - accuracy: 0.88 - ETA: 1s - loss: 0.2568 - accuracy: 0.88 - ETA: 1s - loss: 0.2399 - accuracy: 0.89 - ETA: 0s - loss: 0.2237 - accuracy: 0.90 - ETA: 0s - loss: 0.2258 - accuracy: 0.90 - ETA: 0s - loss: 0.2281 - accuracy: 0.90 - ETA: 0s - loss: 0.2176 - accuracy: 0.90 - ETA: 0s - loss: 0.2080 - accuracy: 0.91 - ETA: 0s - loss: 0.2176 - accuracy: 0.90 - ETA: 0s - loss: 0.2110 - accuracy: 0.90 - ETA: 0s - loss: 0.2374 - accuracy: 0.89 - ETA: 0s - loss: 0.2288 - accuracy: 0.90 - ETA: 0s - loss: 0.2254 - accuracy: 0.90 - ETA: 0s - loss: 0.2304 - accuracy: 0.90 - ETA: 0s - loss: 0.2401 - accuracy: 0.89 - ETA: 0s - loss: 0.2397 - accuracy: 0.89 - ETA: 0s - loss: 0.2338 - accuracy: 0.89 - ETA: 0s - loss: 0.2419 - accuracy: 0.89 - ETA: 0s - loss: 0.2359 - accuracy: 0.89 - 2s 20ms/step - loss: 0.2347 - accuracy: 0.9000 - val_loss: 0.2212 - val_accuracy: 0.9167\n",
      "Epoch 19/20\n",
      "96/96 [==============================] - ETA: 8s - loss: 0.1129 - accuracy: 1.00 - ETA: 2s - loss: 0.2776 - accuracy: 0.84 - ETA: 1s - loss: 0.2312 - accuracy: 0.88 - ETA: 1s - loss: 0.2029 - accuracy: 0.91 - ETA: 1s - loss: 0.2546 - accuracy: 0.88 - ETA: 1s - loss: 0.2558 - accuracy: 0.88 - ETA: 1s - loss: 0.2647 - accuracy: 0.86 - ETA: 1s - loss: 0.2497 - accuracy: 0.88 - ETA: 0s - loss: 0.2468 - accuracy: 0.88 - ETA: 0s - loss: 0.2343 - accuracy: 0.89 - ETA: 0s - loss: 0.2402 - accuracy: 0.89 - ETA: 0s - loss: 0.2434 - accuracy: 0.88 - ETA: 0s - loss: 0.2434 - accuracy: 0.88 - ETA: 0s - loss: 0.2448 - accuracy: 0.88 - ETA: 0s - loss: 0.2394 - accuracy: 0.88 - ETA: 0s - loss: 0.2409 - accuracy: 0.88 - ETA: 0s - loss: 0.2433 - accuracy: 0.89 - ETA: 0s - loss: 0.2383 - accuracy: 0.89 - ETA: 0s - loss: 0.2378 - accuracy: 0.89 - ETA: 0s - loss: 0.2379 - accuracy: 0.88 - ETA: 0s - loss: 0.2306 - accuracy: 0.89 - ETA: 0s - loss: 0.2333 - accuracy: 0.89 - ETA: 0s - loss: 0.2288 - accuracy: 0.90 - ETA: 0s - loss: 0.2210 - accuracy: 0.90 - 2s 20ms/step - loss: 0.2217 - accuracy: 0.9042 - val_loss: 0.1641 - val_accuracy: 0.9333\n",
      "Epoch 20/20\n",
      "96/96 [==============================] - ETA: 8s - loss: 0.0558 - accuracy: 1.00 - ETA: 2s - loss: 0.1257 - accuracy: 0.96 - ETA: 1s - loss: 0.1759 - accuracy: 0.93 - ETA: 1s - loss: 0.1393 - accuracy: 0.95 - ETA: 1s - loss: 0.1829 - accuracy: 0.94 - ETA: 1s - loss: 0.1554 - accuracy: 0.95 - ETA: 1s - loss: 0.1587 - accuracy: 0.94 - ETA: 1s - loss: 0.1537 - accuracy: 0.94 - ETA: 0s - loss: 0.1665 - accuracy: 0.93 - ETA: 0s - loss: 0.1665 - accuracy: 0.93 - ETA: 0s - loss: 0.1651 - accuracy: 0.94 - ETA: 0s - loss: 0.1585 - accuracy: 0.94 - ETA: 0s - loss: 0.1578 - accuracy: 0.94 - ETA: 0s - loss: 0.1598 - accuracy: 0.93 - ETA: 0s - loss: 0.1665 - accuracy: 0.93 - ETA: 0s - loss: 0.1833 - accuracy: 0.92 - ETA: 0s - loss: 0.1931 - accuracy: 0.91 - ETA: 0s - loss: 0.1920 - accuracy: 0.91 - ETA: 0s - loss: 0.1868 - accuracy: 0.92 - ETA: 0s - loss: 0.1878 - accuracy: 0.92 - ETA: 0s - loss: 0.2033 - accuracy: 0.91 - ETA: 0s - loss: 0.1976 - accuracy: 0.91 - ETA: 0s - loss: 0.2016 - accuracy: 0.91 - ETA: 0s - loss: 0.2037 - accuracy: 0.91 - 2s 21ms/step - loss: 0.1995 - accuracy: 0.9167 - val_loss: 0.1984 - val_accuracy: 0.9000\n"
     ]
    }
   ],
   "source": [
    "N_EPOCHS = 20\n",
    "history = model.fit(x=ds_train,  \n",
    "                    epochs=N_EPOCHS, \n",
    "                    validation_data=ds_valid,\n",
    "                    steps_per_epoch=n_train // BATCH_SIZE,\n",
    "                    validation_steps=(len(Y)-n_train) // BATCH_SIZE,\n",
    "                    callbacks=[tensorboard_callback],\n",
    "                    verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "if False and not IN_COLAB:\n",
    "    tb_dir = 'logs/' + str(log_dir.stem)\n",
    "    print(tb_dir)\n",
    "    %tensorboard --logdir={tb_dir}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.plot(history.history['accuracy'])\n",
    "plt.plot(history.history['val_accuracy'])\n",
    "plt.title('model accuracy')\n",
    "plt.ylabel('accuracy')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'validation'], loc='lower right')\n",
    "plt.show()\n",
    "\n",
    "plt.plot(history.history['loss'])\n",
    "plt.plot(history.history['val_loss'])\n",
    "plt.title('model loss')\n",
    "plt.ylabel('loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'validation'], loc='upper right')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "93% isn't much better than what we got with logistic regression,\n",
    "but let's see what we can learn from the model.\n",
    "\n",
    "TODO\n",
    "* Inspect spatial filter\n",
    "* Inspect maximum activations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transfer Learning\n",
    "* Freeze layers, update only first layer, then unfreeze and update"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_2 (InputLayer)         [(None, 801, 31)]         0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 801, 64)           2048      \n",
      "_________________________________________________________________\n",
      "batch_normalization_v2 (Batc (None, 801, 64)           256       \n",
      "_________________________________________________________________\n",
      "conv1d (Conv1D)              (None, 790, 64)           49216     \n",
      "_________________________________________________________________\n",
      "max_pooling1d (MaxPooling1D) (None, 263, 64)           0         \n",
      "_________________________________________________________________\n",
      "dropout (Dropout)            (None, 263, 64)           0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_v2_1 (Ba (None, 263, 64)           256       \n",
      "_________________________________________________________________\n",
      "conv1d_1 (Conv1D)            (None, 252, 64)           49216     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_1 (MaxPooling1 (None, 84, 64)            0         \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 84, 64)            0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_v2_2 (Ba (None, 84, 64)            256       \n",
      "_________________________________________________________________\n",
      "conv1d_2 (Conv1D)            (None, 77, 32)            16416     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_2 (MaxPooling1 (None, 25, 32)            0         \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 25, 32)            0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_v2_3 (Ba (None, 25, 32)            128       \n",
      "_________________________________________________________________\n",
      "conv1d_3 (Conv1D)            (None, 23, 16)            1552      \n",
      "_________________________________________________________________\n",
      "max_pooling1d_3 (MaxPooling1 (None, 7, 16)             0         \n",
      "_________________________________________________________________\n",
      "dropout_3 (Dropout)          (None, 7, 16)             0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_v2_4 (Ba (None, 7, 16)             64        \n",
      "_________________________________________________________________\n",
      "conv1d_4 (Conv1D)            (None, 5, 8)              392       \n",
      "_________________________________________________________________\n",
      "max_pooling1d_4 (MaxPooling1 (None, 1, 8)              0         \n",
      "_________________________________________________________________\n",
      "dropout_4 (Dropout)          (None, 1, 8)              0         \n",
      "_________________________________________________________________\n",
      "flatten (Flatten)            (None, 8)                 0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 3)                 27        \n",
      "=================================================================\n",
      "Total params: 119,827\n",
      "Trainable params: 119,347\n",
      "Non-trainable params: 480\n",
      "_________________________________________________________________\n",
      "Epoch 1/20\n",
      "96/96 [==============================] - ETA: 5:28 - loss: 2.0972 - accuracy: 0.40 - ETA: 1:04 - loss: 2.1898 - accuracy: 0.12 - ETA: 34s - loss: 2.1196 - accuracy: 0.2222 - ETA: 23s - loss: 2.0483 - accuracy: 0.276 - ETA: 17s - loss: 1.9950 - accuracy: 0.258 - ETA: 13s - loss: 1.8753 - accuracy: 0.295 - ETA: 10s - loss: 1.7781 - accuracy: 0.304 - ETA: 8s - loss: 1.6303 - accuracy: 0.351 - ETA: 7s - loss: 1.5859 - accuracy: 0.35 - ETA: 6s - loss: 1.5341 - accuracy: 0.36 - ETA: 5s - loss: 1.4961 - accuracy: 0.37 - ETA: 4s - loss: 1.4287 - accuracy: 0.40 - ETA: 3s - loss: 1.3715 - accuracy: 0.42 - ETA: 3s - loss: 1.3452 - accuracy: 0.43 - ETA: 2s - loss: 1.3124 - accuracy: 0.44 - ETA: 2s - loss: 1.2990 - accuracy: 0.44 - ETA: 1s - loss: 1.2548 - accuracy: 0.46 - ETA: 1s - loss: 1.2340 - accuracy: 0.46 - ETA: 1s - loss: 1.2154 - accuracy: 0.47 - ETA: 0s - loss: 1.1821 - accuracy: 0.48 - ETA: 0s - loss: 1.1599 - accuracy: 0.49 - ETA: 0s - loss: 1.1396 - accuracy: 0.49 - ETA: 0s - loss: 1.1224 - accuracy: 0.50 - 5s 54ms/step - loss: 1.1117 - accuracy: 0.5083 - val_loss: 0.5795 - val_accuracy: 0.7083\n",
      "Epoch 2/20\n",
      "96/96 [==============================] - ETA: 4s - loss: 0.3058 - accuracy: 0.80 - ETA: 1s - loss: 0.8408 - accuracy: 0.60 - ETA: 1s - loss: 0.6527 - accuracy: 0.71 - ETA: 1s - loss: 0.6261 - accuracy: 0.76 - ETA: 1s - loss: 0.6662 - accuracy: 0.72 - ETA: 1s - loss: 0.6834 - accuracy: 0.71 - ETA: 1s - loss: 0.6117 - accuracy: 0.75 - ETA: 0s - loss: 0.6345 - accuracy: 0.71 - ETA: 0s - loss: 0.6224 - accuracy: 0.70 - ETA: 0s - loss: 0.6336 - accuracy: 0.71 - ETA: 0s - loss: 0.6497 - accuracy: 0.70 - ETA: 0s - loss: 0.6496 - accuracy: 0.69 - ETA: 0s - loss: 0.6560 - accuracy: 0.70 - ETA: 0s - loss: 0.6532 - accuracy: 0.69 - ETA: 0s - loss: 0.6451 - accuracy: 0.69 - ETA: 0s - loss: 0.6366 - accuracy: 0.69 - ETA: 0s - loss: 0.6219 - accuracy: 0.70 - ETA: 0s - loss: 0.6140 - accuracy: 0.70 - ETA: 0s - loss: 0.6331 - accuracy: 0.69 - ETA: 0s - loss: 0.6304 - accuracy: 0.70 - ETA: 0s - loss: 0.6402 - accuracy: 0.70 - ETA: 0s - loss: 0.6395 - accuracy: 0.69 - ETA: 0s - loss: 0.6515 - accuracy: 0.69 - ETA: 0s - loss: 0.6551 - accuracy: 0.69 - 1s 15ms/step - loss: 0.6541 - accuracy: 0.6958 - val_loss: 0.4404 - val_accuracy: 0.7750\n",
      "Epoch 3/20\n",
      "96/96 [==============================] - ETA: 3s - loss: 0.3792 - accuracy: 0.80 - ETA: 1s - loss: 0.5930 - accuracy: 0.72 - ETA: 1s - loss: 0.5573 - accuracy: 0.68 - ETA: 1s - loss: 0.4876 - accuracy: 0.73 - ETA: 1s - loss: 0.5040 - accuracy: 0.74 - ETA: 1s - loss: 0.5224 - accuracy: 0.72 - ETA: 0s - loss: 0.4885 - accuracy: 0.75 - ETA: 0s - loss: 0.4880 - accuracy: 0.76 - ETA: 0s - loss: 0.4940 - accuracy: 0.76 - ETA: 0s - loss: 0.4994 - accuracy: 0.75 - ETA: 0s - loss: 0.4878 - accuracy: 0.76 - ETA: 0s - loss: 0.4820 - accuracy: 0.76 - ETA: 0s - loss: 0.4993 - accuracy: 0.75 - ETA: 0s - loss: 0.4917 - accuracy: 0.76 - ETA: 0s - loss: 0.4919 - accuracy: 0.75 - ETA: 0s - loss: 0.4873 - accuracy: 0.76 - ETA: 0s - loss: 0.4828 - accuracy: 0.76 - ETA: 0s - loss: 0.4867 - accuracy: 0.76 - ETA: 0s - loss: 0.4963 - accuracy: 0.74 - ETA: 0s - loss: 0.4921 - accuracy: 0.75 - ETA: 0s - loss: 0.4904 - accuracy: 0.75 - ETA: 0s - loss: 0.5028 - accuracy: 0.74 - ETA: 0s - loss: 0.4978 - accuracy: 0.74 - ETA: 0s - loss: 0.4922 - accuracy: 0.74 - 1s 15ms/step - loss: 0.4941 - accuracy: 0.7521 - val_loss: 0.4099 - val_accuracy: 0.7833\n",
      "Epoch 4/20\n",
      "96/96 [==============================] - ETA: 4s - loss: 0.4595 - accuracy: 0.80 - ETA: 1s - loss: 0.4518 - accuracy: 0.76 - ETA: 1s - loss: 0.4966 - accuracy: 0.75 - ETA: 1s - loss: 0.4763 - accuracy: 0.76 - ETA: 1s - loss: 0.5094 - accuracy: 0.76 - ETA: 1s - loss: 0.5298 - accuracy: 0.75 - ETA: 1s - loss: 0.4942 - accuracy: 0.76 - ETA: 0s - loss: 0.5019 - accuracy: 0.75 - ETA: 0s - loss: 0.5188 - accuracy: 0.74 - ETA: 0s - loss: 0.5110 - accuracy: 0.75 - ETA: 0s - loss: 0.5174 - accuracy: 0.74 - ETA: 0s - loss: 0.5152 - accuracy: 0.75 - ETA: 0s - loss: 0.5116 - accuracy: 0.75 - ETA: 0s - loss: 0.5191 - accuracy: 0.76 - ETA: 0s - loss: 0.5111 - accuracy: 0.76 - ETA: 0s - loss: 0.5055 - accuracy: 0.76 - ETA: 0s - loss: 0.5000 - accuracy: 0.77 - ETA: 0s - loss: 0.4980 - accuracy: 0.76 - ETA: 0s - loss: 0.5067 - accuracy: 0.75 - ETA: 0s - loss: 0.5061 - accuracy: 0.76 - ETA: 0s - loss: 0.5005 - accuracy: 0.77 - ETA: 0s - loss: 0.5112 - accuracy: 0.76 - ETA: 0s - loss: 0.5023 - accuracy: 0.76 - 1s 15ms/step - loss: 0.5009 - accuracy: 0.7646 - val_loss: 0.3897 - val_accuracy: 0.8250\n",
      "Epoch 5/20\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "96/96 [==============================] - ETA: 4s - loss: 0.7692 - accuracy: 0.40 - ETA: 1s - loss: 0.3842 - accuracy: 0.88 - ETA: 1s - loss: 0.3997 - accuracy: 0.88 - ETA: 1s - loss: 0.4582 - accuracy: 0.81 - ETA: 1s - loss: 0.4489 - accuracy: 0.78 - ETA: 1s - loss: 0.4261 - accuracy: 0.80 - ETA: 0s - loss: 0.4413 - accuracy: 0.80 - ETA: 0s - loss: 0.4446 - accuracy: 0.81 - ETA: 0s - loss: 0.4453 - accuracy: 0.81 - ETA: 0s - loss: 0.4372 - accuracy: 0.81 - ETA: 0s - loss: 0.4290 - accuracy: 0.81 - ETA: 0s - loss: 0.4189 - accuracy: 0.81 - ETA: 0s - loss: 0.4203 - accuracy: 0.81 - ETA: 0s - loss: 0.4100 - accuracy: 0.81 - ETA: 0s - loss: 0.4227 - accuracy: 0.81 - ETA: 0s - loss: 0.4212 - accuracy: 0.80 - ETA: 0s - loss: 0.4209 - accuracy: 0.80 - ETA: 0s - loss: 0.4107 - accuracy: 0.81 - ETA: 0s - loss: 0.4133 - accuracy: 0.81 - ETA: 0s - loss: 0.4294 - accuracy: 0.80 - ETA: 0s - loss: 0.4418 - accuracy: 0.79 - ETA: 0s - loss: 0.4395 - accuracy: 0.79 - ETA: 0s - loss: 0.4360 - accuracy: 0.79 - 1s 15ms/step - loss: 0.4288 - accuracy: 0.8021 - val_loss: 0.3966 - val_accuracy: 0.7917\n",
      "Epoch 6/20\n",
      "96/96 [==============================] - ETA: 4s - loss: 0.2596 - accuracy: 0.80 - ETA: 1s - loss: 0.2226 - accuracy: 0.96 - ETA: 1s - loss: 0.2870 - accuracy: 0.91 - ETA: 1s - loss: 0.2973 - accuracy: 0.90 - ETA: 1s - loss: 0.3617 - accuracy: 0.84 - ETA: 1s - loss: 0.3679 - accuracy: 0.85 - ETA: 1s - loss: 0.3564 - accuracy: 0.86 - ETA: 0s - loss: 0.3572 - accuracy: 0.86 - ETA: 0s - loss: 0.3470 - accuracy: 0.86 - ETA: 0s - loss: 0.3788 - accuracy: 0.84 - ETA: 0s - loss: 0.3805 - accuracy: 0.84 - ETA: 0s - loss: 0.3779 - accuracy: 0.84 - ETA: 0s - loss: 0.3683 - accuracy: 0.84 - ETA: 0s - loss: 0.3730 - accuracy: 0.84 - ETA: 0s - loss: 0.3804 - accuracy: 0.84 - ETA: 0s - loss: 0.3840 - accuracy: 0.83 - ETA: 0s - loss: 0.3834 - accuracy: 0.84 - ETA: 0s - loss: 0.3755 - accuracy: 0.84 - ETA: 0s - loss: 0.3725 - accuracy: 0.84 - ETA: 0s - loss: 0.3728 - accuracy: 0.83 - ETA: 0s - loss: 0.3757 - accuracy: 0.83 - ETA: 0s - loss: 0.3704 - accuracy: 0.84 - ETA: 0s - loss: 0.3792 - accuracy: 0.83 - ETA: 0s - loss: 0.3716 - accuracy: 0.83 - 1s 15ms/step - loss: 0.3711 - accuracy: 0.8375 - val_loss: 0.3348 - val_accuracy: 0.8250\n",
      "Epoch 7/20\n",
      "96/96 [==============================] - ETA: 3s - loss: 0.4547 - accuracy: 1.00 - ETA: 1s - loss: 0.2669 - accuracy: 0.92 - ETA: 1s - loss: 0.3010 - accuracy: 0.91 - ETA: 1s - loss: 0.3499 - accuracy: 0.87 - ETA: 1s - loss: 0.3456 - accuracy: 0.87 - ETA: 1s - loss: 0.3287 - accuracy: 0.87 - ETA: 1s - loss: 0.3536 - accuracy: 0.86 - ETA: 0s - loss: 0.3593 - accuracy: 0.85 - ETA: 0s - loss: 0.3455 - accuracy: 0.86 - ETA: 0s - loss: 0.3404 - accuracy: 0.85 - ETA: 0s - loss: 0.3455 - accuracy: 0.85 - ETA: 0s - loss: 0.3430 - accuracy: 0.85 - ETA: 0s - loss: 0.3395 - accuracy: 0.84 - ETA: 0s - loss: 0.3292 - accuracy: 0.85 - ETA: 0s - loss: 0.3160 - accuracy: 0.86 - ETA: 0s - loss: 0.3122 - accuracy: 0.86 - ETA: 0s - loss: 0.3174 - accuracy: 0.86 - ETA: 0s - loss: 0.3256 - accuracy: 0.85 - ETA: 0s - loss: 0.3229 - accuracy: 0.85 - ETA: 0s - loss: 0.3339 - accuracy: 0.85 - ETA: 0s - loss: 0.3433 - accuracy: 0.84 - ETA: 0s - loss: 0.3411 - accuracy: 0.85 - ETA: 0s - loss: 0.3383 - accuracy: 0.85 - ETA: 0s - loss: 0.3329 - accuracy: 0.85 - 1s 15ms/step - loss: 0.3324 - accuracy: 0.8562 - val_loss: 0.3022 - val_accuracy: 0.8417\n",
      "Epoch 8/20\n",
      "96/96 [==============================] - ETA: 3s - loss: 0.1249 - accuracy: 1.00 - ETA: 1s - loss: 0.2252 - accuracy: 0.88 - ETA: 1s - loss: 0.3885 - accuracy: 0.77 - ETA: 1s - loss: 0.3649 - accuracy: 0.75 - ETA: 1s - loss: 0.3990 - accuracy: 0.76 - ETA: 1s - loss: 0.3469 - accuracy: 0.80 - ETA: 1s - loss: 0.3378 - accuracy: 0.81 - ETA: 0s - loss: 0.3244 - accuracy: 0.83 - ETA: 0s - loss: 0.3069 - accuracy: 0.85 - ETA: 0s - loss: 0.3140 - accuracy: 0.84 - ETA: 0s - loss: 0.3172 - accuracy: 0.85 - ETA: 0s - loss: 0.3016 - accuracy: 0.86 - ETA: 0s - loss: 0.3055 - accuracy: 0.85 - ETA: 0s - loss: 0.3031 - accuracy: 0.86 - ETA: 0s - loss: 0.3046 - accuracy: 0.86 - ETA: 0s - loss: 0.3196 - accuracy: 0.85 - ETA: 0s - loss: 0.3190 - accuracy: 0.85 - ETA: 0s - loss: 0.3351 - accuracy: 0.85 - ETA: 0s - loss: 0.3344 - accuracy: 0.85 - ETA: 0s - loss: 0.3248 - accuracy: 0.85 - ETA: 0s - loss: 0.3191 - accuracy: 0.86 - ETA: 0s - loss: 0.3191 - accuracy: 0.86 - ETA: 0s - loss: 0.3229 - accuracy: 0.85 - ETA: 0s - loss: 0.3345 - accuracy: 0.85 - 1s 15ms/step - loss: 0.3303 - accuracy: 0.8583 - val_loss: 0.2779 - val_accuracy: 0.8750\n",
      "Epoch 9/20\n",
      "96/96 [==============================] - ETA: 3s - loss: 0.1542 - accuracy: 1.00 - ETA: 1s - loss: 0.2257 - accuracy: 0.96 - ETA: 1s - loss: 0.2163 - accuracy: 0.95 - ETA: 1s - loss: 0.2320 - accuracy: 0.93 - ETA: 1s - loss: 0.2690 - accuracy: 0.91 - ETA: 1s - loss: 0.2677 - accuracy: 0.92 - ETA: 1s - loss: 0.2629 - accuracy: 0.90 - ETA: 0s - loss: 0.2894 - accuracy: 0.88 - ETA: 0s - loss: 0.3078 - accuracy: 0.86 - ETA: 0s - loss: 0.3042 - accuracy: 0.85 - ETA: 0s - loss: 0.3096 - accuracy: 0.84 - ETA: 0s - loss: 0.2949 - accuracy: 0.85 - ETA: 0s - loss: 0.2882 - accuracy: 0.86 - ETA: 0s - loss: 0.2842 - accuracy: 0.86 - ETA: 0s - loss: 0.2878 - accuracy: 0.86 - ETA: 0s - loss: 0.2907 - accuracy: 0.86 - ETA: 0s - loss: 0.2917 - accuracy: 0.86 - ETA: 0s - loss: 0.2847 - accuracy: 0.87 - ETA: 0s - loss: 0.2864 - accuracy: 0.87 - ETA: 0s - loss: 0.2898 - accuracy: 0.87 - ETA: 0s - loss: 0.2905 - accuracy: 0.87 - ETA: 0s - loss: 0.2890 - accuracy: 0.87 - ETA: 0s - loss: 0.2904 - accuracy: 0.87 - 1s 15ms/step - loss: 0.2961 - accuracy: 0.8667 - val_loss: 0.2827 - val_accuracy: 0.8667\n",
      "Epoch 10/20\n",
      "96/96 [==============================] - ETA: 3s - loss: 0.4649 - accuracy: 0.60 - ETA: 1s - loss: 0.2090 - accuracy: 0.92 - ETA: 1s - loss: 0.2260 - accuracy: 0.93 - ETA: 1s - loss: 0.2171 - accuracy: 0.92 - ETA: 1s - loss: 0.2224 - accuracy: 0.91 - ETA: 1s - loss: 0.2274 - accuracy: 0.90 - ETA: 1s - loss: 0.2377 - accuracy: 0.90 - ETA: 0s - loss: 0.2113 - accuracy: 0.91 - ETA: 0s - loss: 0.2158 - accuracy: 0.90 - ETA: 0s - loss: 0.2342 - accuracy: 0.90 - ETA: 0s - loss: 0.2224 - accuracy: 0.91 - ETA: 0s - loss: 0.2224 - accuracy: 0.91 - ETA: 0s - loss: 0.2127 - accuracy: 0.91 - ETA: 0s - loss: 0.2147 - accuracy: 0.90 - ETA: 0s - loss: 0.2167 - accuracy: 0.90 - ETA: 0s - loss: 0.2261 - accuracy: 0.90 - ETA: 0s - loss: 0.2246 - accuracy: 0.90 - ETA: 0s - loss: 0.2416 - accuracy: 0.89 - ETA: 0s - loss: 0.2558 - accuracy: 0.88 - ETA: 0s - loss: 0.2611 - accuracy: 0.88 - ETA: 0s - loss: 0.2649 - accuracy: 0.88 - ETA: 0s - loss: 0.2629 - accuracy: 0.88 - ETA: 0s - loss: 0.2558 - accuracy: 0.89 - ETA: 0s - loss: 0.2576 - accuracy: 0.89 - 1s 15ms/step - loss: 0.2545 - accuracy: 0.8938 - val_loss: 0.2888 - val_accuracy: 0.8500\n",
      "Epoch 11/20\n",
      "96/96 [==============================] - ETA: 4s - loss: 0.4093 - accuracy: 0.80 - ETA: 1s - loss: 0.2675 - accuracy: 0.84 - ETA: 1s - loss: 0.2623 - accuracy: 0.86 - ETA: 1s - loss: 0.2187 - accuracy: 0.89 - ETA: 1s - loss: 0.2326 - accuracy: 0.88 - ETA: 1s - loss: 0.2181 - accuracy: 0.89 - ETA: 1s - loss: 0.2486 - accuracy: 0.88 - ETA: 0s - loss: 0.2344 - accuracy: 0.88 - ETA: 0s - loss: 0.2215 - accuracy: 0.89 - ETA: 0s - loss: 0.2272 - accuracy: 0.89 - ETA: 0s - loss: 0.2242 - accuracy: 0.90 - ETA: 0s - loss: 0.2222 - accuracy: 0.90 - ETA: 0s - loss: 0.2264 - accuracy: 0.90 - ETA: 0s - loss: 0.2321 - accuracy: 0.89 - ETA: 0s - loss: 0.2321 - accuracy: 0.89 - ETA: 0s - loss: 0.2452 - accuracy: 0.89 - ETA: 0s - loss: 0.2366 - accuracy: 0.89 - ETA: 0s - loss: 0.2501 - accuracy: 0.89 - ETA: 0s - loss: 0.2429 - accuracy: 0.89 - ETA: 0s - loss: 0.2427 - accuracy: 0.89 - ETA: 0s - loss: 0.2524 - accuracy: 0.89 - ETA: 0s - loss: 0.2551 - accuracy: 0.88 - ETA: 0s - loss: 0.2499 - accuracy: 0.88 - ETA: 0s - loss: 0.2493 - accuracy: 0.89 - 1s 15ms/step - loss: 0.2497 - accuracy: 0.8917 - val_loss: 0.2457 - val_accuracy: 0.8750\n",
      "Epoch 12/20\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "96/96 [==============================] - ETA: 4s - loss: 0.0209 - accuracy: 1.00 - ETA: 1s - loss: 0.0916 - accuracy: 0.96 - ETA: 1s - loss: 0.2244 - accuracy: 0.86 - ETA: 1s - loss: 0.2395 - accuracy: 0.87 - ETA: 1s - loss: 0.2298 - accuracy: 0.88 - ETA: 1s - loss: 0.2334 - accuracy: 0.88 - ETA: 1s - loss: 0.2324 - accuracy: 0.89 - ETA: 0s - loss: 0.2227 - accuracy: 0.90 - ETA: 0s - loss: 0.2293 - accuracy: 0.90 - ETA: 0s - loss: 0.2390 - accuracy: 0.90 - ETA: 0s - loss: 0.2308 - accuracy: 0.90 - ETA: 0s - loss: 0.2386 - accuracy: 0.89 - ETA: 0s - loss: 0.2308 - accuracy: 0.90 - ETA: 0s - loss: 0.2316 - accuracy: 0.89 - ETA: 0s - loss: 0.2219 - accuracy: 0.90 - ETA: 0s - loss: 0.2342 - accuracy: 0.89 - ETA: 0s - loss: 0.2315 - accuracy: 0.89 - ETA: 0s - loss: 0.2357 - accuracy: 0.89 - ETA: 0s - loss: 0.2262 - accuracy: 0.90 - ETA: 0s - loss: 0.2267 - accuracy: 0.90 - ETA: 0s - loss: 0.2256 - accuracy: 0.90 - ETA: 0s - loss: 0.2201 - accuracy: 0.90 - ETA: 0s - loss: 0.2208 - accuracy: 0.90 - 1s 15ms/step - loss: 0.2151 - accuracy: 0.9062 - val_loss: 0.2090 - val_accuracy: 0.8833\n",
      "Epoch 13/20\n",
      "96/96 [==============================] - ETA: 4s - loss: 0.3833 - accuracy: 0.80 - ETA: 1s - loss: 0.3132 - accuracy: 0.80 - ETA: 1s - loss: 0.2598 - accuracy: 0.86 - ETA: 1s - loss: 0.2145 - accuracy: 0.89 - ETA: 1s - loss: 0.2256 - accuracy: 0.89 - ETA: 1s - loss: 0.1903 - accuracy: 0.91 - ETA: 1s - loss: 0.1975 - accuracy: 0.92 - ETA: 0s - loss: 0.1904 - accuracy: 0.91 - ETA: 0s - loss: 0.2173 - accuracy: 0.89 - ETA: 0s - loss: 0.2413 - accuracy: 0.89 - ETA: 0s - loss: 0.2289 - accuracy: 0.89 - ETA: 0s - loss: 0.2218 - accuracy: 0.90 - ETA: 0s - loss: 0.2196 - accuracy: 0.89 - ETA: 0s - loss: 0.2143 - accuracy: 0.90 - ETA: 0s - loss: 0.2155 - accuracy: 0.90 - ETA: 0s - loss: 0.2090 - accuracy: 0.90 - ETA: 0s - loss: 0.2130 - accuracy: 0.91 - ETA: 0s - loss: 0.2210 - accuracy: 0.90 - ETA: 0s - loss: 0.2242 - accuracy: 0.90 - ETA: 0s - loss: 0.2178 - accuracy: 0.90 - ETA: 0s - loss: 0.2252 - accuracy: 0.90 - ETA: 0s - loss: 0.2249 - accuracy: 0.90 - ETA: 0s - loss: 0.2311 - accuracy: 0.90 - ETA: 0s - loss: 0.2277 - accuracy: 0.90 - 1s 15ms/step - loss: 0.2259 - accuracy: 0.9042 - val_loss: 0.2155 - val_accuracy: 0.8917\n",
      "Epoch 14/20\n",
      "96/96 [==============================] - ETA: 4s - loss: 0.3151 - accuracy: 0.80 - ETA: 1s - loss: 0.2560 - accuracy: 0.93 - ETA: 1s - loss: 0.2456 - accuracy: 0.90 - ETA: 1s - loss: 0.2309 - accuracy: 0.90 - ETA: 1s - loss: 0.2045 - accuracy: 0.92 - ETA: 1s - loss: 0.1896 - accuracy: 0.92 - ETA: 0s - loss: 0.1937 - accuracy: 0.92 - ETA: 0s - loss: 0.1830 - accuracy: 0.92 - ETA: 0s - loss: 0.1736 - accuracy: 0.92 - ETA: 0s - loss: 0.1804 - accuracy: 0.92 - ETA: 0s - loss: 0.1790 - accuracy: 0.93 - ETA: 0s - loss: 0.1768 - accuracy: 0.92 - ETA: 0s - loss: 0.1759 - accuracy: 0.93 - ETA: 0s - loss: 0.1892 - accuracy: 0.93 - ETA: 0s - loss: 0.1981 - accuracy: 0.92 - ETA: 0s - loss: 0.2054 - accuracy: 0.91 - ETA: 0s - loss: 0.2073 - accuracy: 0.91 - ETA: 0s - loss: 0.2064 - accuracy: 0.91 - ETA: 0s - loss: 0.2046 - accuracy: 0.91 - ETA: 0s - loss: 0.1983 - accuracy: 0.92 - ETA: 0s - loss: 0.1937 - accuracy: 0.92 - ETA: 0s - loss: 0.1856 - accuracy: 0.92 - ETA: 0s - loss: 0.1842 - accuracy: 0.92 - 1s 15ms/step - loss: 0.1922 - accuracy: 0.9229 - val_loss: 0.2166 - val_accuracy: 0.9083\n",
      "Epoch 15/20\n",
      "96/96 [==============================] - ETA: 3s - loss: 0.3052 - accuracy: 0.80 - ETA: 1s - loss: 0.1329 - accuracy: 0.92 - ETA: 1s - loss: 0.1034 - accuracy: 0.95 - ETA: 1s - loss: 0.1141 - accuracy: 0.95 - ETA: 1s - loss: 0.1071 - accuracy: 0.95 - ETA: 1s - loss: 0.1049 - accuracy: 0.95 - ETA: 1s - loss: 0.1053 - accuracy: 0.95 - ETA: 0s - loss: 0.1144 - accuracy: 0.94 - ETA: 0s - loss: 0.1146 - accuracy: 0.94 - ETA: 0s - loss: 0.1156 - accuracy: 0.95 - ETA: 0s - loss: 0.1200 - accuracy: 0.94 - ETA: 0s - loss: 0.1188 - accuracy: 0.94 - ETA: 0s - loss: 0.1224 - accuracy: 0.94 - ETA: 0s - loss: 0.1266 - accuracy: 0.94 - ETA: 0s - loss: 0.1257 - accuracy: 0.94 - ETA: 0s - loss: 0.1227 - accuracy: 0.94 - ETA: 0s - loss: 0.1437 - accuracy: 0.93 - ETA: 0s - loss: 0.1452 - accuracy: 0.93 - ETA: 0s - loss: 0.1413 - accuracy: 0.94 - ETA: 0s - loss: 0.1421 - accuracy: 0.94 - ETA: 0s - loss: 0.1519 - accuracy: 0.93 - ETA: 0s - loss: 0.1528 - accuracy: 0.93 - ETA: 0s - loss: 0.1520 - accuracy: 0.93 - 1s 15ms/step - loss: 0.1535 - accuracy: 0.9292 - val_loss: 0.1673 - val_accuracy: 0.9083\n",
      "Epoch 16/20\n",
      "96/96 [==============================] - ETA: 3s - loss: 0.5671 - accuracy: 0.80 - ETA: 1s - loss: 0.2414 - accuracy: 0.92 - ETA: 1s - loss: 0.2057 - accuracy: 0.93 - ETA: 1s - loss: 0.1793 - accuracy: 0.93 - ETA: 1s - loss: 0.1751 - accuracy: 0.92 - ETA: 1s - loss: 0.1763 - accuracy: 0.92 - ETA: 1s - loss: 0.1785 - accuracy: 0.92 - ETA: 0s - loss: 0.1629 - accuracy: 0.93 - ETA: 0s - loss: 0.1807 - accuracy: 0.93 - ETA: 0s - loss: 0.1721 - accuracy: 0.93 - ETA: 0s - loss: 0.1603 - accuracy: 0.94 - ETA: 0s - loss: 0.1595 - accuracy: 0.93 - ETA: 0s - loss: 0.1784 - accuracy: 0.93 - ETA: 0s - loss: 0.1852 - accuracy: 0.92 - ETA: 0s - loss: 0.1832 - accuracy: 0.92 - ETA: 0s - loss: 0.1818 - accuracy: 0.92 - ETA: 0s - loss: 0.1832 - accuracy: 0.92 - ETA: 0s - loss: 0.1847 - accuracy: 0.92 - ETA: 0s - loss: 0.1823 - accuracy: 0.92 - ETA: 0s - loss: 0.1834 - accuracy: 0.92 - ETA: 0s - loss: 0.1822 - accuracy: 0.92 - ETA: 0s - loss: 0.1859 - accuracy: 0.92 - ETA: 0s - loss: 0.1792 - accuracy: 0.92 - ETA: 0s - loss: 0.1797 - accuracy: 0.92 - 1s 15ms/step - loss: 0.1785 - accuracy: 0.9292 - val_loss: 0.1754 - val_accuracy: 0.9250\n",
      "Epoch 17/20\n",
      "96/96 [==============================] - ETA: 3s - loss: 0.0959 - accuracy: 1.00 - ETA: 1s - loss: 0.1360 - accuracy: 0.93 - ETA: 1s - loss: 0.0954 - accuracy: 0.96 - ETA: 1s - loss: 0.0989 - accuracy: 0.97 - ETA: 1s - loss: 0.1186 - accuracy: 0.96 - ETA: 1s - loss: 0.1302 - accuracy: 0.96 - ETA: 0s - loss: 0.1223 - accuracy: 0.96 - ETA: 0s - loss: 0.1368 - accuracy: 0.95 - ETA: 0s - loss: 0.1260 - accuracy: 0.95 - ETA: 0s - loss: 0.1298 - accuracy: 0.95 - ETA: 0s - loss: 0.1413 - accuracy: 0.95 - ETA: 0s - loss: 0.1431 - accuracy: 0.94 - ETA: 0s - loss: 0.1589 - accuracy: 0.93 - ETA: 0s - loss: 0.1699 - accuracy: 0.92 - ETA: 0s - loss: 0.1613 - accuracy: 0.93 - ETA: 0s - loss: 0.1638 - accuracy: 0.93 - ETA: 0s - loss: 0.1565 - accuracy: 0.93 - ETA: 0s - loss: 0.1631 - accuracy: 0.93 - ETA: 0s - loss: 0.1579 - accuracy: 0.93 - ETA: 0s - loss: 0.1577 - accuracy: 0.93 - ETA: 0s - loss: 0.1591 - accuracy: 0.93 - ETA: 0s - loss: 0.1552 - accuracy: 0.93 - ETA: 0s - loss: 0.1510 - accuracy: 0.94 - ETA: 0s - loss: 0.1508 - accuracy: 0.94 - 1s 15ms/step - loss: 0.1492 - accuracy: 0.9438 - val_loss: 0.1541 - val_accuracy: 0.9083\n",
      "Epoch 18/20\n",
      "96/96 [==============================] - ETA: 3s - loss: 0.2225 - accuracy: 1.00 - ETA: 1s - loss: 0.1095 - accuracy: 0.96 - ETA: 1s - loss: 0.1378 - accuracy: 0.93 - ETA: 1s - loss: 0.1167 - accuracy: 0.95 - ETA: 1s - loss: 0.1155 - accuracy: 0.95 - ETA: 1s - loss: 0.1305 - accuracy: 0.96 - ETA: 1s - loss: 0.1508 - accuracy: 0.93 - ETA: 0s - loss: 0.1770 - accuracy: 0.92 - ETA: 0s - loss: 0.1718 - accuracy: 0.92 - ETA: 0s - loss: 0.1565 - accuracy: 0.93 - ETA: 0s - loss: 0.1537 - accuracy: 0.93 - ETA: 0s - loss: 0.1506 - accuracy: 0.94 - ETA: 0s - loss: 0.1515 - accuracy: 0.93 - ETA: 0s - loss: 0.1529 - accuracy: 0.93 - ETA: 0s - loss: 0.1511 - accuracy: 0.93 - ETA: 0s - loss: 0.1448 - accuracy: 0.94 - ETA: 0s - loss: 0.1418 - accuracy: 0.94 - ETA: 0s - loss: 0.1370 - accuracy: 0.94 - ETA: 0s - loss: 0.1313 - accuracy: 0.94 - ETA: 0s - loss: 0.1304 - accuracy: 0.94 - ETA: 0s - loss: 0.1385 - accuracy: 0.94 - ETA: 0s - loss: 0.1488 - accuracy: 0.93 - ETA: 0s - loss: 0.1447 - accuracy: 0.94 - ETA: 0s - loss: 0.1488 - accuracy: 0.93 - 1s 15ms/step - loss: 0.1480 - accuracy: 0.9375 - val_loss: 0.1845 - val_accuracy: 0.9167\n",
      "Epoch 19/20\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "96/96 [==============================] - ETA: 3s - loss: 0.0460 - accuracy: 1.00 - ETA: 1s - loss: 0.0739 - accuracy: 1.00 - ETA: 1s - loss: 0.1046 - accuracy: 0.95 - ETA: 1s - loss: 0.1391 - accuracy: 0.93 - ETA: 1s - loss: 0.1278 - accuracy: 0.95 - ETA: 1s - loss: 0.1134 - accuracy: 0.96 - ETA: 1s - loss: 0.1124 - accuracy: 0.96 - ETA: 0s - loss: 0.1364 - accuracy: 0.94 - ETA: 0s - loss: 0.1383 - accuracy: 0.94 - ETA: 0s - loss: 0.1376 - accuracy: 0.94 - ETA: 0s - loss: 0.1301 - accuracy: 0.95 - ETA: 0s - loss: 0.1389 - accuracy: 0.94 - ETA: 0s - loss: 0.1565 - accuracy: 0.94 - ETA: 0s - loss: 0.1504 - accuracy: 0.94 - ETA: 0s - loss: 0.1482 - accuracy: 0.95 - ETA: 0s - loss: 0.1435 - accuracy: 0.95 - ETA: 0s - loss: 0.1487 - accuracy: 0.94 - ETA: 0s - loss: 0.1435 - accuracy: 0.95 - ETA: 0s - loss: 0.1391 - accuracy: 0.95 - ETA: 0s - loss: 0.1428 - accuracy: 0.95 - ETA: 0s - loss: 0.1414 - accuracy: 0.95 - ETA: 0s - loss: 0.1401 - accuracy: 0.95 - ETA: 0s - loss: 0.1526 - accuracy: 0.94 - ETA: 0s - loss: 0.1521 - accuracy: 0.94 - 1s 15ms/step - loss: 0.1539 - accuracy: 0.9417 - val_loss: 0.1630 - val_accuracy: 0.9333\n",
      "Epoch 20/20\n",
      "96/96 [==============================] - ETA: 3s - loss: 0.2862 - accuracy: 0.80 - ETA: 1s - loss: 0.1892 - accuracy: 0.88 - ETA: 1s - loss: 0.1309 - accuracy: 0.93 - ETA: 1s - loss: 0.1061 - accuracy: 0.95 - ETA: 1s - loss: 0.1033 - accuracy: 0.95 - ETA: 1s - loss: 0.1175 - accuracy: 0.95 - ETA: 1s - loss: 0.1238 - accuracy: 0.94 - ETA: 0s - loss: 0.1246 - accuracy: 0.93 - ETA: 0s - loss: 0.1180 - accuracy: 0.94 - ETA: 0s - loss: 0.1078 - accuracy: 0.95 - ETA: 0s - loss: 0.1042 - accuracy: 0.95 - ETA: 0s - loss: 0.1102 - accuracy: 0.95 - ETA: 0s - loss: 0.1284 - accuracy: 0.94 - ETA: 0s - loss: 0.1234 - accuracy: 0.94 - ETA: 0s - loss: 0.1227 - accuracy: 0.94 - ETA: 0s - loss: 0.1269 - accuracy: 0.94 - ETA: 0s - loss: 0.1318 - accuracy: 0.94 - ETA: 0s - loss: 0.1518 - accuracy: 0.93 - ETA: 0s - loss: 0.1480 - accuracy: 0.93 - ETA: 0s - loss: 0.1453 - accuracy: 0.94 - ETA: 0s - loss: 0.1440 - accuracy: 0.94 - ETA: 0s - loss: 0.1437 - accuracy: 0.94 - ETA: 0s - loss: 0.1418 - accuracy: 0.94 - ETA: 0s - loss: 0.1571 - accuracy: 0.94 - 1s 15ms/step - loss: 0.1637 - accuracy: 0.9396 - val_loss: 0.1899 - val_accuracy: 0.9250\n"
     ]
    }
   ],
   "source": [
    "#Reset the model weights to before training\n",
    "model.set_weights(Wsave)\n",
    "\n",
    "def replace_input_layers(old_model, new_input_shape):\n",
    "    inputs = tf.keras.layers.Input(shape=new_input_shape)\n",
    "    _y = tf.keras.layers.Dense(64, activation='linear')(inputs)  # spat filt\n",
    "    for layer_ix, layer in enumerate(old_model.layers):\n",
    "        if layer_ix > 1:\n",
    "            _y = layer(_y)\n",
    "    return tf.keras.Model(inputs, _y)\n",
    "\n",
    "# Choose another participant, replace the input layers to match new input size, and retrain the model.\n",
    "X, Y, ax_info = load_faces_houses('de')\n",
    "ds_train, ds_valid, n_train = get_ds_train_valid(X, Y, p_train=PTRAIN, batch_size=BATCH_SIZE)\n",
    "xfer_model = replace_input_layers(model, X.shape[1:])\n",
    "xfer_model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "xfer_model.summary()\n",
    "\n",
    "N_EPOCHS = 20\n",
    "history = xfer_model.fit(x=ds_train,  \n",
    "                     epochs=N_EPOCHS, \n",
    "                     validation_data=ds_valid,\n",
    "                     steps_per_epoch=n_train // BATCH_SIZE,\n",
    "                     validation_steps=(len(Y)-n_train) // BATCH_SIZE,\n",
    "                     verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "94/94 [==============================] - ETA: 17:40 - loss: 8.0613 - accuracy: 0.600 - ETA: 2:47 - loss: 7.7407 - accuracy: 0.433 - ETA: 1:26 - loss: 7.2823 - accuracy: 0.40 - ETA: 56s - loss: 6.3286 - accuracy: 0.4250 - ETA: 38s - loss: 5.5770 - accuracy: 0.454 - ETA: 28s - loss: 5.3938 - accuracy: 0.437 - ETA: 21s - loss: 4.9887 - accuracy: 0.490 - ETA: 17s - loss: 4.5898 - accuracy: 0.510 - ETA: 13s - loss: 4.2742 - accuracy: 0.522 - ETA: 10s - loss: 4.0892 - accuracy: 0.538 - ETA: 8s - loss: 3.7975 - accuracy: 0.552 - ETA: 6s - loss: 3.5697 - accuracy: 0.57 - ETA: 5s - loss: 3.4910 - accuracy: 0.56 - ETA: 3s - loss: 3.4127 - accuracy: 0.56 - ETA: 2s - loss: 3.2199 - accuracy: 0.57 - ETA: 1s - loss: 3.1124 - accuracy: 0.58 - ETA: 0s - loss: 3.0052 - accuracy: 0.59 - 13s 139ms/step - loss: 2.9097 - accuracy: 0.6000 - val_loss: 0.8805 - val_accuracy: 0.6783\n",
      "Epoch 2/10\n",
      "94/94 [==============================] - ETA: 9s - loss: 1.8695 - accuracy: 0.40 - ETA: 2s - loss: 0.9243 - accuracy: 0.76 - ETA: 1s - loss: 0.7746 - accuracy: 0.76 - ETA: 1s - loss: 1.0579 - accuracy: 0.72 - ETA: 0s - loss: 1.1249 - accuracy: 0.73 - ETA: 0s - loss: 1.0226 - accuracy: 0.74 - ETA: 0s - loss: 1.0325 - accuracy: 0.74 - ETA: 0s - loss: 1.0028 - accuracy: 0.74 - ETA: 0s - loss: 0.9512 - accuracy: 0.76 - ETA: 0s - loss: 0.9193 - accuracy: 0.76 - ETA: 0s - loss: 0.9570 - accuracy: 0.76 - ETA: 0s - loss: 0.9568 - accuracy: 0.75 - ETA: 0s - loss: 1.0051 - accuracy: 0.74 - ETA: 0s - loss: 1.0560 - accuracy: 0.74 - ETA: 0s - loss: 1.0395 - accuracy: 0.73 - ETA: 0s - loss: 1.0427 - accuracy: 0.73 - 1s 15ms/step - loss: 1.0339 - accuracy: 0.7362 - val_loss: 0.8710 - val_accuracy: 0.6609\n",
      "Epoch 3/10\n",
      "94/94 [==============================] - ETA: 8s - loss: 7.1095e-04 - accuracy: 1.00 - ETA: 2s - loss: 0.4303 - accuracy: 0.9000   - ETA: 1s - loss: 0.8342 - accuracy: 0.80 - ETA: 1s - loss: 0.8055 - accuracy: 0.80 - ETA: 0s - loss: 0.7725 - accuracy: 0.80 - ETA: 0s - loss: 0.8564 - accuracy: 0.80 - ETA: 0s - loss: 0.8516 - accuracy: 0.80 - ETA: 0s - loss: 0.8764 - accuracy: 0.80 - ETA: 0s - loss: 0.9179 - accuracy: 0.78 - ETA: 0s - loss: 0.8862 - accuracy: 0.78 - ETA: 0s - loss: 0.8428 - accuracy: 0.78 - ETA: 0s - loss: 0.9073 - accuracy: 0.77 - ETA: 0s - loss: 0.8850 - accuracy: 0.77 - ETA: 0s - loss: 0.9022 - accuracy: 0.77 - ETA: 0s - loss: 0.8775 - accuracy: 0.77 - ETA: 0s - loss: 0.8991 - accuracy: 0.77 - 1s 15ms/step - loss: 0.8727 - accuracy: 0.7766 - val_loss: 0.8154 - val_accuracy: 0.7043\n",
      "Epoch 4/10\n",
      "94/94 [==============================] - ETA: 7s - loss: 0.4553 - accuracy: 0.60 - ETA: 2s - loss: 0.7779 - accuracy: 0.73 - ETA: 1s - loss: 0.5708 - accuracy: 0.80 - ETA: 1s - loss: 0.8325 - accuracy: 0.77 - ETA: 0s - loss: 0.8350 - accuracy: 0.77 - ETA: 0s - loss: 0.8439 - accuracy: 0.76 - ETA: 0s - loss: 0.7756 - accuracy: 0.77 - ETA: 0s - loss: 0.7565 - accuracy: 0.78 - ETA: 0s - loss: 0.8071 - accuracy: 0.77 - ETA: 0s - loss: 0.8297 - accuracy: 0.77 - ETA: 0s - loss: 0.8094 - accuracy: 0.76 - ETA: 0s - loss: 0.8412 - accuracy: 0.75 - ETA: 0s - loss: 0.8441 - accuracy: 0.76 - ETA: 0s - loss: 0.8717 - accuracy: 0.76 - ETA: 0s - loss: 0.8687 - accuracy: 0.75 - ETA: 0s - loss: 0.8623 - accuracy: 0.75 - 1s 15ms/step - loss: 0.8679 - accuracy: 0.7532 - val_loss: 0.7833 - val_accuracy: 0.7391\n",
      "Epoch 5/10\n",
      "94/94 [==============================] - ETA: 8s - loss: 1.7042 - accuracy: 0.60 - ETA: 2s - loss: 0.7825 - accuracy: 0.76 - ETA: 1s - loss: 0.6676 - accuracy: 0.80 - ETA: 1s - loss: 0.5955 - accuracy: 0.82 - ETA: 0s - loss: 0.6846 - accuracy: 0.78 - ETA: 0s - loss: 0.6068 - accuracy: 0.78 - ETA: 0s - loss: 0.5756 - accuracy: 0.80 - ETA: 0s - loss: 0.6867 - accuracy: 0.79 - ETA: 0s - loss: 0.7008 - accuracy: 0.80 - ETA: 0s - loss: 0.7294 - accuracy: 0.79 - ETA: 0s - loss: 0.7346 - accuracy: 0.80 - ETA: 0s - loss: 0.6862 - accuracy: 0.81 - ETA: 0s - loss: 0.6796 - accuracy: 0.81 - ETA: 0s - loss: 0.6707 - accuracy: 0.81 - ETA: 0s - loss: 0.6983 - accuracy: 0.80 - ETA: 0s - loss: 0.7161 - accuracy: 0.80 - ETA: 0s - loss: 0.7008 - accuracy: 0.81 - 1s 15ms/step - loss: 0.7257 - accuracy: 0.8064 - val_loss: 0.7461 - val_accuracy: 0.7391\n",
      "Epoch 6/10\n",
      "94/94 [==============================] - ETA: 7s - loss: 0.4158 - accuracy: 0.80 - ETA: 2s - loss: 0.4982 - accuracy: 0.83 - ETA: 1s - loss: 0.4802 - accuracy: 0.83 - ETA: 1s - loss: 0.4767 - accuracy: 0.81 - ETA: 0s - loss: 0.4838 - accuracy: 0.83 - ETA: 0s - loss: 0.5009 - accuracy: 0.84 - ETA: 0s - loss: 0.5763 - accuracy: 0.82 - ETA: 0s - loss: 0.5669 - accuracy: 0.81 - ETA: 0s - loss: 0.6291 - accuracy: 0.80 - ETA: 0s - loss: 0.6231 - accuracy: 0.80 - ETA: 0s - loss: 0.6306 - accuracy: 0.80 - ETA: 0s - loss: 0.6223 - accuracy: 0.80 - ETA: 0s - loss: 0.6159 - accuracy: 0.80 - ETA: 0s - loss: 0.6481 - accuracy: 0.80 - ETA: 0s - loss: 0.6397 - accuracy: 0.80 - ETA: 0s - loss: 0.6433 - accuracy: 0.79 - 1s 15ms/step - loss: 0.6350 - accuracy: 0.8000 - val_loss: 0.7524 - val_accuracy: 0.7130\n",
      "Epoch 7/10\n",
      "94/94 [==============================] - ETA: 8s - loss: 0.5524 - accuracy: 0.80 - ETA: 1s - loss: 0.5943 - accuracy: 0.80 - ETA: 1s - loss: 0.3973 - accuracy: 0.87 - ETA: 1s - loss: 0.3945 - accuracy: 0.86 - ETA: 0s - loss: 0.3918 - accuracy: 0.86 - ETA: 0s - loss: 0.4484 - accuracy: 0.85 - ETA: 0s - loss: 0.4283 - accuracy: 0.85 - ETA: 0s - loss: 0.4392 - accuracy: 0.85 - ETA: 0s - loss: 0.4707 - accuracy: 0.84 - ETA: 0s - loss: 0.4619 - accuracy: 0.84 - ETA: 0s - loss: 0.4544 - accuracy: 0.84 - ETA: 0s - loss: 0.4606 - accuracy: 0.85 - ETA: 0s - loss: 0.4701 - accuracy: 0.84 - ETA: 0s - loss: 0.4679 - accuracy: 0.84 - ETA: 0s - loss: 0.4775 - accuracy: 0.84 - ETA: 0s - loss: 0.4760 - accuracy: 0.84 - 1s 15ms/step - loss: 0.4706 - accuracy: 0.8468 - val_loss: 0.6640 - val_accuracy: 0.7478\n",
      "Epoch 8/10\n",
      "94/94 [==============================] - ETA: 7s - loss: 0.2719 - accuracy: 0.80 - ETA: 2s - loss: 1.0501 - accuracy: 0.80 - ETA: 1s - loss: 0.8949 - accuracy: 0.81 - ETA: 1s - loss: 0.7502 - accuracy: 0.82 - ETA: 0s - loss: 0.6565 - accuracy: 0.83 - ETA: 0s - loss: 0.5703 - accuracy: 0.84 - ETA: 0s - loss: 0.5386 - accuracy: 0.84 - ETA: 0s - loss: 0.4738 - accuracy: 0.85 - ETA: 0s - loss: 0.4550 - accuracy: 0.84 - ETA: 0s - loss: 0.4249 - accuracy: 0.86 - ETA: 0s - loss: 0.4491 - accuracy: 0.86 - ETA: 0s - loss: 0.4264 - accuracy: 0.86 - ETA: 0s - loss: 0.4470 - accuracy: 0.86 - ETA: 0s - loss: 0.4670 - accuracy: 0.87 - ETA: 0s - loss: 0.4628 - accuracy: 0.86 - ETA: 0s - loss: 0.4463 - accuracy: 0.87 - 1s 15ms/step - loss: 0.4721 - accuracy: 0.8723 - val_loss: 0.6719 - val_accuracy: 0.7391\n",
      "Epoch 9/10\n",
      "94/94 [==============================] - ETA: 8s - loss: 1.7243 - accuracy: 0.80 - ETA: 2s - loss: 0.5230 - accuracy: 0.86 - ETA: 1s - loss: 0.3554 - accuracy: 0.88 - ETA: 1s - loss: 0.3546 - accuracy: 0.87 - ETA: 0s - loss: 0.4169 - accuracy: 0.85 - ETA: 0s - loss: 0.3757 - accuracy: 0.85 - ETA: 0s - loss: 0.3567 - accuracy: 0.85 - ETA: 0s - loss: 0.3704 - accuracy: 0.86 - ETA: 0s - loss: 0.3382 - accuracy: 0.86 - ETA: 0s - loss: 0.3409 - accuracy: 0.87 - ETA: 0s - loss: 0.3204 - accuracy: 0.88 - ETA: 0s - loss: 0.3573 - accuracy: 0.87 - ETA: 0s - loss: 0.3933 - accuracy: 0.87 - ETA: 0s - loss: 0.3711 - accuracy: 0.88 - ETA: 0s - loss: 0.3741 - accuracy: 0.88 - ETA: 0s - loss: 0.3908 - accuracy: 0.87 - 1s 15ms/step - loss: 0.3988 - accuracy: 0.8702 - val_loss: 0.6507 - val_accuracy: 0.7391\n",
      "Epoch 10/10\n",
      "94/94 [==============================] - ETA: 7s - loss: 0.8015 - accuracy: 0.60 - ETA: 2s - loss: 0.4395 - accuracy: 0.73 - ETA: 1s - loss: 0.3418 - accuracy: 0.81 - ETA: 1s - loss: 0.3277 - accuracy: 0.83 - ETA: 0s - loss: 0.4181 - accuracy: 0.83 - ETA: 0s - loss: 0.3656 - accuracy: 0.84 - ETA: 0s - loss: 0.3261 - accuracy: 0.86 - ETA: 0s - loss: 0.3131 - accuracy: 0.86 - ETA: 0s - loss: 0.2829 - accuracy: 0.87 - ETA: 0s - loss: 0.3361 - accuracy: 0.86 - ETA: 0s - loss: 0.3298 - accuracy: 0.86 - ETA: 0s - loss: 0.3153 - accuracy: 0.86 - ETA: 0s - loss: 0.3115 - accuracy: 0.87 - ETA: 0s - loss: 0.3191 - accuracy: 0.87 - ETA: 0s - loss: 0.3156 - accuracy: 0.87 - ETA: 0s - loss: 0.3043 - accuracy: 0.87 - 1s 15ms/step - loss: 0.2968 - accuracy: 0.8809 - val_loss: 0.6178 - val_accuracy: 0.7913\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "94/94 [==============================] - ETA: 23:01 - loss: 0.0184 - accuracy: 1.000 - ETA: 5:35 - loss: 0.0859 - accuracy: 0.950 - ETA: 2:40 - loss: 0.1803 - accuracy: 0.87 - ETA: 1:42 - loss: 0.2845 - accuracy: 0.85 - ETA: 1:13 - loss: 0.2314 - accuracy: 0.87 - ETA: 56s - loss: 0.2099 - accuracy: 0.8900 - ETA: 44s - loss: 0.2120 - accuracy: 0.883 - ETA: 35s - loss: 0.2658 - accuracy: 0.864 - ETA: 29s - loss: 0.2458 - accuracy: 0.881 - ETA: 24s - loss: 0.2336 - accuracy: 0.883 - ETA: 20s - loss: 0.2334 - accuracy: 0.885 - ETA: 17s - loss: 0.2438 - accuracy: 0.877 - ETA: 14s - loss: 0.2448 - accuracy: 0.879 - ETA: 12s - loss: 0.2328 - accuracy: 0.884 - ETA: 10s - loss: 0.2325 - accuracy: 0.889 - ETA: 8s - loss: 0.2269 - accuracy: 0.893 - ETA: 7s - loss: 0.2579 - accuracy: 0.88 - ETA: 6s - loss: 0.2695 - accuracy: 0.88 - ETA: 4s - loss: 0.2608 - accuracy: 0.88 - ETA: 3s - loss: 0.2699 - accuracy: 0.88 - ETA: 2s - loss: 0.2842 - accuracy: 0.88 - ETA: 1s - loss: 0.2732 - accuracy: 0.88 - ETA: 1s - loss: 0.2861 - accuracy: 0.88 - ETA: 0s - loss: 0.2811 - accuracy: 0.88 - 17s 181ms/step - loss: 0.2760 - accuracy: 0.8894 - val_loss: 0.4255 - val_accuracy: 0.8522\n",
      "Epoch 2/20\n",
      "94/94 [==============================] - ETA: 9s - loss: 0.5479 - accuracy: 0.60 - ETA: 3s - loss: 0.2585 - accuracy: 0.80 - ETA: 2s - loss: 0.1716 - accuracy: 0.87 - ETA: 1s - loss: 0.1244 - accuracy: 0.91 - ETA: 1s - loss: 0.2183 - accuracy: 0.90 - ETA: 1s - loss: 0.2248 - accuracy: 0.89 - ETA: 1s - loss: 0.2908 - accuracy: 0.88 - ETA: 1s - loss: 0.2640 - accuracy: 0.89 - ETA: 1s - loss: 0.2624 - accuracy: 0.89 - ETA: 1s - loss: 0.2408 - accuracy: 0.90 - ETA: 0s - loss: 0.2562 - accuracy: 0.90 - ETA: 0s - loss: 0.2386 - accuracy: 0.90 - ETA: 0s - loss: 0.2256 - accuracy: 0.91 - ETA: 0s - loss: 0.2151 - accuracy: 0.91 - ETA: 0s - loss: 0.2065 - accuracy: 0.92 - ETA: 0s - loss: 0.1942 - accuracy: 0.92 - ETA: 0s - loss: 0.2143 - accuracy: 0.91 - ETA: 0s - loss: 0.2072 - accuracy: 0.92 - ETA: 0s - loss: 0.2013 - accuracy: 0.92 - ETA: 0s - loss: 0.2213 - accuracy: 0.91 - ETA: 0s - loss: 0.2198 - accuracy: 0.91 - ETA: 0s - loss: 0.2196 - accuracy: 0.91 - ETA: 0s - loss: 0.2216 - accuracy: 0.91 - ETA: 0s - loss: 0.2261 - accuracy: 0.90 - 2s 20ms/step - loss: 0.2263 - accuracy: 0.9085 - val_loss: 0.2341 - val_accuracy: 0.9391\n",
      "Epoch 3/20\n",
      "94/94 [==============================] - ETA: 8s - loss: 0.0518 - accuracy: 1.00 - ETA: 2s - loss: 0.2802 - accuracy: 0.92 - ETA: 1s - loss: 0.1927 - accuracy: 0.93 - ETA: 1s - loss: 0.1933 - accuracy: 0.90 - ETA: 1s - loss: 0.1572 - accuracy: 0.92 - ETA: 1s - loss: 0.1305 - accuracy: 0.94 - ETA: 1s - loss: 0.1279 - accuracy: 0.94 - ETA: 1s - loss: 0.1359 - accuracy: 0.93 - ETA: 1s - loss: 0.1318 - accuracy: 0.93 - ETA: 0s - loss: 0.1256 - accuracy: 0.93 - ETA: 0s - loss: 0.1223 - accuracy: 0.93 - ETA: 0s - loss: 0.1171 - accuracy: 0.94 - ETA: 0s - loss: 0.1140 - accuracy: 0.94 - ETA: 0s - loss: 0.1265 - accuracy: 0.94 - ETA: 0s - loss: 0.1258 - accuracy: 0.94 - ETA: 0s - loss: 0.1641 - accuracy: 0.93 - ETA: 0s - loss: 0.1615 - accuracy: 0.93 - ETA: 0s - loss: 0.1525 - accuracy: 0.94 - ETA: 0s - loss: 0.1637 - accuracy: 0.93 - ETA: 0s - loss: 0.1592 - accuracy: 0.94 - ETA: 0s - loss: 0.2016 - accuracy: 0.93 - ETA: 0s - loss: 0.2028 - accuracy: 0.92 - ETA: 0s - loss: 0.1975 - accuracy: 0.92 - ETA: 0s - loss: 0.2077 - accuracy: 0.92 - 2s 19ms/step - loss: 0.2055 - accuracy: 0.9298 - val_loss: 0.2381 - val_accuracy: 0.9217\n",
      "Epoch 4/20\n",
      "94/94 [==============================] - ETA: 8s - loss: 0.1097 - accuracy: 1.00 - ETA: 2s - loss: 0.0430 - accuracy: 1.00 - ETA: 1s - loss: 0.0647 - accuracy: 0.97 - ETA: 1s - loss: 0.0726 - accuracy: 0.96 - ETA: 1s - loss: 0.0775 - accuracy: 0.96 - ETA: 1s - loss: 0.0916 - accuracy: 0.95 - ETA: 1s - loss: 0.0792 - accuracy: 0.96 - ETA: 1s - loss: 0.0968 - accuracy: 0.95 - ETA: 1s - loss: 0.0916 - accuracy: 0.95 - ETA: 0s - loss: 0.0924 - accuracy: 0.95 - ETA: 0s - loss: 0.0851 - accuracy: 0.96 - ETA: 0s - loss: 0.0877 - accuracy: 0.96 - ETA: 0s - loss: 0.0882 - accuracy: 0.96 - ETA: 0s - loss: 0.0881 - accuracy: 0.96 - ETA: 0s - loss: 0.0865 - accuracy: 0.96 - ETA: 0s - loss: 0.0926 - accuracy: 0.96 - ETA: 0s - loss: 0.0916 - accuracy: 0.96 - ETA: 0s - loss: 0.0935 - accuracy: 0.96 - ETA: 0s - loss: 0.0892 - accuracy: 0.96 - ETA: 0s - loss: 0.1050 - accuracy: 0.95 - ETA: 0s - loss: 0.1044 - accuracy: 0.95 - ETA: 0s - loss: 0.1081 - accuracy: 0.95 - ETA: 0s - loss: 0.1096 - accuracy: 0.95 - ETA: 0s - loss: 0.1105 - accuracy: 0.95 - 2s 20ms/step - loss: 0.1138 - accuracy: 0.9489 - val_loss: 0.2220 - val_accuracy: 0.9304\n",
      "Epoch 5/20\n",
      "94/94 [==============================] - ETA: 8s - loss: 0.0468 - accuracy: 1.00 - ETA: 2s - loss: 0.0328 - accuracy: 1.00 - ETA: 1s - loss: 0.0758 - accuracy: 0.95 - ETA: 1s - loss: 0.0739 - accuracy: 0.96 - ETA: 1s - loss: 0.1452 - accuracy: 0.95 - ETA: 1s - loss: 0.1577 - accuracy: 0.92 - ETA: 1s - loss: 0.1399 - accuracy: 0.93 - ETA: 1s - loss: 0.1281 - accuracy: 0.93 - ETA: 1s - loss: 0.1202 - accuracy: 0.93 - ETA: 0s - loss: 0.1244 - accuracy: 0.93 - ETA: 0s - loss: 0.1130 - accuracy: 0.94 - ETA: 0s - loss: 0.1053 - accuracy: 0.94 - ETA: 0s - loss: 0.1016 - accuracy: 0.94 - ETA: 0s - loss: 0.0973 - accuracy: 0.95 - ETA: 0s - loss: 0.0945 - accuracy: 0.95 - ETA: 0s - loss: 0.0905 - accuracy: 0.95 - ETA: 0s - loss: 0.0918 - accuracy: 0.95 - ETA: 0s - loss: 0.0906 - accuracy: 0.95 - ETA: 0s - loss: 0.0877 - accuracy: 0.95 - ETA: 0s - loss: 0.0853 - accuracy: 0.96 - ETA: 0s - loss: 0.0847 - accuracy: 0.96 - ETA: 0s - loss: 0.0834 - accuracy: 0.96 - ETA: 0s - loss: 0.0876 - accuracy: 0.96 - ETA: 0s - loss: 0.0845 - accuracy: 0.96 - 2s 19ms/step - loss: 0.0950 - accuracy: 0.9617 - val_loss: 0.2238 - val_accuracy: 0.9391\n",
      "Epoch 6/20\n",
      "94/94 [==============================] - ETA: 7s - loss: 0.0904 - accuracy: 1.00 - ETA: 3s - loss: 0.0348 - accuracy: 1.00 - ETA: 2s - loss: 0.0429 - accuracy: 1.00 - ETA: 1s - loss: 0.0729 - accuracy: 0.96 - ETA: 1s - loss: 0.0907 - accuracy: 0.96 - ETA: 1s - loss: 0.0843 - accuracy: 0.97 - ETA: 1s - loss: 0.0761 - accuracy: 0.97 - ETA: 1s - loss: 0.0723 - accuracy: 0.97 - ETA: 1s - loss: 0.0694 - accuracy: 0.98 - ETA: 0s - loss: 0.0629 - accuracy: 0.98 - ETA: 0s - loss: 0.0656 - accuracy: 0.98 - ETA: 0s - loss: 0.0674 - accuracy: 0.97 - ETA: 0s - loss: 0.0739 - accuracy: 0.97 - ETA: 0s - loss: 0.0701 - accuracy: 0.97 - ETA: 0s - loss: 0.0789 - accuracy: 0.96 - ETA: 0s - loss: 0.0768 - accuracy: 0.97 - ETA: 0s - loss: 0.0788 - accuracy: 0.96 - ETA: 0s - loss: 0.0792 - accuracy: 0.96 - ETA: 0s - loss: 0.0764 - accuracy: 0.96 - ETA: 0s - loss: 0.0763 - accuracy: 0.96 - ETA: 0s - loss: 0.0746 - accuracy: 0.96 - ETA: 0s - loss: 0.0727 - accuracy: 0.96 - ETA: 0s - loss: 0.0757 - accuracy: 0.96 - ETA: 0s - loss: 0.0746 - accuracy: 0.96 - 2s 19ms/step - loss: 0.0746 - accuracy: 0.9681 - val_loss: 0.2060 - val_accuracy: 0.9478\n",
      "Epoch 7/20\n",
      "94/94 [==============================] - ETA: 7s - loss: 0.0449 - accuracy: 1.00 - ETA: 3s - loss: 0.0688 - accuracy: 0.95 - ETA: 2s - loss: 0.0418 - accuracy: 0.97 - ETA: 1s - loss: 0.0305 - accuracy: 0.98 - ETA: 1s - loss: 0.0246 - accuracy: 0.98 - ETA: 1s - loss: 0.0259 - accuracy: 0.99 - ETA: 1s - loss: 0.0421 - accuracy: 0.97 - ETA: 1s - loss: 0.0616 - accuracy: 0.96 - ETA: 1s - loss: 0.1055 - accuracy: 0.95 - ETA: 0s - loss: 0.0956 - accuracy: 0.95 - ETA: 0s - loss: 0.0890 - accuracy: 0.96 - ETA: 0s - loss: 0.0860 - accuracy: 0.95 - ETA: 0s - loss: 0.0936 - accuracy: 0.95 - ETA: 0s - loss: 0.0873 - accuracy: 0.95 - ETA: 0s - loss: 0.1034 - accuracy: 0.95 - ETA: 0s - loss: 0.0981 - accuracy: 0.96 - ETA: 0s - loss: 0.0973 - accuracy: 0.96 - ETA: 0s - loss: 0.0922 - accuracy: 0.96 - ETA: 0s - loss: 0.0890 - accuracy: 0.96 - ETA: 0s - loss: 0.0862 - accuracy: 0.96 - ETA: 0s - loss: 0.0880 - accuracy: 0.96 - ETA: 0s - loss: 0.0868 - accuracy: 0.96 - ETA: 0s - loss: 0.0844 - accuracy: 0.96 - ETA: 0s - loss: 0.0849 - accuracy: 0.96 - 2s 20ms/step - loss: 0.0854 - accuracy: 0.9660 - val_loss: 0.2316 - val_accuracy: 0.9217\n",
      "Epoch 8/20\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "94/94 [==============================] - ETA: 7s - loss: 0.0870 - accuracy: 1.00 - ETA: 3s - loss: 0.0703 - accuracy: 0.95 - ETA: 2s - loss: 0.1039 - accuracy: 0.95 - ETA: 1s - loss: 0.1645 - accuracy: 0.93 - ETA: 1s - loss: 0.1321 - accuracy: 0.95 - ETA: 1s - loss: 0.1447 - accuracy: 0.95 - ETA: 1s - loss: 0.1302 - accuracy: 0.95 - ETA: 1s - loss: 0.1129 - accuracy: 0.95 - ETA: 1s - loss: 0.1024 - accuracy: 0.96 - ETA: 0s - loss: 0.1238 - accuracy: 0.96 - ETA: 0s - loss: 0.1150 - accuracy: 0.96 - ETA: 0s - loss: 0.1051 - accuracy: 0.96 - ETA: 0s - loss: 0.1029 - accuracy: 0.97 - ETA: 0s - loss: 0.0971 - accuracy: 0.97 - ETA: 0s - loss: 0.1032 - accuracy: 0.97 - ETA: 0s - loss: 0.1070 - accuracy: 0.97 - ETA: 0s - loss: 0.1049 - accuracy: 0.96 - ETA: 0s - loss: 0.1002 - accuracy: 0.97 - ETA: 0s - loss: 0.0955 - accuracy: 0.97 - ETA: 0s - loss: 0.1018 - accuracy: 0.97 - ETA: 0s - loss: 0.1020 - accuracy: 0.97 - ETA: 0s - loss: 0.1013 - accuracy: 0.97 - ETA: 0s - loss: 0.1073 - accuracy: 0.97 - ETA: 0s - loss: 0.1041 - accuracy: 0.97 - 2s 20ms/step - loss: 0.1174 - accuracy: 0.9702 - val_loss: 0.3105 - val_accuracy: 0.9130\n",
      "Epoch 9/20\n",
      "94/94 [==============================] - ETA: 8s - loss: 0.0431 - accuracy: 1.00 - ETA: 2s - loss: 0.0456 - accuracy: 1.00 - ETA: 1s - loss: 0.0991 - accuracy: 0.97 - ETA: 1s - loss: 0.0926 - accuracy: 0.96 - ETA: 1s - loss: 0.0871 - accuracy: 0.96 - ETA: 1s - loss: 0.1036 - accuracy: 0.95 - ETA: 1s - loss: 0.0908 - accuracy: 0.96 - ETA: 1s - loss: 0.0990 - accuracy: 0.95 - ETA: 1s - loss: 0.0908 - accuracy: 0.96 - ETA: 0s - loss: 0.0878 - accuracy: 0.96 - ETA: 0s - loss: 0.0832 - accuracy: 0.96 - ETA: 0s - loss: 0.0882 - accuracy: 0.96 - ETA: 0s - loss: 0.0845 - accuracy: 0.96 - ETA: 0s - loss: 0.0928 - accuracy: 0.96 - ETA: 0s - loss: 0.0886 - accuracy: 0.96 - ETA: 0s - loss: 0.0912 - accuracy: 0.96 - ETA: 0s - loss: 0.0992 - accuracy: 0.95 - ETA: 0s - loss: 0.1019 - accuracy: 0.95 - ETA: 0s - loss: 0.1005 - accuracy: 0.95 - ETA: 0s - loss: 0.0973 - accuracy: 0.95 - ETA: 0s - loss: 0.0939 - accuracy: 0.95 - ETA: 0s - loss: 0.0925 - accuracy: 0.95 - ETA: 0s - loss: 0.0973 - accuracy: 0.95 - ETA: 0s - loss: 0.0937 - accuracy: 0.95 - 2s 20ms/step - loss: 0.0927 - accuracy: 0.9553 - val_loss: 0.1748 - val_accuracy: 0.9130\n",
      "Epoch 10/20\n",
      "94/94 [==============================] - ETA: 8s - loss: 0.0118 - accuracy: 1.00 - ETA: 3s - loss: 0.1783 - accuracy: 0.95 - ETA: 2s - loss: 0.1913 - accuracy: 0.95 - ETA: 1s - loss: 0.1586 - accuracy: 0.95 - ETA: 1s - loss: 0.1312 - accuracy: 0.96 - ETA: 1s - loss: 0.1108 - accuracy: 0.97 - ETA: 1s - loss: 0.1041 - accuracy: 0.96 - ETA: 1s - loss: 0.0966 - accuracy: 0.97 - ETA: 1s - loss: 0.0966 - accuracy: 0.96 - ETA: 0s - loss: 0.1458 - accuracy: 0.96 - ETA: 0s - loss: 0.1604 - accuracy: 0.96 - ETA: 0s - loss: 0.1602 - accuracy: 0.95 - ETA: 0s - loss: 0.1506 - accuracy: 0.95 - ETA: 0s - loss: 0.1916 - accuracy: 0.94 - ETA: 0s - loss: 0.1874 - accuracy: 0.94 - ETA: 0s - loss: 0.1896 - accuracy: 0.94 - ETA: 0s - loss: 0.1822 - accuracy: 0.94 - ETA: 0s - loss: 0.1766 - accuracy: 0.94 - ETA: 0s - loss: 0.1671 - accuracy: 0.94 - ETA: 0s - loss: 0.1594 - accuracy: 0.94 - ETA: 0s - loss: 0.1533 - accuracy: 0.95 - ETA: 0s - loss: 0.1506 - accuracy: 0.95 - ETA: 0s - loss: 0.1461 - accuracy: 0.95 - ETA: 0s - loss: 0.1469 - accuracy: 0.95 - 2s 20ms/step - loss: 0.1451 - accuracy: 0.9532 - val_loss: 0.1558 - val_accuracy: 0.9217\n",
      "Epoch 11/20\n",
      "94/94 [==============================] - ETA: 8s - loss: 2.7483e-04 - accuracy: 1.00 - ETA: 2s - loss: 0.0234 - accuracy: 1.0000   - ETA: 2s - loss: 0.1129 - accuracy: 0.97 - ETA: 1s - loss: 0.1750 - accuracy: 0.96 - ETA: 1s - loss: 0.1391 - accuracy: 0.97 - ETA: 1s - loss: 0.1152 - accuracy: 0.98 - ETA: 1s - loss: 0.1079 - accuracy: 0.97 - ETA: 1s - loss: 0.0943 - accuracy: 0.97 - ETA: 1s - loss: 0.0991 - accuracy: 0.97 - ETA: 0s - loss: 0.1008 - accuracy: 0.97 - ETA: 0s - loss: 0.0972 - accuracy: 0.98 - ETA: 0s - loss: 0.0891 - accuracy: 0.98 - ETA: 0s - loss: 0.0903 - accuracy: 0.97 - ETA: 0s - loss: 0.0891 - accuracy: 0.98 - ETA: 0s - loss: 0.0873 - accuracy: 0.98 - ETA: 0s - loss: 0.0856 - accuracy: 0.98 - ETA: 0s - loss: 0.0881 - accuracy: 0.97 - ETA: 0s - loss: 0.0983 - accuracy: 0.97 - ETA: 0s - loss: 0.0934 - accuracy: 0.97 - ETA: 0s - loss: 0.0934 - accuracy: 0.97 - ETA: 0s - loss: 0.0949 - accuracy: 0.97 - ETA: 0s - loss: 0.0919 - accuracy: 0.97 - ETA: 0s - loss: 0.0886 - accuracy: 0.97 - ETA: 0s - loss: 0.0864 - accuracy: 0.97 - 2s 20ms/step - loss: 0.0855 - accuracy: 0.9766 - val_loss: 0.1558 - val_accuracy: 0.9391\n",
      "Epoch 12/20\n",
      "94/94 [==============================] - ETA: 7s - loss: 0.2771 - accuracy: 0.80 - ETA: 3s - loss: 0.0779 - accuracy: 0.95 - ETA: 2s - loss: 0.0793 - accuracy: 0.95 - ETA: 1s - loss: 0.0626 - accuracy: 0.96 - ETA: 1s - loss: 0.0648 - accuracy: 0.97 - ETA: 1s - loss: 0.0614 - accuracy: 0.98 - ETA: 1s - loss: 0.0624 - accuracy: 0.97 - ETA: 1s - loss: 0.0575 - accuracy: 0.97 - ETA: 1s - loss: 0.0513 - accuracy: 0.98 - ETA: 0s - loss: 0.0499 - accuracy: 0.98 - ETA: 0s - loss: 0.0550 - accuracy: 0.98 - ETA: 0s - loss: 0.0638 - accuracy: 0.97 - ETA: 0s - loss: 0.0586 - accuracy: 0.97 - ETA: 0s - loss: 0.0549 - accuracy: 0.97 - ETA: 0s - loss: 0.0529 - accuracy: 0.97 - ETA: 0s - loss: 0.0506 - accuracy: 0.98 - ETA: 0s - loss: 0.0480 - accuracy: 0.98 - ETA: 0s - loss: 0.0475 - accuracy: 0.98 - ETA: 0s - loss: 0.0535 - accuracy: 0.97 - ETA: 0s - loss: 0.0553 - accuracy: 0.97 - ETA: 0s - loss: 0.0660 - accuracy: 0.97 - ETA: 0s - loss: 0.0647 - accuracy: 0.97 - ETA: 0s - loss: 0.0683 - accuracy: 0.97 - ETA: 0s - loss: 0.0727 - accuracy: 0.97 - 2s 19ms/step - loss: 0.0777 - accuracy: 0.9702 - val_loss: 0.1987 - val_accuracy: 0.9217\n",
      "Epoch 13/20\n",
      "94/94 [==============================] - ETA: 8s - loss: 0.1251 - accuracy: 1.00 - ETA: 2s - loss: 0.0300 - accuracy: 1.00 - ETA: 1s - loss: 0.0740 - accuracy: 0.95 - ETA: 1s - loss: 0.0854 - accuracy: 0.95 - ETA: 1s - loss: 0.0787 - accuracy: 0.95 - ETA: 1s - loss: 0.0733 - accuracy: 0.95 - ETA: 1s - loss: 0.0776 - accuracy: 0.96 - ETA: 1s - loss: 0.0674 - accuracy: 0.96 - ETA: 0s - loss: 0.0637 - accuracy: 0.96 - ETA: 0s - loss: 0.0608 - accuracy: 0.97 - ETA: 0s - loss: 0.0684 - accuracy: 0.97 - ETA: 0s - loss: 0.0674 - accuracy: 0.97 - ETA: 0s - loss: 0.0655 - accuracy: 0.97 - ETA: 0s - loss: 0.0612 - accuracy: 0.97 - ETA: 0s - loss: 0.0671 - accuracy: 0.97 - ETA: 0s - loss: 0.0695 - accuracy: 0.97 - ETA: 0s - loss: 0.0710 - accuracy: 0.97 - ETA: 0s - loss: 0.0768 - accuracy: 0.96 - ETA: 0s - loss: 0.0749 - accuracy: 0.96 - ETA: 0s - loss: 0.0763 - accuracy: 0.96 - ETA: 0s - loss: 0.0745 - accuracy: 0.97 - ETA: 0s - loss: 0.0711 - accuracy: 0.97 - ETA: 0s - loss: 0.0682 - accuracy: 0.97 - ETA: 0s - loss: 0.0766 - accuracy: 0.96 - 2s 20ms/step - loss: 0.0758 - accuracy: 0.9681 - val_loss: 0.1712 - val_accuracy: 0.9391\n",
      "Epoch 14/20\n",
      "94/94 [==============================] - ETA: 8s - loss: 0.0099 - accuracy: 1.00 - ETA: 2s - loss: 0.0065 - accuracy: 1.00 - ETA: 1s - loss: 0.0250 - accuracy: 1.00 - ETA: 1s - loss: 0.0261 - accuracy: 1.00 - ETA: 1s - loss: 0.0637 - accuracy: 0.98 - ETA: 1s - loss: 0.0590 - accuracy: 0.99 - ETA: 1s - loss: 0.0564 - accuracy: 0.99 - ETA: 1s - loss: 0.0523 - accuracy: 0.99 - ETA: 0s - loss: 0.0525 - accuracy: 0.99 - ETA: 0s - loss: 0.0509 - accuracy: 0.99 - ETA: 0s - loss: 0.0492 - accuracy: 0.99 - ETA: 0s - loss: 0.0457 - accuracy: 0.99 - ETA: 0s - loss: 0.0472 - accuracy: 0.99 - ETA: 0s - loss: 0.0464 - accuracy: 0.99 - ETA: 0s - loss: 0.0489 - accuracy: 0.98 - ETA: 0s - loss: 0.0474 - accuracy: 0.99 - ETA: 0s - loss: 0.0483 - accuracy: 0.98 - ETA: 0s - loss: 0.0458 - accuracy: 0.98 - ETA: 0s - loss: 0.0438 - accuracy: 0.98 - ETA: 0s - loss: 0.0445 - accuracy: 0.98 - ETA: 0s - loss: 0.0440 - accuracy: 0.98 - ETA: 0s - loss: 0.0424 - accuracy: 0.98 - ETA: 0s - loss: 0.0406 - accuracy: 0.98 - ETA: 0s - loss: 0.0390 - accuracy: 0.98 - 2s 19ms/step - loss: 0.0391 - accuracy: 0.9894 - val_loss: 0.2203 - val_accuracy: 0.9391\n",
      "Epoch 15/20\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "94/94 [==============================] - ETA: 7s - loss: 7.6624e-04 - accuracy: 1.00 - ETA: 3s - loss: 0.0272 - accuracy: 1.0000   - ETA: 2s - loss: 0.0525 - accuracy: 0.97 - ETA: 1s - loss: 0.0490 - accuracy: 0.98 - ETA: 1s - loss: 0.0505 - accuracy: 0.97 - ETA: 1s - loss: 0.0518 - accuracy: 0.98 - ETA: 1s - loss: 0.0496 - accuracy: 0.98 - ETA: 1s - loss: 0.0495 - accuracy: 0.98 - ETA: 1s - loss: 0.0479 - accuracy: 0.98 - ETA: 0s - loss: 0.0437 - accuracy: 0.98 - ETA: 0s - loss: 0.0524 - accuracy: 0.98 - ETA: 0s - loss: 0.0573 - accuracy: 0.97 - ETA: 0s - loss: 0.0530 - accuracy: 0.97 - ETA: 0s - loss: 0.0505 - accuracy: 0.97 - ETA: 0s - loss: 0.0474 - accuracy: 0.97 - ETA: 0s - loss: 0.0444 - accuracy: 0.98 - ETA: 0s - loss: 0.0435 - accuracy: 0.98 - ETA: 0s - loss: 0.0489 - accuracy: 0.97 - ETA: 0s - loss: 0.0483 - accuracy: 0.97 - ETA: 0s - loss: 0.0499 - accuracy: 0.97 - ETA: 0s - loss: 0.0514 - accuracy: 0.97 - ETA: 0s - loss: 0.0498 - accuracy: 0.97 - ETA: 0s - loss: 0.0500 - accuracy: 0.97 - ETA: 0s - loss: 0.0568 - accuracy: 0.97 - 2s 20ms/step - loss: 0.0626 - accuracy: 0.9723 - val_loss: 0.2152 - val_accuracy: 0.9304\n",
      "Epoch 16/20\n",
      "94/94 [==============================] - ETA: 8s - loss: 0.0010 - accuracy: 1.00 - ETA: 2s - loss: 0.0514 - accuracy: 0.96 - ETA: 2s - loss: 0.0845 - accuracy: 0.95 - ETA: 1s - loss: 0.0612 - accuracy: 0.96 - ETA: 1s - loss: 0.0515 - accuracy: 0.97 - ETA: 1s - loss: 0.0604 - accuracy: 0.97 - ETA: 1s - loss: 0.0531 - accuracy: 0.97 - ETA: 1s - loss: 0.0492 - accuracy: 0.97 - ETA: 1s - loss: 0.0597 - accuracy: 0.96 - ETA: 0s - loss: 0.0539 - accuracy: 0.97 - ETA: 0s - loss: 0.0493 - accuracy: 0.97 - ETA: 0s - loss: 0.0453 - accuracy: 0.97 - ETA: 0s - loss: 0.0508 - accuracy: 0.97 - ETA: 0s - loss: 0.0489 - accuracy: 0.97 - ETA: 0s - loss: 0.0459 - accuracy: 0.97 - ETA: 0s - loss: 0.0436 - accuracy: 0.98 - ETA: 0s - loss: 0.0460 - accuracy: 0.97 - ETA: 0s - loss: 0.0466 - accuracy: 0.97 - ETA: 0s - loss: 0.0472 - accuracy: 0.97 - ETA: 0s - loss: 0.0474 - accuracy: 0.97 - ETA: 0s - loss: 0.0704 - accuracy: 0.97 - ETA: 0s - loss: 0.0692 - accuracy: 0.97 - ETA: 0s - loss: 0.0673 - accuracy: 0.97 - ETA: 0s - loss: 0.0648 - accuracy: 0.97 - 2s 19ms/step - loss: 0.0641 - accuracy: 0.9745 - val_loss: 0.2016 - val_accuracy: 0.9391\n",
      "Epoch 17/20\n",
      "94/94 [==============================] - ETA: 9s - loss: 0.2095 - accuracy: 1.00 - ETA: 2s - loss: 0.1999 - accuracy: 0.96 - ETA: 2s - loss: 0.1940 - accuracy: 0.95 - ETA: 1s - loss: 0.1368 - accuracy: 0.96 - ETA: 1s - loss: 0.1113 - accuracy: 0.97 - ETA: 1s - loss: 0.0936 - accuracy: 0.98 - ETA: 1s - loss: 0.0807 - accuracy: 0.98 - ETA: 1s - loss: 0.0709 - accuracy: 0.98 - ETA: 1s - loss: 0.0767 - accuracy: 0.97 - ETA: 0s - loss: 0.0778 - accuracy: 0.96 - ETA: 0s - loss: 0.0790 - accuracy: 0.96 - ETA: 0s - loss: 0.1058 - accuracy: 0.96 - ETA: 0s - loss: 0.1047 - accuracy: 0.96 - ETA: 0s - loss: 0.1103 - accuracy: 0.95 - ETA: 0s - loss: 0.1129 - accuracy: 0.95 - ETA: 0s - loss: 0.1059 - accuracy: 0.95 - ETA: 0s - loss: 0.1031 - accuracy: 0.96 - ETA: 0s - loss: 0.1014 - accuracy: 0.96 - ETA: 0s - loss: 0.1012 - accuracy: 0.95 - ETA: 0s - loss: 0.0985 - accuracy: 0.96 - ETA: 0s - loss: 0.0993 - accuracy: 0.95 - ETA: 0s - loss: 0.0952 - accuracy: 0.96 - ETA: 0s - loss: 0.0913 - accuracy: 0.96 - ETA: 0s - loss: 0.0888 - accuracy: 0.96 - 2s 20ms/step - loss: 0.0879 - accuracy: 0.9638 - val_loss: 0.1927 - val_accuracy: 0.9391\n",
      "Epoch 18/20\n",
      "94/94 [==============================] - ETA: 8s - loss: 5.9716e-05 - accuracy: 1.00 - ETA: 2s - loss: 0.0552 - accuracy: 1.0000   - ETA: 1s - loss: 0.0507 - accuracy: 0.97 - ETA: 1s - loss: 0.0741 - accuracy: 0.93 - ETA: 1s - loss: 0.0684 - accuracy: 0.95 - ETA: 1s - loss: 0.0638 - accuracy: 0.96 - ETA: 1s - loss: 0.0546 - accuracy: 0.96 - ETA: 1s - loss: 0.0510 - accuracy: 0.97 - ETA: 1s - loss: 0.0526 - accuracy: 0.97 - ETA: 0s - loss: 0.0505 - accuracy: 0.97 - ETA: 0s - loss: 0.0489 - accuracy: 0.98 - ETA: 0s - loss: 0.0452 - accuracy: 0.98 - ETA: 0s - loss: 0.0500 - accuracy: 0.98 - ETA: 0s - loss: 0.0613 - accuracy: 0.97 - ETA: 0s - loss: 0.0633 - accuracy: 0.97 - ETA: 0s - loss: 0.0614 - accuracy: 0.98 - ETA: 0s - loss: 0.0586 - accuracy: 0.98 - ETA: 0s - loss: 0.0605 - accuracy: 0.97 - ETA: 0s - loss: 0.0579 - accuracy: 0.98 - ETA: 0s - loss: 0.0550 - accuracy: 0.98 - ETA: 0s - loss: 0.0569 - accuracy: 0.98 - ETA: 0s - loss: 0.0545 - accuracy: 0.98 - ETA: 0s - loss: 0.0530 - accuracy: 0.98 - ETA: 0s - loss: 0.0512 - accuracy: 0.98 - 2s 20ms/step - loss: 0.0507 - accuracy: 0.9830 - val_loss: 0.1578 - val_accuracy: 0.9478\n",
      "Epoch 19/20\n",
      "94/94 [==============================] - ETA: 7s - loss: 0.0067 - accuracy: 1.00 - ETA: 3s - loss: 0.0047 - accuracy: 1.00 - ETA: 2s - loss: 0.0031 - accuracy: 1.00 - ETA: 1s - loss: 0.0248 - accuracy: 1.00 - ETA: 1s - loss: 0.0270 - accuracy: 1.00 - ETA: 1s - loss: 0.0355 - accuracy: 0.99 - ETA: 1s - loss: 0.0328 - accuracy: 0.99 - ETA: 1s - loss: 0.0476 - accuracy: 0.97 - ETA: 1s - loss: 0.0417 - accuracy: 0.98 - ETA: 0s - loss: 0.0409 - accuracy: 0.98 - ETA: 0s - loss: 0.0370 - accuracy: 0.98 - ETA: 0s - loss: 0.0368 - accuracy: 0.98 - ETA: 0s - loss: 0.0371 - accuracy: 0.98 - ETA: 0s - loss: 0.0379 - accuracy: 0.98 - ETA: 0s - loss: 0.0380 - accuracy: 0.98 - ETA: 0s - loss: 0.0389 - accuracy: 0.99 - ETA: 0s - loss: 0.0382 - accuracy: 0.99 - ETA: 0s - loss: 0.0385 - accuracy: 0.98 - ETA: 0s - loss: 0.0378 - accuracy: 0.98 - ETA: 0s - loss: 0.0360 - accuracy: 0.98 - ETA: 0s - loss: 0.0356 - accuracy: 0.99 - ETA: 0s - loss: 0.0340 - accuracy: 0.99 - ETA: 0s - loss: 0.0369 - accuracy: 0.99 - ETA: 0s - loss: 0.0355 - accuracy: 0.99 - 2s 20ms/step - loss: 0.0372 - accuracy: 0.9894 - val_loss: 0.1888 - val_accuracy: 0.9565\n",
      "Epoch 20/20\n",
      "94/94 [==============================] - ETA: 8s - loss: 0.0087 - accuracy: 1.00 - ETA: 2s - loss: 0.0069 - accuracy: 1.00 - ETA: 1s - loss: 0.0245 - accuracy: 1.00 - ETA: 1s - loss: 0.0255 - accuracy: 1.00 - ETA: 1s - loss: 0.0222 - accuracy: 1.00 - ETA: 1s - loss: 0.0215 - accuracy: 1.00 - ETA: 1s - loss: 0.0282 - accuracy: 0.99 - ETA: 1s - loss: 0.0376 - accuracy: 0.98 - ETA: 1s - loss: 0.0422 - accuracy: 0.98 - ETA: 0s - loss: 0.0378 - accuracy: 0.98 - ETA: 0s - loss: 0.0376 - accuracy: 0.99 - ETA: 0s - loss: 0.0385 - accuracy: 0.99 - ETA: 0s - loss: 0.0354 - accuracy: 0.99 - ETA: 0s - loss: 0.0364 - accuracy: 0.99 - ETA: 0s - loss: 0.0352 - accuracy: 0.99 - ETA: 0s - loss: 0.0337 - accuracy: 0.99 - ETA: 0s - loss: 0.0319 - accuracy: 0.99 - ETA: 0s - loss: 0.0314 - accuracy: 0.99 - ETA: 0s - loss: 0.0341 - accuracy: 0.99 - ETA: 0s - loss: 0.0325 - accuracy: 0.99 - ETA: 0s - loss: 0.0310 - accuracy: 0.99 - ETA: 0s - loss: 0.0303 - accuracy: 0.99 - ETA: 0s - loss: 0.0293 - accuracy: 0.99 - ETA: 0s - loss: 0.0304 - accuracy: 0.99 - 2s 20ms/step - loss: 0.0309 - accuracy: 0.9915 - val_loss: 0.2095 - val_accuracy: 0.9391\n",
      "Epoch 1/10\n",
      "90/90 [==============================] - ETA: 16:14 - loss: 5.3056 - accuracy: 0.400 - ETA: 3:07 - loss: 9.3542 - accuracy: 0.360 - ETA: 1:19 - loss: 5.0929 - accuracy: 0.54 - ETA: 47s - loss: 5.0706 - accuracy: 0.6118 - ETA: 32s - loss: 4.6858 - accuracy: 0.573 - ETA: 23s - loss: 4.5241 - accuracy: 0.586 - ETA: 17s - loss: 4.0896 - accuracy: 0.600 - ETA: 14s - loss: 3.8444 - accuracy: 0.605 - ETA: 10s - loss: 3.5844 - accuracy: 0.595 - ETA: 8s - loss: 3.4035 - accuracy: 0.596 - ETA: 6s - loss: 3.2083 - accuracy: 0.62 - ETA: 5s - loss: 3.1313 - accuracy: 0.61 - ETA: 3s - loss: 3.1296 - accuracy: 0.61 - ETA: 2s - loss: 3.0598 - accuracy: 0.60 - ETA: 1s - loss: 2.9427 - accuracy: 0.61 - ETA: 0s - loss: 2.8170 - accuracy: 0.61 - 13s 140ms/step - loss: 2.6629 - accuracy: 0.6311 - val_loss: 1.2371 - val_accuracy: 0.6273\n",
      "Epoch 2/10\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "90/90 [==============================] - ETA: 7s - loss: 0.8591 - accuracy: 0.80 - ETA: 2s - loss: 0.5742 - accuracy: 0.80 - ETA: 1s - loss: 1.0156 - accuracy: 0.81 - ETA: 1s - loss: 0.7711 - accuracy: 0.84 - ETA: 0s - loss: 0.8632 - accuracy: 0.81 - ETA: 0s - loss: 0.9857 - accuracy: 0.77 - ETA: 0s - loss: 0.9555 - accuracy: 0.79 - ETA: 0s - loss: 0.9490 - accuracy: 0.80 - ETA: 0s - loss: 0.9455 - accuracy: 0.78 - ETA: 0s - loss: 0.9753 - accuracy: 0.77 - ETA: 0s - loss: 1.1378 - accuracy: 0.76 - ETA: 0s - loss: 1.1134 - accuracy: 0.76 - ETA: 0s - loss: 1.1215 - accuracy: 0.76 - ETA: 0s - loss: 1.0731 - accuracy: 0.76 - ETA: 0s - loss: 1.0291 - accuracy: 0.77 - ETA: 0s - loss: 1.0488 - accuracy: 0.76 - 1s 16ms/step - loss: 1.0424 - accuracy: 0.7667 - val_loss: 1.1509 - val_accuracy: 0.6455\n",
      "Epoch 3/10\n",
      "90/90 [==============================] - ETA: 7s - loss: 0.5436 - accuracy: 0.80 - ETA: 1s - loss: 0.3938 - accuracy: 0.83 - ETA: 1s - loss: 0.7433 - accuracy: 0.78 - ETA: 0s - loss: 0.7293 - accuracy: 0.77 - ETA: 0s - loss: 0.8201 - accuracy: 0.76 - ETA: 0s - loss: 0.8809 - accuracy: 0.76 - ETA: 0s - loss: 0.9845 - accuracy: 0.73 - ETA: 0s - loss: 0.9028 - accuracy: 0.76 - ETA: 0s - loss: 0.8799 - accuracy: 0.77 - ETA: 0s - loss: 0.8796 - accuracy: 0.76 - ETA: 0s - loss: 0.8436 - accuracy: 0.76 - ETA: 0s - loss: 0.8336 - accuracy: 0.76 - ETA: 0s - loss: 0.7976 - accuracy: 0.77 - ETA: 0s - loss: 0.7673 - accuracy: 0.78 - ETA: 0s - loss: 0.7257 - accuracy: 0.78 - ETA: 0s - loss: 0.7116 - accuracy: 0.78 - 1s 15ms/step - loss: 0.7075 - accuracy: 0.7844 - val_loss: 0.8780 - val_accuracy: 0.6909\n",
      "Epoch 4/10\n",
      "90/90 [==============================] - ETA: 6s - loss: 0.2127 - accuracy: 1.00 - ETA: 2s - loss: 0.5820 - accuracy: 0.84 - ETA: 1s - loss: 0.7236 - accuracy: 0.83 - ETA: 1s - loss: 0.6053 - accuracy: 0.84 - ETA: 0s - loss: 0.4873 - accuracy: 0.87 - ETA: 0s - loss: 0.5196 - accuracy: 0.86 - ETA: 0s - loss: 0.4846 - accuracy: 0.86 - ETA: 0s - loss: 0.5416 - accuracy: 0.85 - ETA: 0s - loss: 0.5042 - accuracy: 0.85 - ETA: 0s - loss: 0.5958 - accuracy: 0.83 - ETA: 0s - loss: 0.5807 - accuracy: 0.82 - ETA: 0s - loss: 0.6054 - accuracy: 0.81 - ETA: 0s - loss: 0.6090 - accuracy: 0.81 - ETA: 0s - loss: 0.5975 - accuracy: 0.80 - ETA: 0s - loss: 0.5982 - accuracy: 0.80 - ETA: 0s - loss: 0.5659 - accuracy: 0.81 - ETA: 0s - loss: 0.5475 - accuracy: 0.81 - 1s 16ms/step - loss: 0.5496 - accuracy: 0.8111 - val_loss: 0.8330 - val_accuracy: 0.6727\n",
      "Epoch 5/10\n",
      "90/90 [==============================] - ETA: 6s - loss: 0.1141 - accuracy: 1.00 - ETA: 1s - loss: 0.3035 - accuracy: 0.86 - ETA: 1s - loss: 0.4057 - accuracy: 0.85 - ETA: 0s - loss: 0.6383 - accuracy: 0.77 - ETA: 0s - loss: 0.6321 - accuracy: 0.77 - ETA: 0s - loss: 0.5803 - accuracy: 0.77 - ETA: 0s - loss: 0.6518 - accuracy: 0.77 - ETA: 0s - loss: 0.6375 - accuracy: 0.76 - ETA: 0s - loss: 0.6254 - accuracy: 0.78 - ETA: 0s - loss: 0.6072 - accuracy: 0.78 - ETA: 0s - loss: 0.5761 - accuracy: 0.78 - ETA: 0s - loss: 0.5733 - accuracy: 0.78 - ETA: 0s - loss: 0.5428 - accuracy: 0.78 - ETA: 0s - loss: 0.5878 - accuracy: 0.78 - ETA: 0s - loss: 0.6003 - accuracy: 0.78 - ETA: 0s - loss: 0.5922 - accuracy: 0.77 - 1s 15ms/step - loss: 0.5815 - accuracy: 0.7844 - val_loss: 0.7044 - val_accuracy: 0.7182\n",
      "Epoch 6/10\n",
      "90/90 [==============================] - ETA: 7s - loss: 2.2982e-05 - accuracy: 1.00 - ETA: 1s - loss: 0.5398 - accuracy: 0.8000   - ETA: 1s - loss: 0.4033 - accuracy: 0.80 - ETA: 0s - loss: 0.4238 - accuracy: 0.80 - ETA: 0s - loss: 0.4365 - accuracy: 0.81 - ETA: 0s - loss: 0.4808 - accuracy: 0.81 - ETA: 0s - loss: 0.4655 - accuracy: 0.82 - ETA: 0s - loss: 0.4735 - accuracy: 0.82 - ETA: 0s - loss: 0.4885 - accuracy: 0.82 - ETA: 0s - loss: 0.4616 - accuracy: 0.82 - ETA: 0s - loss: 0.4746 - accuracy: 0.83 - ETA: 0s - loss: 0.4641 - accuracy: 0.83 - ETA: 0s - loss: 0.4876 - accuracy: 0.81 - ETA: 0s - loss: 0.4720 - accuracy: 0.82 - ETA: 0s - loss: 0.4642 - accuracy: 0.82 - 1s 15ms/step - loss: 0.4698 - accuracy: 0.8200 - val_loss: 0.6332 - val_accuracy: 0.7545\n",
      "Epoch 7/10\n",
      "90/90 [==============================] - ETA: 7s - loss: 0.0696 - accuracy: 1.00 - ETA: 1s - loss: 0.4269 - accuracy: 0.80 - ETA: 1s - loss: 0.3449 - accuracy: 0.81 - ETA: 0s - loss: 0.3761 - accuracy: 0.78 - ETA: 0s - loss: 0.4380 - accuracy: 0.80 - ETA: 0s - loss: 0.4215 - accuracy: 0.82 - ETA: 0s - loss: 0.4404 - accuracy: 0.81 - ETA: 0s - loss: 0.4131 - accuracy: 0.82 - ETA: 0s - loss: 0.4364 - accuracy: 0.81 - ETA: 0s - loss: 0.4321 - accuracy: 0.80 - ETA: 0s - loss: 0.4477 - accuracy: 0.80 - ETA: 0s - loss: 0.4345 - accuracy: 0.81 - ETA: 0s - loss: 0.4393 - accuracy: 0.81 - ETA: 0s - loss: 0.4119 - accuracy: 0.83 - ETA: 0s - loss: 0.3947 - accuracy: 0.84 - 1s 15ms/step - loss: 0.3750 - accuracy: 0.8489 - val_loss: 0.6914 - val_accuracy: 0.7545\n",
      "Epoch 8/10\n",
      "90/90 [==============================] - ETA: 6s - loss: 0.5294 - accuracy: 0.80 - ETA: 1s - loss: 0.2508 - accuracy: 0.90 - ETA: 1s - loss: 0.3210 - accuracy: 0.86 - ETA: 0s - loss: 0.2548 - accuracy: 0.90 - ETA: 0s - loss: 0.2611 - accuracy: 0.87 - ETA: 0s - loss: 0.2300 - accuracy: 0.89 - ETA: 0s - loss: 0.2595 - accuracy: 0.86 - ETA: 0s - loss: 0.2790 - accuracy: 0.85 - ETA: 0s - loss: 0.3111 - accuracy: 0.85 - ETA: 0s - loss: 0.3382 - accuracy: 0.85 - ETA: 0s - loss: 0.3314 - accuracy: 0.85 - ETA: 0s - loss: 0.3666 - accuracy: 0.85 - ETA: 0s - loss: 0.3519 - accuracy: 0.86 - ETA: 0s - loss: 0.3502 - accuracy: 0.85 - ETA: 0s - loss: 0.3455 - accuracy: 0.85 - ETA: 0s - loss: 0.3593 - accuracy: 0.85 - 1s 15ms/step - loss: 0.3553 - accuracy: 0.8556 - val_loss: 0.6453 - val_accuracy: 0.7727\n",
      "Epoch 9/10\n",
      "90/90 [==============================] - ETA: 7s - loss: 0.0732 - accuracy: 1.00 - ETA: 1s - loss: 0.1053 - accuracy: 0.94 - ETA: 1s - loss: 0.2101 - accuracy: 0.93 - ETA: 0s - loss: 0.2137 - accuracy: 0.92 - ETA: 0s - loss: 0.2821 - accuracy: 0.90 - ETA: 0s - loss: 0.3166 - accuracy: 0.87 - ETA: 0s - loss: 0.3085 - accuracy: 0.87 - ETA: 0s - loss: 0.2870 - accuracy: 0.87 - ETA: 0s - loss: 0.2829 - accuracy: 0.87 - ETA: 0s - loss: 0.2600 - accuracy: 0.89 - ETA: 0s - loss: 0.2529 - accuracy: 0.88 - ETA: 0s - loss: 0.2667 - accuracy: 0.88 - ETA: 0s - loss: 0.2881 - accuracy: 0.88 - ETA: 0s - loss: 0.2988 - accuracy: 0.88 - ETA: 0s - loss: 0.3062 - accuracy: 0.87 - 1s 15ms/step - loss: 0.2999 - accuracy: 0.8756 - val_loss: 0.5925 - val_accuracy: 0.7727\n",
      "Epoch 10/10\n",
      "90/90 [==============================] - ETA: 6s - loss: 0.1222 - accuracy: 1.00 - ETA: 1s - loss: 0.3386 - accuracy: 0.93 - ETA: 1s - loss: 0.3021 - accuracy: 0.88 - ETA: 1s - loss: 0.2960 - accuracy: 0.88 - ETA: 0s - loss: 0.2837 - accuracy: 0.86 - ETA: 0s - loss: 0.2900 - accuracy: 0.86 - ETA: 0s - loss: 0.2516 - accuracy: 0.88 - ETA: 0s - loss: 0.2711 - accuracy: 0.86 - ETA: 0s - loss: 0.2564 - accuracy: 0.87 - ETA: 0s - loss: 0.2446 - accuracy: 0.87 - ETA: 0s - loss: 0.2353 - accuracy: 0.88 - ETA: 0s - loss: 0.2553 - accuracy: 0.86 - ETA: 0s - loss: 0.2462 - accuracy: 0.87 - ETA: 0s - loss: 0.2387 - accuracy: 0.87 - ETA: 0s - loss: 0.2497 - accuracy: 0.87 - ETA: 0s - loss: 0.2359 - accuracy: 0.87 - 1s 16ms/step - loss: 0.2561 - accuracy: 0.8778 - val_loss: 0.5030 - val_accuracy: 0.7818\n",
      "Epoch 1/20\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "90/90 [==============================] - ETA: 21:17 - loss: 0.0586 - accuracy: 1.000 - ETA: 4:05 - loss: 0.1637 - accuracy: 0.880 - ETA: 2:10 - loss: 0.1087 - accuracy: 0.91 - ETA: 1:26 - loss: 0.1226 - accuracy: 0.92 - ETA: 1:02 - loss: 0.2506 - accuracy: 0.89 - ETA: 48s - loss: 0.3108 - accuracy: 0.8762 - ETA: 38s - loss: 0.3579 - accuracy: 0.856 - ETA: 31s - loss: 0.3293 - accuracy: 0.869 - ETA: 25s - loss: 0.3031 - accuracy: 0.878 - ETA: 21s - loss: 0.2778 - accuracy: 0.886 - ETA: 17s - loss: 0.2814 - accuracy: 0.887 - ETA: 15s - loss: 0.3043 - accuracy: 0.884 - ETA: 12s - loss: 0.2904 - accuracy: 0.885 - ETA: 10s - loss: 0.2991 - accuracy: 0.883 - ETA: 8s - loss: 0.2787 - accuracy: 0.891 - ETA: 7s - loss: 0.2704 - accuracy: 0.89 - ETA: 5s - loss: 0.2679 - accuracy: 0.89 - ETA: 4s - loss: 0.3003 - accuracy: 0.89 - ETA: 3s - loss: 0.2911 - accuracy: 0.89 - ETA: 2s - loss: 0.2900 - accuracy: 0.89 - ETA: 1s - loss: 0.2920 - accuracy: 0.89 - ETA: 0s - loss: 0.3262 - accuracy: 0.89 - ETA: 0s - loss: 0.3444 - accuracy: 0.88 - 16s 183ms/step - loss: 0.3431 - accuracy: 0.8889 - val_loss: 0.2772 - val_accuracy: 0.9091\n",
      "Epoch 2/20\n",
      "90/90 [==============================] - ETA: 10s - loss: 0.0254 - accuracy: 1.000 - ETA: 3s - loss: 0.3446 - accuracy: 0.950 - ETA: 2s - loss: 0.1809 - accuracy: 0.97 - ETA: 1s - loss: 0.2254 - accuracy: 0.93 - ETA: 1s - loss: 0.1922 - accuracy: 0.93 - ETA: 1s - loss: 0.2045 - accuracy: 0.92 - ETA: 1s - loss: 0.2711 - accuracy: 0.91 - ETA: 1s - loss: 0.2628 - accuracy: 0.92 - ETA: 1s - loss: 0.2711 - accuracy: 0.91 - ETA: 0s - loss: 0.2411 - accuracy: 0.92 - ETA: 0s - loss: 0.2241 - accuracy: 0.93 - ETA: 0s - loss: 0.2159 - accuracy: 0.93 - ETA: 0s - loss: 0.2178 - accuracy: 0.92 - ETA: 0s - loss: 0.2445 - accuracy: 0.91 - ETA: 0s - loss: 0.2783 - accuracy: 0.91 - ETA: 0s - loss: 0.2783 - accuracy: 0.91 - ETA: 0s - loss: 0.2838 - accuracy: 0.90 - ETA: 0s - loss: 0.2783 - accuracy: 0.90 - ETA: 0s - loss: 0.2714 - accuracy: 0.90 - ETA: 0s - loss: 0.2619 - accuracy: 0.90 - ETA: 0s - loss: 0.2596 - accuracy: 0.90 - ETA: 0s - loss: 0.2587 - accuracy: 0.90 - ETA: 0s - loss: 0.2610 - accuracy: 0.89 - 2s 21ms/step - loss: 0.2616 - accuracy: 0.8978 - val_loss: 0.2781 - val_accuracy: 0.8909\n",
      "Epoch 3/20\n",
      "90/90 [==============================] - ETA: 6s - loss: 0.2759 - accuracy: 0.80 - ETA: 2s - loss: 0.3089 - accuracy: 0.85 - ETA: 1s - loss: 0.2124 - accuracy: 0.87 - ETA: 1s - loss: 0.2437 - accuracy: 0.86 - ETA: 1s - loss: 0.2256 - accuracy: 0.87 - ETA: 1s - loss: 0.1898 - accuracy: 0.90 - ETA: 1s - loss: 0.2196 - accuracy: 0.90 - ETA: 1s - loss: 0.2373 - accuracy: 0.89 - ETA: 0s - loss: 0.2393 - accuracy: 0.89 - ETA: 0s - loss: 0.2391 - accuracy: 0.89 - ETA: 0s - loss: 0.2596 - accuracy: 0.88 - ETA: 0s - loss: 0.2466 - accuracy: 0.89 - ETA: 0s - loss: 0.2305 - accuracy: 0.89 - ETA: 0s - loss: 0.2201 - accuracy: 0.90 - ETA: 0s - loss: 0.2130 - accuracy: 0.90 - ETA: 0s - loss: 0.2118 - accuracy: 0.90 - ETA: 0s - loss: 0.2109 - accuracy: 0.90 - ETA: 0s - loss: 0.2116 - accuracy: 0.90 - ETA: 0s - loss: 0.2257 - accuracy: 0.89 - ETA: 0s - loss: 0.2270 - accuracy: 0.89 - ETA: 0s - loss: 0.2295 - accuracy: 0.89 - ETA: 0s - loss: 0.2275 - accuracy: 0.89 - ETA: 0s - loss: 0.2214 - accuracy: 0.89 - 2s 21ms/step - loss: 0.2178 - accuracy: 0.9000 - val_loss: 0.2427 - val_accuracy: 0.9182\n",
      "Epoch 4/20\n",
      "90/90 [==============================] - ETA: 7s - loss: 0.2557 - accuracy: 1.00 - ETA: 2s - loss: 0.1527 - accuracy: 0.90 - ETA: 1s - loss: 0.1056 - accuracy: 0.92 - ETA: 1s - loss: 0.1181 - accuracy: 0.93 - ETA: 1s - loss: 0.1202 - accuracy: 0.93 - ETA: 1s - loss: 0.1315 - accuracy: 0.94 - ETA: 1s - loss: 0.1351 - accuracy: 0.93 - ETA: 1s - loss: 0.1236 - accuracy: 0.93 - ETA: 0s - loss: 0.1213 - accuracy: 0.93 - ETA: 0s - loss: 0.1131 - accuracy: 0.94 - ETA: 0s - loss: 0.1242 - accuracy: 0.94 - ETA: 0s - loss: 0.1317 - accuracy: 0.93 - ETA: 0s - loss: 0.1378 - accuracy: 0.92 - ETA: 0s - loss: 0.1343 - accuracy: 0.92 - ETA: 0s - loss: 0.1311 - accuracy: 0.92 - ETA: 0s - loss: 0.1431 - accuracy: 0.92 - ETA: 0s - loss: 0.1473 - accuracy: 0.91 - ETA: 0s - loss: 0.1402 - accuracy: 0.92 - ETA: 0s - loss: 0.1383 - accuracy: 0.92 - ETA: 0s - loss: 0.1333 - accuracy: 0.92 - ETA: 0s - loss: 0.1333 - accuracy: 0.92 - ETA: 0s - loss: 0.1275 - accuracy: 0.92 - ETA: 0s - loss: 0.1252 - accuracy: 0.93 - 2s 20ms/step - loss: 0.1275 - accuracy: 0.9289 - val_loss: 0.2628 - val_accuracy: 0.9182\n",
      "Epoch 5/20\n",
      "90/90 [==============================] - ETA: 7s - loss: 0.0155 - accuracy: 1.00 - ETA: 2s - loss: 0.0482 - accuracy: 1.00 - ETA: 1s - loss: 0.0408 - accuracy: 1.00 - ETA: 1s - loss: 0.0769 - accuracy: 0.98 - ETA: 1s - loss: 0.1035 - accuracy: 0.97 - ETA: 1s - loss: 0.0950 - accuracy: 0.97 - ETA: 1s - loss: 0.1153 - accuracy: 0.96 - ETA: 1s - loss: 0.1172 - accuracy: 0.95 - ETA: 0s - loss: 0.1267 - accuracy: 0.95 - ETA: 0s - loss: 0.1369 - accuracy: 0.94 - ETA: 0s - loss: 0.1381 - accuracy: 0.94 - ETA: 0s - loss: 0.1304 - accuracy: 0.94 - ETA: 0s - loss: 0.1256 - accuracy: 0.95 - ETA: 0s - loss: 0.1314 - accuracy: 0.93 - ETA: 0s - loss: 0.1245 - accuracy: 0.94 - ETA: 0s - loss: 0.1423 - accuracy: 0.94 - ETA: 0s - loss: 0.1482 - accuracy: 0.93 - ETA: 0s - loss: 0.1457 - accuracy: 0.94 - ETA: 0s - loss: 0.1423 - accuracy: 0.94 - ETA: 0s - loss: 0.1464 - accuracy: 0.94 - ETA: 0s - loss: 0.1457 - accuracy: 0.93 - ETA: 0s - loss: 0.1447 - accuracy: 0.93 - ETA: 0s - loss: 0.1476 - accuracy: 0.93 - 2s 20ms/step - loss: 0.1467 - accuracy: 0.9400 - val_loss: 0.2723 - val_accuracy: 0.8909\n",
      "Epoch 6/20\n",
      "90/90 [==============================] - ETA: 6s - loss: 0.0617 - accuracy: 1.00 - ETA: 2s - loss: 0.0403 - accuracy: 1.00 - ETA: 1s - loss: 0.0940 - accuracy: 0.97 - ETA: 1s - loss: 0.0786 - accuracy: 0.98 - ETA: 1s - loss: 0.0900 - accuracy: 0.97 - ETA: 1s - loss: 0.1598 - accuracy: 0.95 - ETA: 1s - loss: 0.1418 - accuracy: 0.95 - ETA: 1s - loss: 0.1330 - accuracy: 0.95 - ETA: 0s - loss: 0.1212 - accuracy: 0.96 - ETA: 0s - loss: 0.1256 - accuracy: 0.95 - ETA: 0s - loss: 0.1204 - accuracy: 0.95 - ETA: 0s - loss: 0.1428 - accuracy: 0.94 - ETA: 0s - loss: 0.1355 - accuracy: 0.95 - ETA: 0s - loss: 0.1328 - accuracy: 0.95 - ETA: 0s - loss: 0.1303 - accuracy: 0.95 - ETA: 0s - loss: 0.1317 - accuracy: 0.94 - ETA: 0s - loss: 0.1314 - accuracy: 0.94 - ETA: 0s - loss: 0.1404 - accuracy: 0.94 - ETA: 0s - loss: 0.1422 - accuracy: 0.94 - ETA: 0s - loss: 0.1379 - accuracy: 0.95 - ETA: 0s - loss: 0.1360 - accuracy: 0.95 - ETA: 0s - loss: 0.1370 - accuracy: 0.94 - ETA: 0s - loss: 0.1336 - accuracy: 0.95 - 2s 20ms/step - loss: 0.1341 - accuracy: 0.9489 - val_loss: 0.2328 - val_accuracy: 0.9000\n",
      "Epoch 7/20\n",
      "90/90 [==============================] - ETA: 6s - loss: 0.0228 - accuracy: 1.00 - ETA: 2s - loss: 0.0204 - accuracy: 1.00 - ETA: 1s - loss: 0.0358 - accuracy: 1.00 - ETA: 1s - loss: 0.0452 - accuracy: 1.00 - ETA: 1s - loss: 0.0463 - accuracy: 1.00 - ETA: 1s - loss: 0.0781 - accuracy: 0.98 - ETA: 1s - loss: 0.0854 - accuracy: 0.96 - ETA: 1s - loss: 0.0982 - accuracy: 0.97 - ETA: 0s - loss: 0.0948 - accuracy: 0.96 - ETA: 0s - loss: 0.0907 - accuracy: 0.97 - ETA: 0s - loss: 0.0907 - accuracy: 0.97 - ETA: 0s - loss: 0.0856 - accuracy: 0.97 - ETA: 0s - loss: 0.0871 - accuracy: 0.97 - ETA: 0s - loss: 0.0931 - accuracy: 0.96 - ETA: 0s - loss: 0.0993 - accuracy: 0.96 - ETA: 0s - loss: 0.0980 - accuracy: 0.97 - ETA: 0s - loss: 0.1047 - accuracy: 0.96 - ETA: 0s - loss: 0.1013 - accuracy: 0.96 - ETA: 0s - loss: 0.0975 - accuracy: 0.96 - ETA: 0s - loss: 0.0981 - accuracy: 0.96 - ETA: 0s - loss: 0.0957 - accuracy: 0.97 - ETA: 0s - loss: 0.0920 - accuracy: 0.97 - ETA: 0s - loss: 0.0921 - accuracy: 0.97 - 2s 20ms/step - loss: 0.0909 - accuracy: 0.9733 - val_loss: 0.2228 - val_accuracy: 0.9182\n",
      "Epoch 8/20\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "90/90 [==============================] - ETA: 7s - loss: 0.0029 - accuracy: 1.00 - ETA: 2s - loss: 0.0473 - accuracy: 1.00 - ETA: 1s - loss: 0.0419 - accuracy: 1.00 - ETA: 1s - loss: 0.0505 - accuracy: 0.98 - ETA: 1s - loss: 0.0547 - accuracy: 0.97 - ETA: 1s - loss: 0.0547 - accuracy: 0.98 - ETA: 1s - loss: 0.0497 - accuracy: 0.98 - ETA: 1s - loss: 0.0516 - accuracy: 0.98 - ETA: 0s - loss: 0.0509 - accuracy: 0.98 - ETA: 0s - loss: 0.0515 - accuracy: 0.98 - ETA: 0s - loss: 0.0482 - accuracy: 0.98 - ETA: 0s - loss: 0.0472 - accuracy: 0.98 - ETA: 0s - loss: 0.0473 - accuracy: 0.98 - ETA: 0s - loss: 0.0528 - accuracy: 0.98 - ETA: 0s - loss: 0.0510 - accuracy: 0.98 - ETA: 0s - loss: 0.0501 - accuracy: 0.98 - ETA: 0s - loss: 0.0531 - accuracy: 0.98 - ETA: 0s - loss: 0.0748 - accuracy: 0.97 - ETA: 0s - loss: 0.0763 - accuracy: 0.98 - ETA: 0s - loss: 0.0792 - accuracy: 0.97 - ETA: 0s - loss: 0.0839 - accuracy: 0.97 - ETA: 0s - loss: 0.0802 - accuracy: 0.97 - ETA: 0s - loss: 0.0782 - accuracy: 0.97 - 2s 20ms/step - loss: 0.0780 - accuracy: 0.9756 - val_loss: 0.2748 - val_accuracy: 0.9273\n",
      "Epoch 9/20\n",
      "90/90 [==============================] - ETA: 6s - loss: 0.0787 - accuracy: 1.00 - ETA: 2s - loss: 0.0659 - accuracy: 0.95 - ETA: 1s - loss: 0.0330 - accuracy: 0.97 - ETA: 1s - loss: 0.0644 - accuracy: 0.96 - ETA: 1s - loss: 0.0560 - accuracy: 0.97 - ETA: 1s - loss: 0.0562 - accuracy: 0.98 - ETA: 1s - loss: 0.0932 - accuracy: 0.95 - ETA: 1s - loss: 0.0946 - accuracy: 0.95 - ETA: 0s - loss: 0.0842 - accuracy: 0.96 - ETA: 0s - loss: 0.0795 - accuracy: 0.96 - ETA: 0s - loss: 0.0975 - accuracy: 0.95 - ETA: 0s - loss: 0.0947 - accuracy: 0.95 - ETA: 0s - loss: 0.0974 - accuracy: 0.95 - ETA: 0s - loss: 0.0916 - accuracy: 0.96 - ETA: 0s - loss: 0.0895 - accuracy: 0.96 - ETA: 0s - loss: 0.0980 - accuracy: 0.96 - ETA: 0s - loss: 0.1052 - accuracy: 0.95 - ETA: 0s - loss: 0.1089 - accuracy: 0.95 - ETA: 0s - loss: 0.1115 - accuracy: 0.95 - ETA: 0s - loss: 0.1128 - accuracy: 0.95 - ETA: 0s - loss: 0.1081 - accuracy: 0.95 - ETA: 0s - loss: 0.1100 - accuracy: 0.95 - ETA: 0s - loss: 0.1081 - accuracy: 0.95 - 2s 20ms/step - loss: 0.1061 - accuracy: 0.9556 - val_loss: 0.3252 - val_accuracy: 0.9273\n",
      "Epoch 10/20\n",
      "90/90 [==============================] - ETA: 7s - loss: 0.3732 - accuracy: 0.80 - ETA: 2s - loss: 0.1459 - accuracy: 0.92 - ETA: 1s - loss: 0.0893 - accuracy: 0.95 - ETA: 1s - loss: 0.1602 - accuracy: 0.92 - ETA: 1s - loss: 0.1261 - accuracy: 0.94 - ETA: 1s - loss: 0.1199 - accuracy: 0.94 - ETA: 1s - loss: 0.1133 - accuracy: 0.94 - ETA: 0s - loss: 0.1004 - accuracy: 0.95 - ETA: 0s - loss: 0.0994 - accuracy: 0.95 - ETA: 0s - loss: 0.1037 - accuracy: 0.95 - ETA: 0s - loss: 0.0998 - accuracy: 0.95 - ETA: 0s - loss: 0.0960 - accuracy: 0.95 - ETA: 0s - loss: 0.0912 - accuracy: 0.96 - ETA: 0s - loss: 0.0880 - accuracy: 0.96 - ETA: 0s - loss: 0.0910 - accuracy: 0.95 - ETA: 0s - loss: 0.1033 - accuracy: 0.95 - ETA: 0s - loss: 0.1163 - accuracy: 0.94 - ETA: 0s - loss: 0.1140 - accuracy: 0.94 - ETA: 0s - loss: 0.1226 - accuracy: 0.94 - ETA: 0s - loss: 0.1173 - accuracy: 0.94 - ETA: 0s - loss: 0.1150 - accuracy: 0.94 - ETA: 0s - loss: 0.1110 - accuracy: 0.95 - 2s 20ms/step - loss: 0.1092 - accuracy: 0.9533 - val_loss: 0.3402 - val_accuracy: 0.9091\n",
      "Epoch 11/20\n",
      "90/90 [==============================] - ETA: 7s - loss: 0.1762 - accuracy: 1.00 - ETA: 2s - loss: 0.0837 - accuracy: 0.95 - ETA: 2s - loss: 0.0701 - accuracy: 0.95 - ETA: 1s - loss: 0.0904 - accuracy: 0.93 - ETA: 1s - loss: 0.0874 - accuracy: 0.93 - ETA: 1s - loss: 0.0828 - accuracy: 0.95 - ETA: 1s - loss: 0.0952 - accuracy: 0.95 - ETA: 1s - loss: 0.1048 - accuracy: 0.94 - ETA: 0s - loss: 0.0955 - accuracy: 0.95 - ETA: 0s - loss: 0.0935 - accuracy: 0.95 - ETA: 0s - loss: 0.0848 - accuracy: 0.96 - ETA: 0s - loss: 0.0864 - accuracy: 0.95 - ETA: 0s - loss: 0.0933 - accuracy: 0.95 - ETA: 0s - loss: 0.0864 - accuracy: 0.95 - ETA: 0s - loss: 0.0929 - accuracy: 0.95 - ETA: 0s - loss: 0.0999 - accuracy: 0.95 - ETA: 0s - loss: 0.0985 - accuracy: 0.95 - ETA: 0s - loss: 0.0934 - accuracy: 0.95 - ETA: 0s - loss: 0.0909 - accuracy: 0.95 - ETA: 0s - loss: 0.0878 - accuracy: 0.95 - ETA: 0s - loss: 0.0876 - accuracy: 0.95 - ETA: 0s - loss: 0.0934 - accuracy: 0.95 - ETA: 0s - loss: 0.0909 - accuracy: 0.95 - 2s 21ms/step - loss: 0.0931 - accuracy: 0.9533 - val_loss: 0.2306 - val_accuracy: 0.9273\n",
      "Epoch 12/20\n",
      "90/90 [==============================] - ETA: 7s - loss: 0.1478 - accuracy: 0.80 - ETA: 2s - loss: 0.0836 - accuracy: 0.95 - ETA: 1s - loss: 0.0790 - accuracy: 0.95 - ETA: 1s - loss: 0.1231 - accuracy: 0.91 - ETA: 1s - loss: 0.1144 - accuracy: 0.92 - ETA: 1s - loss: 0.1471 - accuracy: 0.91 - ETA: 1s - loss: 0.1446 - accuracy: 0.91 - ETA: 1s - loss: 0.1449 - accuracy: 0.92 - ETA: 0s - loss: 0.1326 - accuracy: 0.92 - ETA: 0s - loss: 0.1194 - accuracy: 0.93 - ETA: 0s - loss: 0.1103 - accuracy: 0.94 - ETA: 0s - loss: 0.1049 - accuracy: 0.94 - ETA: 0s - loss: 0.0996 - accuracy: 0.95 - ETA: 0s - loss: 0.0920 - accuracy: 0.95 - ETA: 0s - loss: 0.0889 - accuracy: 0.95 - ETA: 0s - loss: 0.0985 - accuracy: 0.95 - ETA: 0s - loss: 0.0986 - accuracy: 0.95 - ETA: 0s - loss: 0.0935 - accuracy: 0.95 - ETA: 0s - loss: 0.0987 - accuracy: 0.95 - ETA: 0s - loss: 0.1102 - accuracy: 0.95 - ETA: 0s - loss: 0.1065 - accuracy: 0.95 - ETA: 0s - loss: 0.1026 - accuracy: 0.95 - ETA: 0s - loss: 0.0994 - accuracy: 0.96 - 2s 20ms/step - loss: 0.1023 - accuracy: 0.9578 - val_loss: 0.4685 - val_accuracy: 0.8818\n",
      "Epoch 13/20\n",
      "90/90 [==============================] - ETA: 6s - loss: 0.1870 - accuracy: 0.80 - ETA: 2s - loss: 0.1633 - accuracy: 0.85 - ETA: 1s - loss: 0.1034 - accuracy: 0.92 - ETA: 1s - loss: 0.1098 - accuracy: 0.95 - ETA: 1s - loss: 0.1136 - accuracy: 0.95 - ETA: 1s - loss: 0.1036 - accuracy: 0.96 - ETA: 1s - loss: 0.0876 - accuracy: 0.96 - ETA: 1s - loss: 0.0811 - accuracy: 0.96 - ETA: 0s - loss: 0.0765 - accuracy: 0.96 - ETA: 0s - loss: 0.0703 - accuracy: 0.96 - ETA: 0s - loss: 0.1188 - accuracy: 0.96 - ETA: 0s - loss: 0.1102 - accuracy: 0.96 - ETA: 0s - loss: 0.1074 - accuracy: 0.96 - ETA: 0s - loss: 0.1078 - accuracy: 0.96 - ETA: 0s - loss: 0.1077 - accuracy: 0.96 - ETA: 0s - loss: 0.1007 - accuracy: 0.96 - ETA: 0s - loss: 0.1073 - accuracy: 0.96 - ETA: 0s - loss: 0.1118 - accuracy: 0.95 - ETA: 0s - loss: 0.1129 - accuracy: 0.95 - ETA: 0s - loss: 0.1239 - accuracy: 0.95 - ETA: 0s - loss: 0.1188 - accuracy: 0.95 - ETA: 0s - loss: 0.1168 - accuracy: 0.95 - ETA: 0s - loss: 0.1133 - accuracy: 0.95 - 2s 20ms/step - loss: 0.1108 - accuracy: 0.9533 - val_loss: 0.2198 - val_accuracy: 0.9091\n",
      "Epoch 14/20\n",
      "90/90 [==============================] - ETA: 6s - loss: 0.0487 - accuracy: 1.00 - ETA: 2s - loss: 0.0814 - accuracy: 1.00 - ETA: 1s - loss: 0.1013 - accuracy: 0.97 - ETA: 1s - loss: 0.1711 - accuracy: 0.96 - ETA: 1s - loss: 0.1382 - accuracy: 0.97 - ETA: 1s - loss: 0.1215 - accuracy: 0.97 - ETA: 1s - loss: 0.1101 - accuracy: 0.97 - ETA: 1s - loss: 0.0989 - accuracy: 0.97 - ETA: 0s - loss: 0.0928 - accuracy: 0.97 - ETA: 0s - loss: 0.0936 - accuracy: 0.97 - ETA: 0s - loss: 0.0924 - accuracy: 0.97 - ETA: 0s - loss: 0.0880 - accuracy: 0.97 - ETA: 0s - loss: 0.0861 - accuracy: 0.97 - ETA: 0s - loss: 0.0861 - accuracy: 0.96 - ETA: 0s - loss: 0.0809 - accuracy: 0.97 - ETA: 0s - loss: 0.0838 - accuracy: 0.97 - ETA: 0s - loss: 0.0827 - accuracy: 0.96 - ETA: 0s - loss: 0.0826 - accuracy: 0.96 - ETA: 0s - loss: 0.0790 - accuracy: 0.96 - ETA: 0s - loss: 0.0794 - accuracy: 0.96 - ETA: 0s - loss: 0.0782 - accuracy: 0.96 - ETA: 0s - loss: 0.0798 - accuracy: 0.96 - ETA: 0s - loss: 0.0777 - accuracy: 0.96 - 2s 20ms/step - loss: 0.0761 - accuracy: 0.9667 - val_loss: 0.1519 - val_accuracy: 0.9455\n",
      "Epoch 15/20\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "90/90 [==============================] - ETA: 7s - loss: 0.0221 - accuracy: 1.00 - ETA: 2s - loss: 0.0696 - accuracy: 0.96 - ETA: 1s - loss: 0.0499 - accuracy: 0.97 - ETA: 1s - loss: 0.0502 - accuracy: 0.96 - ETA: 1s - loss: 0.0610 - accuracy: 0.96 - ETA: 1s - loss: 0.0519 - accuracy: 0.97 - ETA: 1s - loss: 0.0444 - accuracy: 0.97 - ETA: 1s - loss: 0.0587 - accuracy: 0.96 - ETA: 0s - loss: 0.0796 - accuracy: 0.95 - ETA: 0s - loss: 0.0765 - accuracy: 0.96 - ETA: 0s - loss: 0.0778 - accuracy: 0.96 - ETA: 0s - loss: 0.0794 - accuracy: 0.95 - ETA: 0s - loss: 0.0753 - accuracy: 0.95 - ETA: 0s - loss: 0.0705 - accuracy: 0.96 - ETA: 0s - loss: 0.0978 - accuracy: 0.95 - ETA: 0s - loss: 0.0952 - accuracy: 0.95 - ETA: 0s - loss: 0.1001 - accuracy: 0.95 - ETA: 0s - loss: 0.0992 - accuracy: 0.95 - ETA: 0s - loss: 0.0979 - accuracy: 0.95 - ETA: 0s - loss: 0.0994 - accuracy: 0.95 - ETA: 0s - loss: 0.0991 - accuracy: 0.95 - ETA: 0s - loss: 0.1000 - accuracy: 0.95 - ETA: 0s - loss: 0.0955 - accuracy: 0.95 - 2s 20ms/step - loss: 0.0945 - accuracy: 0.9533 - val_loss: 0.1765 - val_accuracy: 0.9364\n",
      "Epoch 16/20\n",
      "90/90 [==============================] - ETA: 6s - loss: 0.1630 - accuracy: 1.00 - ETA: 2s - loss: 0.0450 - accuracy: 1.00 - ETA: 1s - loss: 0.0693 - accuracy: 0.97 - ETA: 1s - loss: 0.0772 - accuracy: 0.96 - ETA: 1s - loss: 0.0652 - accuracy: 0.97 - ETA: 1s - loss: 0.0643 - accuracy: 0.98 - ETA: 1s - loss: 0.0664 - accuracy: 0.97 - ETA: 1s - loss: 0.0658 - accuracy: 0.97 - ETA: 0s - loss: 0.0586 - accuracy: 0.98 - ETA: 0s - loss: 0.0552 - accuracy: 0.98 - ETA: 0s - loss: 0.0508 - accuracy: 0.98 - ETA: 0s - loss: 0.0506 - accuracy: 0.98 - ETA: 0s - loss: 0.0491 - accuracy: 0.98 - ETA: 0s - loss: 0.0502 - accuracy: 0.98 - ETA: 0s - loss: 0.0497 - accuracy: 0.98 - ETA: 0s - loss: 0.0522 - accuracy: 0.98 - ETA: 0s - loss: 0.0505 - accuracy: 0.98 - ETA: 0s - loss: 0.0479 - accuracy: 0.98 - ETA: 0s - loss: 0.0658 - accuracy: 0.98 - ETA: 0s - loss: 0.0661 - accuracy: 0.97 - ETA: 0s - loss: 0.0688 - accuracy: 0.97 - ETA: 0s - loss: 0.0682 - accuracy: 0.97 - ETA: 0s - loss: 0.0690 - accuracy: 0.97 - 2s 20ms/step - loss: 0.0709 - accuracy: 0.9733 - val_loss: 0.1163 - val_accuracy: 0.9545\n",
      "Epoch 17/20\n",
      "90/90 [==============================] - ETA: 7s - loss: 0.0069 - accuracy: 1.00 - ETA: 2s - loss: 0.0110 - accuracy: 1.00 - ETA: 1s - loss: 0.0406 - accuracy: 0.97 - ETA: 1s - loss: 0.0709 - accuracy: 0.95 - ETA: 1s - loss: 0.0769 - accuracy: 0.96 - ETA: 1s - loss: 0.0672 - accuracy: 0.97 - ETA: 1s - loss: 0.0566 - accuracy: 0.97 - ETA: 1s - loss: 0.0661 - accuracy: 0.97 - ETA: 0s - loss: 0.0669 - accuracy: 0.96 - ETA: 0s - loss: 0.0649 - accuracy: 0.97 - ETA: 0s - loss: 0.0601 - accuracy: 0.97 - ETA: 0s - loss: 0.0773 - accuracy: 0.97 - ETA: 0s - loss: 0.0740 - accuracy: 0.97 - ETA: 0s - loss: 0.0754 - accuracy: 0.97 - ETA: 0s - loss: 0.0704 - accuracy: 0.97 - ETA: 0s - loss: 0.0725 - accuracy: 0.97 - ETA: 0s - loss: 0.0720 - accuracy: 0.97 - ETA: 0s - loss: 0.0682 - accuracy: 0.97 - ETA: 0s - loss: 0.0685 - accuracy: 0.97 - ETA: 0s - loss: 0.0659 - accuracy: 0.97 - ETA: 0s - loss: 0.0633 - accuracy: 0.97 - ETA: 0s - loss: 0.0635 - accuracy: 0.97 - ETA: 0s - loss: 0.0631 - accuracy: 0.97 - 2s 20ms/step - loss: 0.0625 - accuracy: 0.9778 - val_loss: 0.1639 - val_accuracy: 0.9455\n",
      "Epoch 18/20\n",
      "90/90 [==============================] - ETA: 6s - loss: 0.5476 - accuracy: 0.80 - ETA: 2s - loss: 0.1496 - accuracy: 0.95 - ETA: 1s - loss: 0.0817 - accuracy: 0.97 - ETA: 1s - loss: 0.0600 - accuracy: 0.98 - ETA: 1s - loss: 0.0452 - accuracy: 0.98 - ETA: 1s - loss: 0.0426 - accuracy: 0.99 - ETA: 1s - loss: 0.0466 - accuracy: 0.98 - ETA: 1s - loss: 0.0498 - accuracy: 0.97 - ETA: 0s - loss: 0.0554 - accuracy: 0.98 - ETA: 0s - loss: 0.0629 - accuracy: 0.97 - ETA: 0s - loss: 0.0601 - accuracy: 0.97 - ETA: 0s - loss: 0.0608 - accuracy: 0.97 - ETA: 0s - loss: 0.0600 - accuracy: 0.97 - ETA: 0s - loss: 0.0635 - accuracy: 0.96 - ETA: 0s - loss: 0.0638 - accuracy: 0.96 - ETA: 0s - loss: 0.0638 - accuracy: 0.96 - ETA: 0s - loss: 0.0636 - accuracy: 0.96 - ETA: 0s - loss: 0.0603 - accuracy: 0.96 - ETA: 0s - loss: 0.0643 - accuracy: 0.96 - ETA: 0s - loss: 0.0672 - accuracy: 0.96 - ETA: 0s - loss: 0.0719 - accuracy: 0.96 - ETA: 0s - loss: 0.0741 - accuracy: 0.96 - ETA: 0s - loss: 0.0757 - accuracy: 0.96 - 2s 20ms/step - loss: 0.0765 - accuracy: 0.9622 - val_loss: 0.1257 - val_accuracy: 0.9545\n",
      "Epoch 19/20\n",
      "90/90 [==============================] - ETA: 7s - loss: 3.6955e-06 - accuracy: 1.00 - ETA: 2s - loss: 0.0144 - accuracy: 1.0000   - ETA: 2s - loss: 0.0219 - accuracy: 1.00 - ETA: 1s - loss: 0.0213 - accuracy: 1.00 - ETA: 1s - loss: 0.0180 - accuracy: 1.00 - ETA: 1s - loss: 0.0288 - accuracy: 0.99 - ETA: 1s - loss: 0.0506 - accuracy: 0.98 - ETA: 1s - loss: 0.0675 - accuracy: 0.97 - ETA: 0s - loss: 0.0734 - accuracy: 0.97 - ETA: 0s - loss: 0.0798 - accuracy: 0.97 - ETA: 0s - loss: 0.0845 - accuracy: 0.97 - ETA: 0s - loss: 0.0827 - accuracy: 0.97 - ETA: 0s - loss: 0.0763 - accuracy: 0.97 - ETA: 0s - loss: 0.0782 - accuracy: 0.97 - ETA: 0s - loss: 0.0761 - accuracy: 0.97 - ETA: 0s - loss: 0.0727 - accuracy: 0.97 - ETA: 0s - loss: 0.0709 - accuracy: 0.97 - ETA: 0s - loss: 0.0697 - accuracy: 0.97 - ETA: 0s - loss: 0.0758 - accuracy: 0.97 - ETA: 0s - loss: 0.0736 - accuracy: 0.97 - ETA: 0s - loss: 0.0707 - accuracy: 0.97 - ETA: 0s - loss: 0.0690 - accuracy: 0.97 - ETA: 0s - loss: 0.0665 - accuracy: 0.97 - 2s 20ms/step - loss: 0.0667 - accuracy: 0.9778 - val_loss: 0.1699 - val_accuracy: 0.9364\n",
      "Epoch 20/20\n",
      "90/90 [==============================] - ETA: 6s - loss: 0.6483 - accuracy: 0.80 - ETA: 2s - loss: 0.1647 - accuracy: 0.95 - ETA: 1s - loss: 0.1015 - accuracy: 0.97 - ETA: 1s - loss: 0.0962 - accuracy: 0.96 - ETA: 1s - loss: 0.0729 - accuracy: 0.97 - ETA: 1s - loss: 0.0585 - accuracy: 0.98 - ETA: 1s - loss: 0.0581 - accuracy: 0.98 - ETA: 1s - loss: 0.0657 - accuracy: 0.97 - ETA: 0s - loss: 0.0662 - accuracy: 0.97 - ETA: 0s - loss: 0.0598 - accuracy: 0.97 - ETA: 0s - loss: 0.0607 - accuracy: 0.97 - ETA: 0s - loss: 0.0575 - accuracy: 0.97 - ETA: 0s - loss: 0.0540 - accuracy: 0.97 - ETA: 0s - loss: 0.0546 - accuracy: 0.98 - ETA: 0s - loss: 0.0512 - accuracy: 0.98 - ETA: 0s - loss: 0.0479 - accuracy: 0.98 - ETA: 0s - loss: 0.0489 - accuracy: 0.98 - ETA: 0s - loss: 0.0493 - accuracy: 0.98 - ETA: 0s - loss: 0.0529 - accuracy: 0.98 - ETA: 0s - loss: 0.0545 - accuracy: 0.98 - ETA: 0s - loss: 0.0546 - accuracy: 0.98 - ETA: 0s - loss: 0.0593 - accuracy: 0.97 - ETA: 0s - loss: 0.0584 - accuracy: 0.97 - 2s 20ms/step - loss: 0.0571 - accuracy: 0.9800 - val_loss: 0.1821 - val_accuracy: 0.9455\n",
      "Epoch 1/10\n",
      "95/95 [==============================] - ETA: 18:11 - loss: 4.5330 - accuracy: 0.400 - ETA: 3:30 - loss: 4.6504 - accuracy: 0.480 - ETA: 1:29 - loss: 2.8655 - accuracy: 0.63 - ETA: 54s - loss: 2.4095 - accuracy: 0.6471 - ETA: 35s - loss: 2.1489 - accuracy: 0.666 - ETA: 27s - loss: 2.0356 - accuracy: 0.689 - ETA: 21s - loss: 1.9111 - accuracy: 0.682 - ETA: 17s - loss: 1.9072 - accuracy: 0.671 - ETA: 13s - loss: 1.8633 - accuracy: 0.662 - ETA: 10s - loss: 1.7342 - accuracy: 0.680 - ETA: 8s - loss: 1.6423 - accuracy: 0.689 - ETA: 6s - loss: 1.5478 - accuracy: 0.70 - ETA: 5s - loss: 1.5753 - accuracy: 0.68 - ETA: 3s - loss: 1.5030 - accuracy: 0.69 - ETA: 2s - loss: 1.4326 - accuracy: 0.70 - ETA: 1s - loss: 1.4129 - accuracy: 0.70 - ETA: 0s - loss: 1.3729 - accuracy: 0.71 - ETA: 0s - loss: 1.3525 - accuracy: 0.71 - 14s 146ms/step - loss: 1.3419 - accuracy: 0.7179 - val_loss: 0.8569 - val_accuracy: 0.7478\n",
      "Epoch 2/10\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "95/95 [==============================] - ETA: 8s - loss: 0.4008 - accuracy: 0.80 - ETA: 2s - loss: 1.1713 - accuracy: 0.80 - ETA: 1s - loss: 0.8967 - accuracy: 0.80 - ETA: 1s - loss: 0.8104 - accuracy: 0.76 - ETA: 1s - loss: 0.9632 - accuracy: 0.75 - ETA: 0s - loss: 0.9368 - accuracy: 0.75 - ETA: 0s - loss: 0.8066 - accuracy: 0.78 - ETA: 0s - loss: 0.7305 - accuracy: 0.79 - ETA: 0s - loss: 0.7490 - accuracy: 0.77 - ETA: 0s - loss: 0.7250 - accuracy: 0.77 - ETA: 0s - loss: 0.7529 - accuracy: 0.77 - ETA: 0s - loss: 0.7315 - accuracy: 0.77 - ETA: 0s - loss: 0.7447 - accuracy: 0.76 - ETA: 0s - loss: 0.7250 - accuracy: 0.76 - ETA: 0s - loss: 0.7237 - accuracy: 0.75 - ETA: 0s - loss: 0.7197 - accuracy: 0.75 - ETA: 0s - loss: 0.7027 - accuracy: 0.76 - ETA: 0s - loss: 0.6692 - accuracy: 0.77 - ETA: 0s - loss: 0.6523 - accuracy: 0.77 - 2s 16ms/step - loss: 0.6501 - accuracy: 0.7684 - val_loss: 0.6119 - val_accuracy: 0.8087\n",
      "Epoch 3/10\n",
      "95/95 [==============================] - ETA: 7s - loss: 0.1454 - accuracy: 0.80 - ETA: 1s - loss: 0.2835 - accuracy: 0.88 - ETA: 1s - loss: 0.3914 - accuracy: 0.86 - ETA: 0s - loss: 0.4669 - accuracy: 0.84 - ETA: 0s - loss: 0.5485 - accuracy: 0.82 - ETA: 0s - loss: 0.5227 - accuracy: 0.81 - ETA: 0s - loss: 0.5590 - accuracy: 0.82 - ETA: 0s - loss: 0.5525 - accuracy: 0.81 - ETA: 0s - loss: 0.5733 - accuracy: 0.82 - ETA: 0s - loss: 0.5626 - accuracy: 0.81 - ETA: 0s - loss: 0.5772 - accuracy: 0.80 - ETA: 0s - loss: 0.6071 - accuracy: 0.80 - ETA: 0s - loss: 0.5823 - accuracy: 0.81 - ETA: 0s - loss: 0.5471 - accuracy: 0.82 - ETA: 0s - loss: 0.5309 - accuracy: 0.82 - ETA: 0s - loss: 0.5235 - accuracy: 0.83 - 1s 15ms/step - loss: 0.5258 - accuracy: 0.8316 - val_loss: 0.4824 - val_accuracy: 0.8261\n",
      "Epoch 4/10\n",
      "95/95 [==============================] - ETA: 7s - loss: 0.3351 - accuracy: 0.80 - ETA: 1s - loss: 0.3734 - accuracy: 0.82 - ETA: 1s - loss: 0.3511 - accuracy: 0.84 - ETA: 1s - loss: 0.3476 - accuracy: 0.85 - ETA: 0s - loss: 0.3378 - accuracy: 0.84 - ETA: 0s - loss: 0.3164 - accuracy: 0.85 - ETA: 0s - loss: 0.3184 - accuracy: 0.86 - ETA: 0s - loss: 0.3367 - accuracy: 0.86 - ETA: 0s - loss: 0.3179 - accuracy: 0.87 - ETA: 0s - loss: 0.3097 - accuracy: 0.88 - ETA: 0s - loss: 0.3160 - accuracy: 0.88 - ETA: 0s - loss: 0.3108 - accuracy: 0.88 - ETA: 0s - loss: 0.3189 - accuracy: 0.88 - ETA: 0s - loss: 0.3173 - accuracy: 0.88 - ETA: 0s - loss: 0.3046 - accuracy: 0.88 - ETA: 0s - loss: 0.2947 - accuracy: 0.89 - 1s 15ms/step - loss: 0.2976 - accuracy: 0.8884 - val_loss: 0.3943 - val_accuracy: 0.8522\n",
      "Epoch 5/10\n",
      "95/95 [==============================] - ETA: 7s - loss: 0.8029 - accuracy: 0.80 - ETA: 1s - loss: 0.5107 - accuracy: 0.88 - ETA: 1s - loss: 0.4677 - accuracy: 0.86 - ETA: 1s - loss: 0.3793 - accuracy: 0.86 - ETA: 0s - loss: 0.3360 - accuracy: 0.86 - ETA: 0s - loss: 0.3260 - accuracy: 0.87 - ETA: 0s - loss: 0.2922 - accuracy: 0.88 - ETA: 0s - loss: 0.2662 - accuracy: 0.89 - ETA: 0s - loss: 0.2802 - accuracy: 0.88 - ETA: 0s - loss: 0.2512 - accuracy: 0.89 - ETA: 0s - loss: 0.2576 - accuracy: 0.89 - ETA: 0s - loss: 0.2504 - accuracy: 0.89 - ETA: 0s - loss: 0.2458 - accuracy: 0.90 - ETA: 0s - loss: 0.2632 - accuracy: 0.89 - ETA: 0s - loss: 0.2563 - accuracy: 0.89 - ETA: 0s - loss: 0.2666 - accuracy: 0.89 - ETA: 0s - loss: 0.2539 - accuracy: 0.89 - 1s 15ms/step - loss: 0.2515 - accuracy: 0.8968 - val_loss: 0.3642 - val_accuracy: 0.8696\n",
      "Epoch 6/10\n",
      "95/95 [==============================] - ETA: 7s - loss: 0.0202 - accuracy: 1.00 - ETA: 1s - loss: 0.2622 - accuracy: 0.91 - ETA: 1s - loss: 0.1858 - accuracy: 0.92 - ETA: 1s - loss: 0.1652 - accuracy: 0.93 - ETA: 0s - loss: 0.2088 - accuracy: 0.92 - ETA: 0s - loss: 0.1846 - accuracy: 0.92 - ETA: 0s - loss: 0.1761 - accuracy: 0.92 - ETA: 0s - loss: 0.1776 - accuracy: 0.93 - ETA: 0s - loss: 0.1733 - accuracy: 0.93 - ETA: 0s - loss: 0.1946 - accuracy: 0.92 - ETA: 0s - loss: 0.1872 - accuracy: 0.92 - ETA: 0s - loss: 0.1812 - accuracy: 0.92 - ETA: 0s - loss: 0.1691 - accuracy: 0.93 - ETA: 0s - loss: 0.1785 - accuracy: 0.92 - ETA: 0s - loss: 0.1758 - accuracy: 0.93 - ETA: 0s - loss: 0.1845 - accuracy: 0.92 - 1s 15ms/step - loss: 0.1835 - accuracy: 0.9242 - val_loss: 0.3709 - val_accuracy: 0.8696\n",
      "Epoch 7/10\n",
      "95/95 [==============================] - ETA: 7s - loss: 0.0284 - accuracy: 1.00 - ETA: 1s - loss: 0.1247 - accuracy: 0.91 - ETA: 1s - loss: 0.1151 - accuracy: 0.92 - ETA: 1s - loss: 0.1259 - accuracy: 0.93 - ETA: 0s - loss: 0.1105 - accuracy: 0.94 - ETA: 0s - loss: 0.1499 - accuracy: 0.93 - ETA: 0s - loss: 0.1413 - accuracy: 0.94 - ETA: 0s - loss: 0.1518 - accuracy: 0.93 - ETA: 0s - loss: 0.1736 - accuracy: 0.93 - ETA: 0s - loss: 0.1597 - accuracy: 0.93 - ETA: 0s - loss: 0.1509 - accuracy: 0.94 - ETA: 0s - loss: 0.1405 - accuracy: 0.94 - ETA: 0s - loss: 0.1442 - accuracy: 0.94 - ETA: 0s - loss: 0.1394 - accuracy: 0.94 - ETA: 0s - loss: 0.1432 - accuracy: 0.93 - ETA: 0s - loss: 0.1382 - accuracy: 0.93 - 1s 15ms/step - loss: 0.1442 - accuracy: 0.9347 - val_loss: 0.3363 - val_accuracy: 0.8696\n",
      "Epoch 8/10\n",
      "95/95 [==============================] - ETA: 7s - loss: 0.0027 - accuracy: 1.00 - ETA: 1s - loss: 0.1360 - accuracy: 0.91 - ETA: 1s - loss: 0.0880 - accuracy: 0.95 - ETA: 1s - loss: 0.1140 - accuracy: 0.93 - ETA: 0s - loss: 0.1004 - accuracy: 0.94 - ETA: 0s - loss: 0.1054 - accuracy: 0.94 - ETA: 0s - loss: 0.1013 - accuracy: 0.95 - ETA: 0s - loss: 0.1110 - accuracy: 0.94 - ETA: 0s - loss: 0.1077 - accuracy: 0.95 - ETA: 0s - loss: 0.1003 - accuracy: 0.95 - ETA: 0s - loss: 0.0993 - accuracy: 0.95 - ETA: 0s - loss: 0.0937 - accuracy: 0.96 - ETA: 0s - loss: 0.0905 - accuracy: 0.96 - ETA: 0s - loss: 0.0891 - accuracy: 0.96 - ETA: 0s - loss: 0.0904 - accuracy: 0.96 - ETA: 0s - loss: 0.0903 - accuracy: 0.96 - 1s 15ms/step - loss: 0.0898 - accuracy: 0.9600 - val_loss: 0.3359 - val_accuracy: 0.8609\n",
      "Epoch 9/10\n",
      "95/95 [==============================] - ETA: 7s - loss: 0.0578 - accuracy: 1.00 - ETA: 1s - loss: 0.1826 - accuracy: 0.88 - ETA: 1s - loss: 0.1011 - accuracy: 0.93 - ETA: 1s - loss: 0.1169 - accuracy: 0.93 - ETA: 0s - loss: 0.1058 - accuracy: 0.94 - ETA: 0s - loss: 0.0984 - accuracy: 0.94 - ETA: 0s - loss: 0.0971 - accuracy: 0.95 - ETA: 0s - loss: 0.0964 - accuracy: 0.94 - ETA: 0s - loss: 0.1120 - accuracy: 0.94 - ETA: 0s - loss: 0.1070 - accuracy: 0.94 - ETA: 0s - loss: 0.1060 - accuracy: 0.94 - ETA: 0s - loss: 0.1034 - accuracy: 0.94 - ETA: 0s - loss: 0.1020 - accuracy: 0.94 - ETA: 0s - loss: 0.1008 - accuracy: 0.94 - ETA: 0s - loss: 0.1001 - accuracy: 0.94 - ETA: 0s - loss: 0.1102 - accuracy: 0.94 - 1s 15ms/step - loss: 0.1199 - accuracy: 0.9453 - val_loss: 0.3405 - val_accuracy: 0.8609\n",
      "Epoch 10/10\n",
      "95/95 [==============================] - ETA: 7s - loss: 0.0722 - accuracy: 1.00 - ETA: 1s - loss: 0.1256 - accuracy: 0.93 - ETA: 1s - loss: 0.1540 - accuracy: 0.91 - ETA: 1s - loss: 0.1112 - accuracy: 0.94 - ETA: 0s - loss: 0.1401 - accuracy: 0.93 - ETA: 0s - loss: 0.1122 - accuracy: 0.94 - ETA: 0s - loss: 0.1016 - accuracy: 0.94 - ETA: 0s - loss: 0.1146 - accuracy: 0.93 - ETA: 0s - loss: 0.1061 - accuracy: 0.94 - ETA: 0s - loss: 0.1106 - accuracy: 0.93 - ETA: 0s - loss: 0.1109 - accuracy: 0.93 - ETA: 0s - loss: 0.1053 - accuracy: 0.94 - ETA: 0s - loss: 0.1136 - accuracy: 0.94 - ETA: 0s - loss: 0.1085 - accuracy: 0.94 - ETA: 0s - loss: 0.1027 - accuracy: 0.95 - ETA: 0s - loss: 0.1024 - accuracy: 0.95 - 1s 16ms/step - loss: 0.0985 - accuracy: 0.9558 - val_loss: 0.3067 - val_accuracy: 0.8696\n",
      "Epoch 1/20\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "95/95 [==============================] - ETA: 29:00 - loss: 0.0258 - accuracy: 1.000 - ETA: 9:29 - loss: 0.1779 - accuracy: 0.933 - ETA: 3:54 - loss: 0.0821 - accuracy: 0.97 - ETA: 2:22 - loss: 0.0655 - accuracy: 0.98 - ETA: 1:39 - loss: 0.0597 - accuracy: 0.97 - ETA: 1:15 - loss: 0.0516 - accuracy: 0.97 - ETA: 59s - loss: 0.0495 - accuracy: 0.9826 - ETA: 47s - loss: 0.0441 - accuracy: 0.985 - ETA: 39s - loss: 0.0417 - accuracy: 0.987 - ETA: 32s - loss: 0.0372 - accuracy: 0.988 - ETA: 27s - loss: 0.0727 - accuracy: 0.979 - ETA: 23s - loss: 0.0711 - accuracy: 0.976 - ETA: 19s - loss: 0.0724 - accuracy: 0.974 - ETA: 16s - loss: 0.0692 - accuracy: 0.976 - ETA: 14s - loss: 0.0669 - accuracy: 0.978 - ETA: 11s - loss: 0.0640 - accuracy: 0.979 - ETA: 9s - loss: 0.0639 - accuracy: 0.981 - ETA: 8s - loss: 0.0860 - accuracy: 0.97 - ETA: 6s - loss: 0.0838 - accuracy: 0.97 - ETA: 5s - loss: 0.0831 - accuracy: 0.97 - ETA: 3s - loss: 0.0842 - accuracy: 0.97 - ETA: 2s - loss: 0.0847 - accuracy: 0.97 - ETA: 1s - loss: 0.0855 - accuracy: 0.97 - ETA: 0s - loss: 0.0844 - accuracy: 0.97 - 21s 219ms/step - loss: 0.0827 - accuracy: 0.9726 - val_loss: 0.1674 - val_accuracy: 0.9478\n",
      "Epoch 2/20\n",
      "95/95 [==============================] - ETA: 9s - loss: 0.1059 - accuracy: 1.00 - ETA: 2s - loss: 0.2820 - accuracy: 0.92 - ETA: 2s - loss: 0.1906 - accuracy: 0.93 - ETA: 1s - loss: 0.1637 - accuracy: 0.93 - ETA: 1s - loss: 0.1302 - accuracy: 0.95 - ETA: 1s - loss: 0.1134 - accuracy: 0.95 - ETA: 1s - loss: 0.1180 - accuracy: 0.94 - ETA: 1s - loss: 0.1204 - accuracy: 0.94 - ETA: 1s - loss: 0.1084 - accuracy: 0.95 - ETA: 0s - loss: 0.1292 - accuracy: 0.94 - ETA: 0s - loss: 0.1240 - accuracy: 0.94 - ETA: 0s - loss: 0.1142 - accuracy: 0.95 - ETA: 0s - loss: 0.1052 - accuracy: 0.95 - ETA: 0s - loss: 0.1011 - accuracy: 0.95 - ETA: 0s - loss: 0.0982 - accuracy: 0.95 - ETA: 0s - loss: 0.1190 - accuracy: 0.95 - ETA: 0s - loss: 0.1137 - accuracy: 0.95 - ETA: 0s - loss: 0.1189 - accuracy: 0.95 - ETA: 0s - loss: 0.1241 - accuracy: 0.94 - ETA: 0s - loss: 0.1204 - accuracy: 0.94 - ETA: 0s - loss: 0.1151 - accuracy: 0.95 - ETA: 0s - loss: 0.1097 - accuracy: 0.95 - ETA: 0s - loss: 0.1091 - accuracy: 0.95 - ETA: 0s - loss: 0.1045 - accuracy: 0.95 - 2s 20ms/step - loss: 0.1055 - accuracy: 0.9558 - val_loss: 0.2319 - val_accuracy: 0.9652\n",
      "Epoch 3/20\n",
      "95/95 [==============================] - ETA: 6s - loss: 8.8880e-04 - accuracy: 1.00 - ETA: 2s - loss: 0.0014 - accuracy: 1.0000   - ETA: 1s - loss: 0.0448 - accuracy: 1.00 - ETA: 1s - loss: 0.0351 - accuracy: 1.00 - ETA: 1s - loss: 0.0391 - accuracy: 1.00 - ETA: 1s - loss: 0.1774 - accuracy: 0.99 - ETA: 1s - loss: 0.1761 - accuracy: 0.96 - ETA: 1s - loss: 0.1648 - accuracy: 0.96 - ETA: 1s - loss: 0.1494 - accuracy: 0.96 - ETA: 0s - loss: 0.1328 - accuracy: 0.97 - ETA: 0s - loss: 0.1249 - accuracy: 0.97 - ETA: 0s - loss: 0.1136 - accuracy: 0.97 - ETA: 0s - loss: 0.1415 - accuracy: 0.97 - ETA: 0s - loss: 0.1386 - accuracy: 0.97 - ETA: 0s - loss: 0.1289 - accuracy: 0.97 - ETA: 0s - loss: 0.1261 - accuracy: 0.97 - ETA: 0s - loss: 0.1395 - accuracy: 0.96 - ETA: 0s - loss: 0.1393 - accuracy: 0.96 - ETA: 0s - loss: 0.1322 - accuracy: 0.96 - ETA: 0s - loss: 0.1301 - accuracy: 0.96 - ETA: 0s - loss: 0.1243 - accuracy: 0.97 - ETA: 0s - loss: 0.1189 - accuracy: 0.97 - ETA: 0s - loss: 0.1144 - accuracy: 0.97 - ETA: 0s - loss: 0.1167 - accuracy: 0.97 - 2s 20ms/step - loss: 0.1131 - accuracy: 0.9726 - val_loss: 0.2096 - val_accuracy: 0.9739\n",
      "Epoch 4/20\n",
      "95/95 [==============================] - ETA: 8s - loss: 0.0476 - accuracy: 1.00 - ETA: 3s - loss: 0.0821 - accuracy: 0.95 - ETA: 2s - loss: 0.0499 - accuracy: 0.97 - ETA: 2s - loss: 0.0357 - accuracy: 0.98 - ETA: 2s - loss: 0.0404 - accuracy: 0.98 - ETA: 1s - loss: 0.0401 - accuracy: 0.98 - ETA: 1s - loss: 0.0349 - accuracy: 0.99 - ETA: 1s - loss: 0.0342 - accuracy: 0.99 - ETA: 1s - loss: 0.0321 - accuracy: 0.99 - ETA: 1s - loss: 0.0346 - accuracy: 0.99 - ETA: 1s - loss: 0.0347 - accuracy: 0.99 - ETA: 0s - loss: 0.0372 - accuracy: 0.99 - ETA: 0s - loss: 0.0370 - accuracy: 0.99 - ETA: 0s - loss: 0.0364 - accuracy: 0.99 - ETA: 0s - loss: 0.0354 - accuracy: 0.99 - ETA: 0s - loss: 0.0341 - accuracy: 0.99 - ETA: 0s - loss: 0.0319 - accuracy: 0.99 - ETA: 0s - loss: 0.0364 - accuracy: 0.99 - ETA: 0s - loss: 0.0355 - accuracy: 0.99 - ETA: 0s - loss: 0.0336 - accuracy: 0.99 - ETA: 0s - loss: 0.0320 - accuracy: 0.99 - ETA: 0s - loss: 0.0344 - accuracy: 0.99 - ETA: 0s - loss: 0.0399 - accuracy: 0.99 - ETA: 0s - loss: 0.0388 - accuracy: 0.99 - ETA: 0s - loss: 0.0406 - accuracy: 0.99 - 2s 20ms/step - loss: 0.0398 - accuracy: 0.9937 - val_loss: 0.2113 - val_accuracy: 0.9652\n",
      "Epoch 5/20\n",
      "95/95 [==============================] - ETA: 8s - loss: 0.0051 - accuracy: 1.00 - ETA: 2s - loss: 0.0101 - accuracy: 1.00 - ETA: 1s - loss: 0.0248 - accuracy: 1.00 - ETA: 1s - loss: 0.0265 - accuracy: 1.00 - ETA: 1s - loss: 0.0307 - accuracy: 0.98 - ETA: 1s - loss: 0.0345 - accuracy: 0.98 - ETA: 1s - loss: 0.0482 - accuracy: 0.97 - ETA: 1s - loss: 0.0503 - accuracy: 0.97 - ETA: 1s - loss: 0.0460 - accuracy: 0.97 - ETA: 0s - loss: 0.0442 - accuracy: 0.97 - ETA: 0s - loss: 0.0409 - accuracy: 0.98 - ETA: 0s - loss: 0.0430 - accuracy: 0.97 - ETA: 0s - loss: 0.0424 - accuracy: 0.97 - ETA: 0s - loss: 0.0394 - accuracy: 0.98 - ETA: 0s - loss: 0.0394 - accuracy: 0.98 - ETA: 0s - loss: 0.0377 - accuracy: 0.98 - ETA: 0s - loss: 0.0385 - accuracy: 0.98 - ETA: 0s - loss: 0.0364 - accuracy: 0.98 - ETA: 0s - loss: 0.0387 - accuracy: 0.98 - ETA: 0s - loss: 0.0368 - accuracy: 0.98 - ETA: 0s - loss: 0.0350 - accuracy: 0.98 - ETA: 0s - loss: 0.0419 - accuracy: 0.97 - ETA: 0s - loss: 0.0481 - accuracy: 0.97 - ETA: 0s - loss: 0.0486 - accuracy: 0.97 - 2s 20ms/step - loss: 0.0475 - accuracy: 0.9789 - val_loss: 0.1513 - val_accuracy: 0.9739\n",
      "Epoch 6/20\n",
      "95/95 [==============================] - ETA: 6s - loss: 3.8624e-06 - accuracy: 1.00 - ETA: 2s - loss: 0.0013 - accuracy: 1.0000   - ETA: 2s - loss: 0.0134 - accuracy: 1.00 - ETA: 1s - loss: 0.0188 - accuracy: 1.00 - ETA: 1s - loss: 0.0158 - accuracy: 1.00 - ETA: 1s - loss: 0.0295 - accuracy: 0.99 - ETA: 1s - loss: 0.0354 - accuracy: 0.98 - ETA: 1s - loss: 0.0350 - accuracy: 0.98 - ETA: 1s - loss: 0.0308 - accuracy: 0.98 - ETA: 0s - loss: 0.0278 - accuracy: 0.98 - ETA: 0s - loss: 0.0272 - accuracy: 0.99 - ETA: 0s - loss: 0.0275 - accuracy: 0.99 - ETA: 0s - loss: 0.0277 - accuracy: 0.99 - ETA: 0s - loss: 0.0258 - accuracy: 0.99 - ETA: 0s - loss: 0.0249 - accuracy: 0.99 - ETA: 0s - loss: 0.0236 - accuracy: 0.99 - ETA: 0s - loss: 0.0261 - accuracy: 0.99 - ETA: 0s - loss: 0.0284 - accuracy: 0.98 - ETA: 0s - loss: 0.0270 - accuracy: 0.98 - ETA: 0s - loss: 0.0259 - accuracy: 0.98 - ETA: 0s - loss: 0.0247 - accuracy: 0.98 - ETA: 0s - loss: 0.0238 - accuracy: 0.99 - ETA: 0s - loss: 0.0228 - accuracy: 0.99 - ETA: 0s - loss: 0.0258 - accuracy: 0.98 - 2s 20ms/step - loss: 0.0259 - accuracy: 0.9895 - val_loss: 0.1359 - val_accuracy: 0.9652\n",
      "Epoch 7/20\n",
      "95/95 [==============================] - ETA: 6s - loss: 0.1272 - accuracy: 1.00 - ETA: 2s - loss: 0.1386 - accuracy: 0.90 - ETA: 2s - loss: 0.2067 - accuracy: 0.92 - ETA: 1s - loss: 0.1536 - accuracy: 0.93 - ETA: 1s - loss: 0.1162 - accuracy: 0.95 - ETA: 1s - loss: 0.1108 - accuracy: 0.95 - ETA: 1s - loss: 0.1021 - accuracy: 0.95 - ETA: 1s - loss: 0.0906 - accuracy: 0.95 - ETA: 1s - loss: 0.0801 - accuracy: 0.96 - ETA: 0s - loss: 0.0731 - accuracy: 0.96 - ETA: 0s - loss: 0.0681 - accuracy: 0.97 - ETA: 0s - loss: 0.0628 - accuracy: 0.97 - ETA: 0s - loss: 0.0593 - accuracy: 0.97 - ETA: 0s - loss: 0.0604 - accuracy: 0.97 - ETA: 0s - loss: 0.0587 - accuracy: 0.97 - ETA: 0s - loss: 0.0613 - accuracy: 0.97 - ETA: 0s - loss: 0.0631 - accuracy: 0.96 - ETA: 0s - loss: 0.0626 - accuracy: 0.96 - ETA: 0s - loss: 0.0591 - accuracy: 0.96 - ETA: 0s - loss: 0.0605 - accuracy: 0.96 - ETA: 0s - loss: 0.0575 - accuracy: 0.97 - ETA: 0s - loss: 0.0561 - accuracy: 0.97 - ETA: 0s - loss: 0.0537 - accuracy: 0.97 - ETA: 0s - loss: 0.0528 - accuracy: 0.97 - 2s 20ms/step - loss: 0.0526 - accuracy: 0.9747 - val_loss: 0.1793 - val_accuracy: 0.9739\n",
      "Epoch 8/20\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "95/95 [==============================] - ETA: 8s - loss: 4.5261e-04 - accuracy: 1.00 - ETA: 2s - loss: 0.0110 - accuracy: 1.0000   - ETA: 1s - loss: 0.0077 - accuracy: 1.00 - ETA: 1s - loss: 0.0057 - accuracy: 1.00 - ETA: 1s - loss: 0.0054 - accuracy: 1.00 - ETA: 1s - loss: 0.0129 - accuracy: 1.00 - ETA: 1s - loss: 0.0169 - accuracy: 1.00 - ETA: 1s - loss: 0.0204 - accuracy: 0.99 - ETA: 1s - loss: 0.0185 - accuracy: 0.99 - ETA: 0s - loss: 0.0187 - accuracy: 0.99 - ETA: 0s - loss: 0.0174 - accuracy: 0.99 - ETA: 0s - loss: 0.0183 - accuracy: 0.99 - ETA: 0s - loss: 0.0211 - accuracy: 0.99 - ETA: 0s - loss: 0.0229 - accuracy: 0.99 - ETA: 0s - loss: 0.0236 - accuracy: 0.99 - ETA: 0s - loss: 0.0221 - accuracy: 0.99 - ETA: 0s - loss: 0.0238 - accuracy: 0.99 - ETA: 0s - loss: 0.0236 - accuracy: 0.99 - ETA: 0s - loss: 0.0237 - accuracy: 0.99 - ETA: 0s - loss: 0.0244 - accuracy: 0.99 - ETA: 0s - loss: 0.0233 - accuracy: 0.99 - ETA: 0s - loss: 0.0278 - accuracy: 0.99 - ETA: 0s - loss: 0.0273 - accuracy: 0.99 - ETA: 0s - loss: 0.0299 - accuracy: 0.98 - 2s 20ms/step - loss: 0.0297 - accuracy: 0.9895 - val_loss: 0.1253 - val_accuracy: 0.9739\n",
      "Epoch 9/20\n",
      "95/95 [==============================] - ETA: 7s - loss: 0.0142 - accuracy: 1.00 - ETA: 3s - loss: 0.0125 - accuracy: 1.00 - ETA: 2s - loss: 0.0094 - accuracy: 1.00 - ETA: 1s - loss: 0.0380 - accuracy: 0.98 - ETA: 1s - loss: 0.0331 - accuracy: 0.98 - ETA: 1s - loss: 0.0340 - accuracy: 0.99 - ETA: 1s - loss: 0.0294 - accuracy: 0.99 - ETA: 1s - loss: 0.0313 - accuracy: 0.98 - ETA: 1s - loss: 0.0276 - accuracy: 0.98 - ETA: 0s - loss: 0.0251 - accuracy: 0.98 - ETA: 0s - loss: 0.0237 - accuracy: 0.99 - ETA: 0s - loss: 0.0230 - accuracy: 0.99 - ETA: 0s - loss: 0.0250 - accuracy: 0.99 - ETA: 0s - loss: 0.0239 - accuracy: 0.99 - ETA: 0s - loss: 0.0232 - accuracy: 0.99 - ETA: 0s - loss: 0.0239 - accuracy: 0.99 - ETA: 0s - loss: 0.0226 - accuracy: 0.99 - ETA: 0s - loss: 0.0216 - accuracy: 0.99 - ETA: 0s - loss: 0.0259 - accuracy: 0.99 - ETA: 0s - loss: 0.0273 - accuracy: 0.99 - ETA: 0s - loss: 0.0261 - accuracy: 0.99 - ETA: 0s - loss: 0.0258 - accuracy: 0.99 - ETA: 0s - loss: 0.0248 - accuracy: 0.99 - ETA: 0s - loss: 0.0249 - accuracy: 0.99 - 2s 20ms/step - loss: 0.0268 - accuracy: 0.9916 - val_loss: 0.1126 - val_accuracy: 0.9826\n",
      "Epoch 10/20\n",
      "95/95 [==============================] - ETA: 7s - loss: 9.0598e-06 - accuracy: 1.00 - ETA: 2s - loss: 9.8671e-04 - accuracy: 1.00 - ETA: 1s - loss: 0.0017 - accuracy: 1.0000   - ETA: 1s - loss: 0.0092 - accuracy: 1.00 - ETA: 1s - loss: 0.0073 - accuracy: 1.00 - ETA: 1s - loss: 0.0158 - accuracy: 1.00 - ETA: 1s - loss: 0.0133 - accuracy: 1.00 - ETA: 1s - loss: 0.0170 - accuracy: 1.00 - ETA: 0s - loss: 0.0153 - accuracy: 1.00 - ETA: 0s - loss: 0.0146 - accuracy: 1.00 - ETA: 0s - loss: 0.0370 - accuracy: 0.99 - ETA: 0s - loss: 0.0338 - accuracy: 0.99 - ETA: 0s - loss: 0.0314 - accuracy: 0.99 - ETA: 0s - loss: 0.0292 - accuracy: 0.99 - ETA: 0s - loss: 0.0319 - accuracy: 0.98 - ETA: 0s - loss: 0.0385 - accuracy: 0.98 - ETA: 0s - loss: 0.0380 - accuracy: 0.98 - ETA: 0s - loss: 0.0365 - accuracy: 0.98 - ETA: 0s - loss: 0.0347 - accuracy: 0.98 - ETA: 0s - loss: 0.0369 - accuracy: 0.98 - ETA: 0s - loss: 0.0365 - accuracy: 0.98 - ETA: 0s - loss: 0.0439 - accuracy: 0.98 - ETA: 0s - loss: 0.0499 - accuracy: 0.98 - ETA: 0s - loss: 0.0482 - accuracy: 0.98 - 2s 20ms/step - loss: 0.0473 - accuracy: 0.9832 - val_loss: 0.1145 - val_accuracy: 0.9739\n",
      "Epoch 11/20\n",
      "95/95 [==============================] - ETA: 8s - loss: 0.0170 - accuracy: 1.00 - ETA: 2s - loss: 0.0824 - accuracy: 0.96 - ETA: 1s - loss: 0.0716 - accuracy: 0.95 - ETA: 1s - loss: 0.0675 - accuracy: 0.95 - ETA: 1s - loss: 0.0692 - accuracy: 0.95 - ETA: 1s - loss: 0.0763 - accuracy: 0.95 - ETA: 1s - loss: 0.0643 - accuracy: 0.96 - ETA: 1s - loss: 0.0592 - accuracy: 0.96 - ETA: 1s - loss: 0.0524 - accuracy: 0.96 - ETA: 0s - loss: 0.0469 - accuracy: 0.97 - ETA: 0s - loss: 0.0679 - accuracy: 0.95 - ETA: 0s - loss: 0.0692 - accuracy: 0.95 - ETA: 0s - loss: 0.0715 - accuracy: 0.95 - ETA: 0s - loss: 0.0661 - accuracy: 0.95 - ETA: 0s - loss: 0.0619 - accuracy: 0.96 - ETA: 0s - loss: 0.0582 - accuracy: 0.96 - ETA: 0s - loss: 0.0558 - accuracy: 0.96 - ETA: 0s - loss: 0.0526 - accuracy: 0.96 - ETA: 0s - loss: 0.0504 - accuracy: 0.96 - ETA: 0s - loss: 0.0479 - accuracy: 0.97 - ETA: 0s - loss: 0.0456 - accuracy: 0.97 - ETA: 0s - loss: 0.0435 - accuracy: 0.97 - ETA: 0s - loss: 0.0434 - accuracy: 0.97 - ETA: 0s - loss: 0.0426 - accuracy: 0.97 - 2s 21ms/step - loss: 0.0440 - accuracy: 0.9747 - val_loss: 0.1898 - val_accuracy: 0.9478\n",
      "Epoch 12/20\n",
      "95/95 [==============================] - ETA: 6s - loss: 4.8015e-05 - accuracy: 1.00 - ETA: 2s - loss: 0.0834 - accuracy: 0.9500   - ETA: 1s - loss: 0.0460 - accuracy: 0.97 - ETA: 1s - loss: 0.0316 - accuracy: 0.98 - ETA: 1s - loss: 0.0620 - accuracy: 0.97 - ETA: 1s - loss: 0.0616 - accuracy: 0.98 - ETA: 1s - loss: 0.0499 - accuracy: 0.98 - ETA: 1s - loss: 0.0504 - accuracy: 0.98 - ETA: 1s - loss: 0.0444 - accuracy: 0.98 - ETA: 0s - loss: 0.0551 - accuracy: 0.98 - ETA: 0s - loss: 0.0567 - accuracy: 0.98 - ETA: 0s - loss: 0.0516 - accuracy: 0.98 - ETA: 0s - loss: 0.0524 - accuracy: 0.98 - ETA: 0s - loss: 0.0509 - accuracy: 0.98 - ETA: 0s - loss: 0.0517 - accuracy: 0.98 - ETA: 0s - loss: 0.0503 - accuracy: 0.98 - ETA: 0s - loss: 0.0505 - accuracy: 0.98 - ETA: 0s - loss: 0.0477 - accuracy: 0.98 - ETA: 0s - loss: 0.0452 - accuracy: 0.98 - ETA: 0s - loss: 0.0441 - accuracy: 0.98 - ETA: 0s - loss: 0.0431 - accuracy: 0.98 - ETA: 0s - loss: 0.0410 - accuracy: 0.98 - ETA: 0s - loss: 0.0407 - accuracy: 0.98 - ETA: 0s - loss: 0.0484 - accuracy: 0.98 - 2s 20ms/step - loss: 0.0506 - accuracy: 0.9853 - val_loss: 0.1835 - val_accuracy: 0.9478\n",
      "Epoch 13/20\n",
      "95/95 [==============================] - ETA: 8s - loss: 1.8276e-04 - accuracy: 1.00 - ETA: 2s - loss: 0.0483 - accuracy: 0.9600   - ETA: 1s - loss: 0.0307 - accuracy: 0.97 - ETA: 1s - loss: 0.0320 - accuracy: 0.98 - ETA: 1s - loss: 0.0252 - accuracy: 0.98 - ETA: 1s - loss: 0.0206 - accuracy: 0.99 - ETA: 1s - loss: 0.0249 - accuracy: 0.99 - ETA: 1s - loss: 0.0216 - accuracy: 0.99 - ETA: 0s - loss: 0.0232 - accuracy: 0.99 - ETA: 0s - loss: 0.0207 - accuracy: 0.99 - ETA: 0s - loss: 0.0342 - accuracy: 0.99 - ETA: 0s - loss: 0.0314 - accuracy: 0.99 - ETA: 0s - loss: 0.0289 - accuracy: 0.99 - ETA: 0s - loss: 0.0309 - accuracy: 0.98 - ETA: 0s - loss: 0.0288 - accuracy: 0.98 - ETA: 0s - loss: 0.0271 - accuracy: 0.99 - ETA: 0s - loss: 0.0279 - accuracy: 0.99 - ETA: 0s - loss: 0.0271 - accuracy: 0.99 - ETA: 0s - loss: 0.0281 - accuracy: 0.99 - ETA: 0s - loss: 0.0287 - accuracy: 0.99 - ETA: 0s - loss: 0.0273 - accuracy: 0.99 - ETA: 0s - loss: 0.0261 - accuracy: 0.99 - ETA: 0s - loss: 0.0298 - accuracy: 0.99 - ETA: 0s - loss: 0.0305 - accuracy: 0.99 - 2s 20ms/step - loss: 0.0300 - accuracy: 0.9916 - val_loss: 0.1867 - val_accuracy: 0.9478\n",
      "Epoch 14/20\n",
      "95/95 [==============================] - ETA: 6s - loss: 4.7922e-06 - accuracy: 1.00 - ETA: 2s - loss: 0.0227 - accuracy: 1.0000   - ETA: 2s - loss: 0.0385 - accuracy: 0.97 - ETA: 1s - loss: 0.0543 - accuracy: 0.96 - ETA: 1s - loss: 0.0407 - accuracy: 0.97 - ETA: 1s - loss: 0.0334 - accuracy: 0.98 - ETA: 1s - loss: 0.0311 - accuracy: 0.98 - ETA: 1s - loss: 0.0269 - accuracy: 0.98 - ETA: 1s - loss: 0.0238 - accuracy: 0.98 - ETA: 0s - loss: 0.0285 - accuracy: 0.98 - ETA: 0s - loss: 0.0316 - accuracy: 0.98 - ETA: 0s - loss: 0.0304 - accuracy: 0.98 - ETA: 0s - loss: 0.0340 - accuracy: 0.97 - ETA: 0s - loss: 0.0315 - accuracy: 0.98 - ETA: 0s - loss: 0.0294 - accuracy: 0.98 - ETA: 0s - loss: 0.0276 - accuracy: 0.98 - ETA: 0s - loss: 0.0287 - accuracy: 0.98 - ETA: 0s - loss: 0.0281 - accuracy: 0.98 - ETA: 0s - loss: 0.0266 - accuracy: 0.98 - ETA: 0s - loss: 0.0253 - accuracy: 0.98 - ETA: 0s - loss: 0.0269 - accuracy: 0.98 - ETA: 0s - loss: 0.0262 - accuracy: 0.98 - ETA: 0s - loss: 0.0261 - accuracy: 0.98 - ETA: 0s - loss: 0.0269 - accuracy: 0.98 - 2s 20ms/step - loss: 0.0261 - accuracy: 0.9895 - val_loss: 0.1404 - val_accuracy: 0.9652\n",
      "Epoch 15/20\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "95/95 [==============================] - ETA: 7s - loss: 2.7945e-04 - accuracy: 1.00 - ETA: 2s - loss: 0.0218 - accuracy: 1.0000   - ETA: 1s - loss: 0.0288 - accuracy: 1.00 - ETA: 1s - loss: 0.0410 - accuracy: 0.98 - ETA: 1s - loss: 0.0350 - accuracy: 0.98 - ETA: 1s - loss: 0.0285 - accuracy: 0.99 - ETA: 1s - loss: 0.0271 - accuracy: 0.99 - ETA: 1s - loss: 0.0392 - accuracy: 0.97 - ETA: 1s - loss: 0.0347 - accuracy: 0.98 - ETA: 0s - loss: 0.0310 - accuracy: 0.98 - ETA: 0s - loss: 0.0304 - accuracy: 0.98 - ETA: 0s - loss: 0.0293 - accuracy: 0.98 - ETA: 0s - loss: 0.0270 - accuracy: 0.98 - ETA: 0s - loss: 0.0255 - accuracy: 0.98 - ETA: 0s - loss: 0.0259 - accuracy: 0.98 - ETA: 0s - loss: 0.0296 - accuracy: 0.98 - ETA: 0s - loss: 0.0291 - accuracy: 0.98 - ETA: 0s - loss: 0.0403 - accuracy: 0.97 - ETA: 0s - loss: 0.0468 - accuracy: 0.97 - ETA: 0s - loss: 0.0446 - accuracy: 0.97 - ETA: 0s - loss: 0.0424 - accuracy: 0.98 - ETA: 0s - loss: 0.0405 - accuracy: 0.98 - ETA: 0s - loss: 0.0432 - accuracy: 0.97 - ETA: 0s - loss: 0.0424 - accuracy: 0.97 - 2s 20ms/step - loss: 0.0415 - accuracy: 0.9789 - val_loss: 0.1096 - val_accuracy: 0.9652\n",
      "Epoch 16/20\n",
      "95/95 [==============================] - ETA: 7s - loss: 6.1173e-05 - accuracy: 1.00 - ETA: 3s - loss: 2.7875e-04 - accuracy: 1.00 - ETA: 2s - loss: 0.0198 - accuracy: 0.9750   - ETA: 1s - loss: 0.0553 - accuracy: 0.95 - ETA: 1s - loss: 0.0472 - accuracy: 0.96 - ETA: 1s - loss: 0.0497 - accuracy: 0.96 - ETA: 1s - loss: 0.0452 - accuracy: 0.96 - ETA: 1s - loss: 0.0399 - accuracy: 0.97 - ETA: 1s - loss: 0.0369 - accuracy: 0.97 - ETA: 0s - loss: 0.0340 - accuracy: 0.97 - ETA: 0s - loss: 0.0306 - accuracy: 0.98 - ETA: 0s - loss: 0.0337 - accuracy: 0.97 - ETA: 0s - loss: 0.0368 - accuracy: 0.97 - ETA: 0s - loss: 0.0623 - accuracy: 0.97 - ETA: 0s - loss: 0.0595 - accuracy: 0.97 - ETA: 0s - loss: 0.0559 - accuracy: 0.97 - ETA: 0s - loss: 0.0525 - accuracy: 0.97 - ETA: 0s - loss: 0.0675 - accuracy: 0.96 - ETA: 0s - loss: 0.0654 - accuracy: 0.96 - ETA: 0s - loss: 0.0625 - accuracy: 0.97 - ETA: 0s - loss: 0.0958 - accuracy: 0.97 - ETA: 0s - loss: 0.0937 - accuracy: 0.97 - ETA: 0s - loss: 0.0912 - accuracy: 0.97 - ETA: 0s - loss: 0.0900 - accuracy: 0.97 - 2s 20ms/step - loss: 0.0886 - accuracy: 0.9726 - val_loss: 0.2518 - val_accuracy: 0.9652\n",
      "Epoch 17/20\n",
      "95/95 [==============================] - ETA: 7s - loss: 0.0090 - accuracy: 1.00 - ETA: 3s - loss: 0.0023 - accuracy: 1.00 - ETA: 2s - loss: 0.0907 - accuracy: 0.95 - ETA: 1s - loss: 0.0674 - accuracy: 0.96 - ETA: 1s - loss: 0.0511 - accuracy: 0.97 - ETA: 1s - loss: 0.0544 - accuracy: 0.97 - ETA: 1s - loss: 0.0615 - accuracy: 0.96 - ETA: 1s - loss: 0.0589 - accuracy: 0.97 - ETA: 1s - loss: 0.0563 - accuracy: 0.97 - ETA: 0s - loss: 0.0718 - accuracy: 0.96 - ETA: 0s - loss: 0.0699 - accuracy: 0.97 - ETA: 0s - loss: 0.0640 - accuracy: 0.97 - ETA: 0s - loss: 0.0600 - accuracy: 0.97 - ETA: 0s - loss: 0.0591 - accuracy: 0.97 - ETA: 0s - loss: 0.0558 - accuracy: 0.97 - ETA: 0s - loss: 0.0521 - accuracy: 0.98 - ETA: 0s - loss: 0.0489 - accuracy: 0.98 - ETA: 0s - loss: 0.0461 - accuracy: 0.98 - ETA: 0s - loss: 0.0442 - accuracy: 0.98 - ETA: 0s - loss: 0.0453 - accuracy: 0.98 - ETA: 0s - loss: 0.0452 - accuracy: 0.98 - ETA: 0s - loss: 0.0456 - accuracy: 0.98 - ETA: 0s - loss: 0.0472 - accuracy: 0.98 - ETA: 0s - loss: 0.0455 - accuracy: 0.98 - 2s 20ms/step - loss: 0.0469 - accuracy: 0.9811 - val_loss: 0.1932 - val_accuracy: 0.9652\n",
      "Epoch 18/20\n",
      "95/95 [==============================] - ETA: 8s - loss: 0.0040 - accuracy: 1.00 - ETA: 2s - loss: 0.0029 - accuracy: 1.00 - ETA: 1s - loss: 0.0152 - accuracy: 1.00 - ETA: 1s - loss: 0.0226 - accuracy: 1.00 - ETA: 1s - loss: 0.0318 - accuracy: 0.98 - ETA: 1s - loss: 0.0357 - accuracy: 0.99 - ETA: 1s - loss: 0.0431 - accuracy: 0.98 - ETA: 1s - loss: 0.0488 - accuracy: 0.97 - ETA: 1s - loss: 0.0574 - accuracy: 0.97 - ETA: 0s - loss: 0.0576 - accuracy: 0.97 - ETA: 0s - loss: 0.0520 - accuracy: 0.97 - ETA: 0s - loss: 0.0475 - accuracy: 0.97 - ETA: 0s - loss: 0.0550 - accuracy: 0.97 - ETA: 0s - loss: 0.0542 - accuracy: 0.97 - ETA: 0s - loss: 0.0578 - accuracy: 0.97 - ETA: 0s - loss: 0.0628 - accuracy: 0.96 - ETA: 0s - loss: 0.0625 - accuracy: 0.96 - ETA: 0s - loss: 0.0632 - accuracy: 0.96 - ETA: 0s - loss: 0.0615 - accuracy: 0.96 - ETA: 0s - loss: 0.0590 - accuracy: 0.96 - ETA: 0s - loss: 0.0562 - accuracy: 0.97 - ETA: 0s - loss: 0.0538 - accuracy: 0.97 - ETA: 0s - loss: 0.0530 - accuracy: 0.97 - ETA: 0s - loss: 0.0508 - accuracy: 0.97 - 2s 20ms/step - loss: 0.0497 - accuracy: 0.9747 - val_loss: 0.1435 - val_accuracy: 0.9652\n",
      "Epoch 19/20\n",
      "95/95 [==============================] - ETA: 6s - loss: 1.7404e-05 - accuracy: 1.00 - ETA: 2s - loss: 0.0027 - accuracy: 1.0000   - ETA: 2s - loss: 0.0335 - accuracy: 1.00 - ETA: 1s - loss: 0.0268 - accuracy: 1.00 - ETA: 1s - loss: 0.0204 - accuracy: 1.00 - ETA: 1s - loss: 0.0226 - accuracy: 1.00 - ETA: 1s - loss: 0.0229 - accuracy: 1.00 - ETA: 1s - loss: 0.0202 - accuracy: 1.00 - ETA: 1s - loss: 0.0200 - accuracy: 1.00 - ETA: 0s - loss: 0.0383 - accuracy: 0.98 - ETA: 0s - loss: 0.0350 - accuracy: 0.99 - ETA: 0s - loss: 0.0330 - accuracy: 0.99 - ETA: 0s - loss: 0.0303 - accuracy: 0.99 - ETA: 0s - loss: 0.0281 - accuracy: 0.99 - ETA: 0s - loss: 0.0262 - accuracy: 0.99 - ETA: 0s - loss: 0.0251 - accuracy: 0.99 - ETA: 0s - loss: 0.0282 - accuracy: 0.99 - ETA: 0s - loss: 0.0298 - accuracy: 0.98 - ETA: 0s - loss: 0.0285 - accuracy: 0.98 - ETA: 0s - loss: 0.0276 - accuracy: 0.98 - ETA: 0s - loss: 0.0265 - accuracy: 0.99 - ETA: 0s - loss: 0.0266 - accuracy: 0.99 - ETA: 0s - loss: 0.0254 - accuracy: 0.99 - ETA: 0s - loss: 0.0246 - accuracy: 0.99 - 2s 20ms/step - loss: 0.0244 - accuracy: 0.9916 - val_loss: 0.1430 - val_accuracy: 0.9652\n",
      "Epoch 20/20\n",
      "95/95 [==============================] - ETA: 6s - loss: 0.0046 - accuracy: 1.00 - ETA: 2s - loss: 0.0023 - accuracy: 1.00 - ETA: 2s - loss: 0.0350 - accuracy: 0.97 - ETA: 1s - loss: 0.0234 - accuracy: 0.98 - ETA: 1s - loss: 0.0207 - accuracy: 0.98 - ETA: 1s - loss: 0.0272 - accuracy: 0.98 - ETA: 1s - loss: 0.0283 - accuracy: 0.98 - ETA: 1s - loss: 0.0245 - accuracy: 0.98 - ETA: 1s - loss: 0.0240 - accuracy: 0.98 - ETA: 0s - loss: 0.0215 - accuracy: 0.98 - ETA: 0s - loss: 0.0226 - accuracy: 0.99 - ETA: 0s - loss: 0.0211 - accuracy: 0.99 - ETA: 0s - loss: 0.0197 - accuracy: 0.99 - ETA: 0s - loss: 0.0207 - accuracy: 0.99 - ETA: 0s - loss: 0.0235 - accuracy: 0.98 - ETA: 0s - loss: 0.0268 - accuracy: 0.98 - ETA: 0s - loss: 0.0265 - accuracy: 0.98 - ETA: 0s - loss: 0.0259 - accuracy: 0.98 - ETA: 0s - loss: 0.0263 - accuracy: 0.98 - ETA: 0s - loss: 0.0250 - accuracy: 0.98 - ETA: 0s - loss: 0.0238 - accuracy: 0.99 - ETA: 0s - loss: 0.0235 - accuracy: 0.99 - ETA: 0s - loss: 0.0268 - accuracy: 0.98 - ETA: 0s - loss: 0.0284 - accuracy: 0.98 - 2s 21ms/step - loss: 0.0272 - accuracy: 0.9874 - val_loss: 0.1357 - val_accuracy: 0.9652\n",
      "Epoch 1/10\n",
      "96/96 [==============================] - ETA: 19:57 - loss: 6.8616 - accuracy: 0.200 - ETA: 3:50 - loss: 6.6960 - accuracy: 0.320 - ETA: 1:49 - loss: 5.6539 - accuracy: 0.36 - ETA: 1:08 - loss: 4.4986 - accuracy: 0.42 - ETA: 45s - loss: 3.8187 - accuracy: 0.4667 - ETA: 32s - loss: 3.3348 - accuracy: 0.548 - ETA: 24s - loss: 2.9390 - accuracy: 0.581 - ETA: 18s - loss: 2.6266 - accuracy: 0.615 - ETA: 14s - loss: 2.4227 - accuracy: 0.626 - ETA: 11s - loss: 2.2154 - accuracy: 0.651 - ETA: 9s - loss: 2.1728 - accuracy: 0.649 - ETA: 6s - loss: 2.1269 - accuracy: 0.64 - ETA: 5s - loss: 2.0966 - accuracy: 0.65 - ETA: 3s - loss: 1.9966 - accuracy: 0.65 - ETA: 2s - loss: 1.9274 - accuracy: 0.66 - ETA: 1s - loss: 1.8668 - accuracy: 0.66 - ETA: 0s - loss: 1.8488 - accuracy: 0.66 - 14s 151ms/step - loss: 1.8138 - accuracy: 0.6604 - val_loss: 0.8588 - val_accuracy: 0.6917\n",
      "Epoch 2/10\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "96/96 [==============================] - ETA: 19s - loss: 2.0495 - accuracy: 0.800 - ETA: 3s - loss: 1.4176 - accuracy: 0.733 - ETA: 2s - loss: 1.2241 - accuracy: 0.70 - ETA: 1s - loss: 1.1273 - accuracy: 0.76 - ETA: 1s - loss: 1.1936 - accuracy: 0.74 - ETA: 1s - loss: 1.1679 - accuracy: 0.73 - ETA: 1s - loss: 1.1000 - accuracy: 0.75 - ETA: 0s - loss: 1.0191 - accuracy: 0.75 - ETA: 0s - loss: 1.0542 - accuracy: 0.75 - ETA: 0s - loss: 1.1040 - accuracy: 0.75 - ETA: 0s - loss: 1.1551 - accuracy: 0.75 - ETA: 0s - loss: 1.1533 - accuracy: 0.75 - ETA: 0s - loss: 1.1080 - accuracy: 0.75 - ETA: 0s - loss: 1.1198 - accuracy: 0.75 - ETA: 0s - loss: 1.0757 - accuracy: 0.76 - ETA: 0s - loss: 1.0694 - accuracy: 0.75 - ETA: 0s - loss: 1.0934 - accuracy: 0.74 - ETA: 0s - loss: 1.0929 - accuracy: 0.74 - 2s 17ms/step - loss: 1.0990 - accuracy: 0.7479 - val_loss: 0.8097 - val_accuracy: 0.7000\n",
      "Epoch 3/10\n",
      "96/96 [==============================] - ETA: 7s - loss: 1.1970 - accuracy: 0.80 - ETA: 1s - loss: 0.9381 - accuracy: 0.80 - ETA: 1s - loss: 0.7543 - accuracy: 0.80 - ETA: 0s - loss: 0.7844 - accuracy: 0.80 - ETA: 0s - loss: 0.8187 - accuracy: 0.77 - ETA: 0s - loss: 0.9166 - accuracy: 0.78 - ETA: 0s - loss: 0.9617 - accuracy: 0.76 - ETA: 0s - loss: 0.9396 - accuracy: 0.76 - ETA: 0s - loss: 0.9173 - accuracy: 0.74 - ETA: 0s - loss: 0.8695 - accuracy: 0.76 - ETA: 0s - loss: 0.8807 - accuracy: 0.75 - ETA: 0s - loss: 0.8466 - accuracy: 0.75 - ETA: 0s - loss: 0.8666 - accuracy: 0.75 - ETA: 0s - loss: 0.8406 - accuracy: 0.75 - ETA: 0s - loss: 0.8387 - accuracy: 0.75 - ETA: 0s - loss: 0.8161 - accuracy: 0.75 - ETA: 0s - loss: 0.8527 - accuracy: 0.75 - 1s 16ms/step - loss: 0.8471 - accuracy: 0.7542 - val_loss: 0.7512 - val_accuracy: 0.7167\n",
      "Epoch 4/10\n",
      "96/96 [==============================] - ETA: 5s - loss: 0.2830 - accuracy: 0.80 - ETA: 2s - loss: 0.9972 - accuracy: 0.76 - ETA: 1s - loss: 0.9083 - accuracy: 0.71 - ETA: 1s - loss: 0.7744 - accuracy: 0.75 - ETA: 0s - loss: 0.7021 - accuracy: 0.77 - ETA: 0s - loss: 0.7353 - accuracy: 0.78 - ETA: 0s - loss: 0.7104 - accuracy: 0.79 - ETA: 0s - loss: 0.6800 - accuracy: 0.79 - ETA: 0s - loss: 0.6410 - accuracy: 0.80 - ETA: 0s - loss: 0.6461 - accuracy: 0.80 - ETA: 0s - loss: 0.6697 - accuracy: 0.80 - ETA: 0s - loss: 0.6733 - accuracy: 0.80 - ETA: 0s - loss: 0.6869 - accuracy: 0.79 - ETA: 0s - loss: 0.6782 - accuracy: 0.80 - ETA: 0s - loss: 0.6698 - accuracy: 0.80 - ETA: 0s - loss: 0.6665 - accuracy: 0.80 - ETA: 0s - loss: 0.6836 - accuracy: 0.80 - 1s 16ms/step - loss: 0.6771 - accuracy: 0.8083 - val_loss: 0.6984 - val_accuracy: 0.7583\n",
      "Epoch 5/10\n",
      "96/96 [==============================] - ETA: 6s - loss: 1.0954 - accuracy: 0.80 - ETA: 1s - loss: 0.9610 - accuracy: 0.76 - ETA: 1s - loss: 0.7633 - accuracy: 0.78 - ETA: 1s - loss: 0.7312 - accuracy: 0.77 - ETA: 0s - loss: 0.6712 - accuracy: 0.79 - ETA: 0s - loss: 0.6510 - accuracy: 0.77 - ETA: 0s - loss: 0.6208 - accuracy: 0.78 - ETA: 0s - loss: 0.6409 - accuracy: 0.78 - ETA: 0s - loss: 0.6069 - accuracy: 0.78 - ETA: 0s - loss: 0.6192 - accuracy: 0.77 - ETA: 0s - loss: 0.6116 - accuracy: 0.77 - ETA: 0s - loss: 0.5843 - accuracy: 0.78 - ETA: 0s - loss: 0.5914 - accuracy: 0.78 - ETA: 0s - loss: 0.5803 - accuracy: 0.78 - ETA: 0s - loss: 0.5824 - accuracy: 0.79 - ETA: 0s - loss: 0.5636 - accuracy: 0.79 - ETA: 0s - loss: 0.5544 - accuracy: 0.79 - 1s 16ms/step - loss: 0.5512 - accuracy: 0.7958 - val_loss: 0.5969 - val_accuracy: 0.7417\n",
      "Epoch 6/10\n",
      "96/96 [==============================] - ETA: 6s - loss: 0.6040 - accuracy: 0.60 - ETA: 2s - loss: 0.3283 - accuracy: 0.80 - ETA: 1s - loss: 0.5659 - accuracy: 0.78 - ETA: 1s - loss: 0.4932 - accuracy: 0.77 - ETA: 0s - loss: 0.5478 - accuracy: 0.77 - ETA: 0s - loss: 0.5317 - accuracy: 0.77 - ETA: 0s - loss: 0.4930 - accuracy: 0.79 - ETA: 0s - loss: 0.5059 - accuracy: 0.79 - ETA: 0s - loss: 0.5176 - accuracy: 0.79 - ETA: 0s - loss: 0.4850 - accuracy: 0.80 - ETA: 0s - loss: 0.5166 - accuracy: 0.80 - ETA: 0s - loss: 0.5009 - accuracy: 0.81 - ETA: 0s - loss: 0.4771 - accuracy: 0.82 - ETA: 0s - loss: 0.5019 - accuracy: 0.81 - ETA: 0s - loss: 0.4787 - accuracy: 0.81 - ETA: 0s - loss: 0.4883 - accuracy: 0.81 - ETA: 0s - loss: 0.4758 - accuracy: 0.81 - 1s 15ms/step - loss: 0.4710 - accuracy: 0.8208 - val_loss: 0.5680 - val_accuracy: 0.7667\n",
      "Epoch 7/10\n",
      "96/96 [==============================] - ETA: 5s - loss: 0.3656 - accuracy: 0.80 - ETA: 2s - loss: 0.2841 - accuracy: 0.80 - ETA: 1s - loss: 0.3733 - accuracy: 0.78 - ETA: 1s - loss: 0.2769 - accuracy: 0.84 - ETA: 0s - loss: 0.2698 - accuracy: 0.85 - ETA: 0s - loss: 0.2555 - accuracy: 0.86 - ETA: 0s - loss: 0.2737 - accuracy: 0.85 - ETA: 0s - loss: 0.2869 - accuracy: 0.85 - ETA: 0s - loss: 0.2730 - accuracy: 0.86 - ETA: 0s - loss: 0.2943 - accuracy: 0.86 - ETA: 0s - loss: 0.2928 - accuracy: 0.85 - ETA: 0s - loss: 0.3105 - accuracy: 0.85 - ETA: 0s - loss: 0.3479 - accuracy: 0.83 - ETA: 0s - loss: 0.3413 - accuracy: 0.84 - ETA: 0s - loss: 0.3851 - accuracy: 0.84 - ETA: 0s - loss: 0.3844 - accuracy: 0.84 - ETA: 0s - loss: 0.3734 - accuracy: 0.85 - 1s 15ms/step - loss: 0.3746 - accuracy: 0.8500 - val_loss: 0.5039 - val_accuracy: 0.7833\n",
      "Epoch 8/10\n",
      "96/96 [==============================] - ETA: 6s - loss: 0.1092 - accuracy: 1.00 - ETA: 2s - loss: 0.0537 - accuracy: 1.00 - ETA: 1s - loss: 0.1635 - accuracy: 0.94 - ETA: 1s - loss: 0.3136 - accuracy: 0.84 - ETA: 0s - loss: 0.3108 - accuracy: 0.85 - ETA: 0s - loss: 0.3109 - accuracy: 0.85 - ETA: 0s - loss: 0.3348 - accuracy: 0.85 - ETA: 0s - loss: 0.3417 - accuracy: 0.85 - ETA: 0s - loss: 0.3678 - accuracy: 0.84 - ETA: 0s - loss: 0.3950 - accuracy: 0.84 - ETA: 0s - loss: 0.4212 - accuracy: 0.83 - ETA: 0s - loss: 0.4098 - accuracy: 0.84 - ETA: 0s - loss: 0.3963 - accuracy: 0.84 - ETA: 0s - loss: 0.4024 - accuracy: 0.84 - ETA: 0s - loss: 0.3840 - accuracy: 0.85 - ETA: 0s - loss: 0.4221 - accuracy: 0.84 - ETA: 0s - loss: 0.4166 - accuracy: 0.84 - 1s 15ms/step - loss: 0.4081 - accuracy: 0.8500 - val_loss: 0.4901 - val_accuracy: 0.8083\n",
      "Epoch 9/10\n",
      "96/96 [==============================] - ETA: 6s - loss: 0.0126 - accuracy: 1.00 - ETA: 1s - loss: 0.0993 - accuracy: 0.96 - ETA: 1s - loss: 0.1037 - accuracy: 0.96 - ETA: 1s - loss: 0.1256 - accuracy: 0.96 - ETA: 0s - loss: 0.1029 - accuracy: 0.97 - ETA: 0s - loss: 0.0914 - accuracy: 0.97 - ETA: 0s - loss: 0.1503 - accuracy: 0.95 - ETA: 0s - loss: 0.1609 - accuracy: 0.93 - ETA: 0s - loss: 0.2066 - accuracy: 0.92 - ETA: 0s - loss: 0.2514 - accuracy: 0.90 - ETA: 0s - loss: 0.2647 - accuracy: 0.90 - ETA: 0s - loss: 0.2883 - accuracy: 0.89 - ETA: 0s - loss: 0.3092 - accuracy: 0.87 - ETA: 0s - loss: 0.3187 - accuracy: 0.86 - ETA: 0s - loss: 0.3081 - accuracy: 0.86 - ETA: 0s - loss: 0.3290 - accuracy: 0.86 - ETA: 0s - loss: 0.3196 - accuracy: 0.87 - 1s 16ms/step - loss: 0.3154 - accuracy: 0.8729 - val_loss: 0.4311 - val_accuracy: 0.8083\n",
      "Epoch 10/10\n",
      "96/96 [==============================] - ETA: 6s - loss: 4.3450e-04 - accuracy: 1.00 - ETA: 1s - loss: 0.4284 - accuracy: 0.8000   - ETA: 1s - loss: 0.3759 - accuracy: 0.83 - ETA: 1s - loss: 0.2929 - accuracy: 0.86 - ETA: 0s - loss: 0.3216 - accuracy: 0.85 - ETA: 0s - loss: 0.3039 - accuracy: 0.86 - ETA: 0s - loss: 0.2867 - accuracy: 0.86 - ETA: 0s - loss: 0.2733 - accuracy: 0.86 - ETA: 0s - loss: 0.2416 - accuracy: 0.87 - ETA: 0s - loss: 0.2681 - accuracy: 0.87 - ETA: 0s - loss: 0.2684 - accuracy: 0.87 - ETA: 0s - loss: 0.2585 - accuracy: 0.87 - ETA: 0s - loss: 0.2599 - accuracy: 0.87 - ETA: 0s - loss: 0.2596 - accuracy: 0.87 - ETA: 0s - loss: 0.2598 - accuracy: 0.88 - ETA: 0s - loss: 0.2524 - accuracy: 0.88 - ETA: 0s - loss: 0.2547 - accuracy: 0.88 - 2s 16ms/step - loss: 0.2700 - accuracy: 0.8813 - val_loss: 0.3882 - val_accuracy: 0.8250\n",
      "Epoch 1/20\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-17-31cb86b69fd6>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     41\u001b[0m                      \u001b[0msteps_per_epoch\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mn_train\u001b[0m \u001b[1;33m//\u001b[0m \u001b[0mBATCH_SIZE\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     42\u001b[0m                      \u001b[0mvalidation_steps\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mY\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m-\u001b[0m\u001b[0mn_train\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m//\u001b[0m \u001b[0mBATCH_SIZE\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 43\u001b[1;33m                      verbose=1)\n\u001b[0m\u001b[0;32m     44\u001b[0m     \u001b[0mfull_hists\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtemp\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     45\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\.conda\\envs\\indl\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_freq, max_queue_size, workers, use_multiprocessing, **kwargs)\u001b[0m\n\u001b[0;32m    789\u001b[0m           \u001b[0mworkers\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    790\u001b[0m           \u001b[0mshuffle\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mshuffle\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 791\u001b[1;33m           initial_epoch=initial_epoch)\n\u001b[0m\u001b[0;32m    792\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    793\u001b[0m     \u001b[1;31m# Case 3: Symbolic tensors or Numpy array-like.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\.conda\\envs\\indl\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mfit_generator\u001b[1;34m(self, generator, steps_per_epoch, epochs, verbose, callbacks, validation_data, validation_steps, validation_freq, class_weight, max_queue_size, workers, use_multiprocessing, shuffle, initial_epoch)\u001b[0m\n\u001b[0;32m   1513\u001b[0m         \u001b[0mshuffle\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mshuffle\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1514\u001b[0m         \u001b[0minitial_epoch\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0minitial_epoch\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1515\u001b[1;33m         steps_name='steps_per_epoch')\n\u001b[0m\u001b[0;32m   1516\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1517\u001b[0m   def evaluate_generator(self,\n",
      "\u001b[1;32m~\\.conda\\envs\\indl\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\training_generator.py\u001b[0m in \u001b[0;36mmodel_iteration\u001b[1;34m(model, data, steps_per_epoch, epochs, verbose, callbacks, validation_data, validation_steps, validation_freq, class_weight, max_queue_size, workers, use_multiprocessing, shuffle, initial_epoch, mode, batch_size, steps_name, **kwargs)\u001b[0m\n\u001b[0;32m    255\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    256\u001b[0m       \u001b[0mis_deferred\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_is_compiled\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 257\u001b[1;33m       \u001b[0mbatch_outs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mbatch_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0mbatch_data\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    258\u001b[0m       \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbatch_outs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    259\u001b[0m         \u001b[0mbatch_outs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mbatch_outs\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\.conda\\envs\\indl\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mtrain_on_batch\u001b[1;34m(self, x, y, sample_weight, class_weight, reset_metrics)\u001b[0m\n\u001b[0;32m   1256\u001b[0m         \u001b[0moutputs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrain_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mins\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# pylint: disable=not-callable\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1257\u001b[0m       \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1258\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_make_fit_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1259\u001b[0m         \u001b[0moutputs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_fit_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mins\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# pylint: disable=not-callable\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1260\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\.conda\\envs\\indl\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\training.py\u001b[0m in \u001b[0;36m_make_fit_function\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   2175\u001b[0m     ]\n\u001b[0;32m   2176\u001b[0m     self._make_train_function_helper(\n\u001b[1;32m-> 2177\u001b[1;33m         '_fit_function', [self.total_loss] + metrics_tensors)\n\u001b[0m\u001b[0;32m   2178\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2179\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0m_make_test_function_helper\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfn_name\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moutputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\.conda\\envs\\indl\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\training.py\u001b[0m in \u001b[0;36m_make_train_function_helper\u001b[1;34m(self, fn_name, outputs)\u001b[0m\n\u001b[0;32m   2147\u001b[0m             \u001b[1;31m# Training updates\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2148\u001b[0m             updates = self.optimizer.get_updates(\n\u001b[1;32m-> 2149\u001b[1;33m                 params=self._collected_trainable_weights, loss=self.total_loss)\n\u001b[0m\u001b[0;32m   2150\u001b[0m       \u001b[1;31m# Unconditional updates\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2151\u001b[0m       \u001b[0mupdates\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_updates_for\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\.conda\\envs\\indl\\lib\\site-packages\\tensorflow\\python\\keras\\optimizer_v2\\optimizer_v2.py\u001b[0m in \u001b[0;36mget_updates\u001b[1;34m(self, loss, params)\u001b[0m\n\u001b[0;32m    446\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    447\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0mget_updates\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mloss\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mparams\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 448\u001b[1;33m     \u001b[0mgrads\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_gradients\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mparams\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    449\u001b[0m     \u001b[0mgrads_and_vars\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mzip\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mgrads\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mparams\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    450\u001b[0m     self._assert_valid_dtypes([\n",
      "\u001b[1;32m~\\.conda\\envs\\indl\\lib\\site-packages\\tensorflow\\python\\keras\\optimizer_v2\\optimizer_v2.py\u001b[0m in \u001b[0;36mget_gradients\u001b[1;34m(self, loss, params)\u001b[0m\n\u001b[0;32m    359\u001b[0m         function not implemented).\n\u001b[0;32m    360\u001b[0m     \"\"\"\n\u001b[1;32m--> 361\u001b[1;33m     \u001b[0mgrads\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mgradients\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgradients\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mparams\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    362\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[1;32mNone\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mgrads\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    363\u001b[0m       raise ValueError(\"An operation has `None` for gradient. \"\n",
      "\u001b[1;32m~\\.conda\\envs\\indl\\lib\\site-packages\\tensorflow\\python\\ops\\gradients_impl.py\u001b[0m in \u001b[0;36mgradients\u001b[1;34m(ys, xs, grad_ys, name, colocate_gradients_with_ops, gate_gradients, aggregation_method, stop_gradients, unconnected_gradients)\u001b[0m\n\u001b[0;32m    156\u001b[0m         \u001b[0mys\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mxs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgrad_ys\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcolocate_gradients_with_ops\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    157\u001b[0m         \u001b[0mgate_gradients\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maggregation_method\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstop_gradients\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 158\u001b[1;33m         unconnected_gradients)\n\u001b[0m\u001b[0;32m    159\u001b[0m   \u001b[1;31m# pylint: enable=protected-access\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    160\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\.conda\\envs\\indl\\lib\\site-packages\\tensorflow\\python\\ops\\gradients_util.py\u001b[0m in \u001b[0;36m_GradientsHelper\u001b[1;34m(ys, xs, grad_ys, name, colocate_gradients_with_ops, gate_gradients, aggregation_method, stop_gradients, unconnected_gradients, src_graph)\u001b[0m\n\u001b[0;32m    615\u001b[0m     \u001b[0mstop_gradient_ops\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mop\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mt\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mstop_gradients\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    616\u001b[0m     reachable_to_ops, pending_count, loop_state = _PendingCount(\n\u001b[1;32m--> 617\u001b[1;33m         to_ops, from_ops, colocate_gradients_with_ops, func_graphs, xs)\n\u001b[0m\u001b[0;32m    618\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    619\u001b[0m     \u001b[1;31m# Iterate over the collected ops.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\.conda\\envs\\indl\\lib\\site-packages\\tensorflow\\python\\ops\\gradients_util.py\u001b[0m in \u001b[0;36m_PendingCount\u001b[1;34m(to_ops, from_ops, colocate_gradients_with_ops, func_graphs, xs)\u001b[0m\n\u001b[0;32m    147\u001b[0m   \u001b[1;31m# Mark reachable ops from from_ops.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    148\u001b[0m   \u001b[0mreached_ops\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mset\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 149\u001b[1;33m   \u001b[0m_MarkReachedOps\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfrom_ops\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mreached_ops\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfunc_graphs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    150\u001b[0m   \u001b[1;31m# X in reached_ops iff X is reachable from from_ops by a path of zero or more\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    151\u001b[0m   \u001b[1;31m# backpropagatable tensors.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\.conda\\envs\\indl\\lib\\site-packages\\tensorflow\\python\\ops\\gradients_util.py\u001b[0m in \u001b[0;36m_MarkReachedOps\u001b[1;34m(from_ops, reached_ops, func_graphs)\u001b[0m\n\u001b[0;32m    118\u001b[0m       \u001b[1;32mfor\u001b[0m \u001b[0moutput\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mop\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    119\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0m_IsBackpropagatable\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 120\u001b[1;33m           \u001b[0mqueue\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mextend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0m_Consumers\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfunc_graphs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    121\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    122\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\.conda\\envs\\indl\\lib\\site-packages\\tensorflow\\python\\ops\\gradients_util.py\u001b[0m in \u001b[0;36m_Consumers\u001b[1;34m(t, func_graphs)\u001b[0m\n\u001b[0;32m    539\u001b[0m   \u001b[1;32mfor\u001b[0m \u001b[0mfunc\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mfunc_graphs\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    540\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0minput_t\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mplaceholder\u001b[0m \u001b[1;32min\u001b[0m \u001b[0m_Captures\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfunc\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 541\u001b[1;33m       \u001b[1;32mif\u001b[0m \u001b[0minput_t\u001b[0m \u001b[1;33m==\u001b[0m \u001b[0mt\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    542\u001b[0m         \u001b[0mconsumers\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mextend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0m_Consumers\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mplaceholder\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfunc_graphs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    543\u001b[0m   \u001b[1;32mreturn\u001b[0m \u001b[0mconsumers\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\.conda\\envs\\indl\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py\u001b[0m in \u001b[0;36m__eq__\u001b[1;34m(self, other)\u001b[0m\n\u001b[0;32m    615\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0m__eq__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mother\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    616\u001b[0m     \u001b[1;31m# Necessary to support Python's collection membership operators\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 617\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0mid\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m==\u001b[0m \u001b[0mid\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mother\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    618\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    619\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0m__copy__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Iterate through each participant\n",
    "participant_names = ['ja', 'ca', 'wc', 'de', 'zt', 'fp']  # , 'mv'\n",
    "\n",
    "input_hists = []\n",
    "full_hists = []\n",
    "\n",
    "for p_ix, p_name in enumerate(participant_names):\n",
    "    \n",
    "    # Load their data\n",
    "    X, Y, ax_info = load_faces_houses(p_name)\n",
    "    ds_train, ds_valid, n_train = get_ds_train_valid(X, Y, p_train=PTRAIN, batch_size=20)\n",
    "    \n",
    "    # Make a new model with the proper input size\n",
    "    xfer_model = replace_input_layers(xfer_model, X.shape[1:])\n",
    "    \n",
    "    # But first we'll freeze layers other than input layers.\n",
    "    for layer_ix, layer in enumerate(xfer_model.layers):\n",
    "        if layer_ix > 1:\n",
    "            layer.trainable=False\n",
    "    \n",
    "    # Train for a couple epochs to update input layers only\n",
    "    xfer_model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "    temp = xfer_model.fit(x=ds_train,  \n",
    "                     epochs=10, \n",
    "                     validation_data=ds_valid,\n",
    "                     steps_per_epoch=n_train // BATCH_SIZE,\n",
    "                     validation_steps=(len(Y)-n_train) // BATCH_SIZE,\n",
    "                     verbose=1)\n",
    "    input_hists.append(temp)\n",
    "    \n",
    "    # Unfreeze later layers\n",
    "    for layer_ix, layer in enumerate(xfer_model.layers):\n",
    "        if layer_ix > 1:\n",
    "            layer.trainable=True\n",
    "    \n",
    "    # Train for longer at a much lower rate\n",
    "    xfer_model.compile(loss='categorical_crossentropy', optimizer=tf.keras.optimizers.RMSprop(lr=1e-5), metrics=['accuracy'])\n",
    "    temp = xfer_model.fit(x=ds_train,  \n",
    "                     epochs=N_EPOCHS, \n",
    "                     validation_data=ds_valid,\n",
    "                     steps_per_epoch=n_train // BATCH_SIZE,\n",
    "                     validation_steps=(len(Y)-n_train) // BATCH_SIZE,\n",
    "                     verbose=1)\n",
    "    full_hists.append(temp)\n",
    "\n",
    "# Save the weights for later\n",
    "W_Xfer = xfer_model.get_weights()\n",
    "xfer_model.save('xfer_model.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "24\n"
     ]
    }
   ],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PyCharm (IntracranialNeurophysDL)",
   "language": "python",
   "name": "pycharm-d1267685"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  },
  "pycharm": {
   "stem_cell": {
    "cell_type": "raw",
    "metadata": {
     "collapsed": false
    },
    "source": []
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
